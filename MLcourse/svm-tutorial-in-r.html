<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning</title>
  <meta name="description" content="Machine Learning course">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning" />
  
  <meta name="twitter:description" content="Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany">


<meta name="date" content="2018-09-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="model-selection.html">

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-88489172-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-88489172-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESILV Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i>Course Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i>What is Machine Learning ?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i>Supervised Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning</b></span></li>
<li class="part"><span><b>Regression</b></span></li>
<li class="chapter" data-level="1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regression.html"><a href="linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regression.html"><a href="linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i>Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression Model</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.5</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.6</b> Assessing the Accuracy of the Coefficient Estimates</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i>Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="linear-regression.html"><a href="linear-regression.html#anova-and-model-fit"><i class="fa fa-check"></i><b>1.7</b> ANOVA and model fit</a><ul>
<li class="chapter" data-level="1.7.1" data-path="linear-regression.html"><a href="linear-regression.html#anova"><i class="fa fa-check"></i><b>1.7.1</b> ANOVA</a></li>
<li class="chapter" data-level="1.7.2" data-path="linear-regression.html"><a href="linear-regression.html#the-r2-statistic"><i class="fa fa-check"></i><b>1.7.2</b> The <span class="math inline">\(R^2\)</span> Statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html"><i class="fa fa-check"></i>PW 1</a><ul>
<li class="chapter" data-level="1.8" data-path="pw-1.html"><a href="pw-1.html#some-r-basics"><i class="fa fa-check"></i><b>1.8</b> Some <code>R</code> basics</a><ul>
<li class="chapter" data-level="1.8.1" data-path="pw-1.html"><a href="pw-1.html#basic-commands"><i class="fa fa-check"></i><b>1.8.1</b> Basic Commands</a></li>
<li class="chapter" data-level="1.8.2" data-path="pw-1.html"><a href="pw-1.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="pw-1.html"><a href="pw-1.html#matrices-data-frames-and-lists"><i class="fa fa-check"></i><b>1.8.3</b> Matrices, data frames and lists</a></li>
<li class="chapter" data-level="1.8.4" data-path="pw-1.html"><a href="pw-1.html#graphics"><i class="fa fa-check"></i><b>1.8.4</b> Graphics</a></li>
<li class="chapter" data-level="1.8.5" data-path="pw-1.html"><a href="pw-1.html#distributions"><i class="fa fa-check"></i><b>1.8.5</b> Distributions</a></li>
<li class="chapter" data-level="1.8.6" data-path="pw-1.html"><a href="pw-1.html#working-directory"><i class="fa fa-check"></i><b>1.8.6</b> Working directory</a></li>
<li class="chapter" data-level="1.8.7" data-path="pw-1.html"><a href="pw-1.html#loading-data"><i class="fa fa-check"></i><b>1.8.7</b> Loading Data</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="pw-1.html"><a href="pw-1.html#regression"><i class="fa fa-check"></i><b>1.9</b> Regression</a><ul>
<li class="chapter" data-level="1.9.1" data-path="pw-1.html"><a href="pw-1.html#the-lm-function"><i class="fa fa-check"></i><b>1.9.1</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="pw-1.html"><a href="pw-1.html#boston"><i class="fa fa-check"></i><b>1.9.2</b> Predicting House Value: Boston dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-consid"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#how-to-select-the-best-performing-model"><i class="fa fa-check"></i><b>2.4</b> How to select the best performing model</a><ul>
<li><a href="multiple-linear-regression.html#use-the-adjusted-r_adj2-for-univariate-models">Use the Adjusted <span class="math inline">\(R_{adj}^2\)</span> for univariate models</a></li>
<li class="chapter" data-level="" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#have-a-look-at-the-residuals-or-error-terms"><i class="fa fa-check"></i>Have a look at the residuals or error terms</a></li>
<li class="chapter" data-level="" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#histogram-of-residuals"><i class="fa fa-check"></i>Histogram of residuals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html"><i class="fa fa-check"></i>PW 2</a><ul>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#multiple-linear-regression-1"><i class="fa fa-check"></i>Multiple Linear Regression</a></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="part"><span><b>Classification</b></span></li>
<li class="chapter" data-level="3" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-1"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="3.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#prediction-1"><i class="fa fa-check"></i><b>3.2.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="3.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logreg-examps"><i class="fa fa-check"></i><b>3.4</b> Example</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logreg-examps-challenger"><i class="fa fa-check"></i><b>3.4.1</b> Case study: <em>The Challenger disaster</em></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-introRStudio.html"><a href="app-introRStudio.html"><i class="fa fa-check"></i><b>A</b> Introduction to <code>RStudio</code></a></li>
<li class="chapter" data-level="B" data-path="app-ht.html"><a href="app-ht.html"><i class="fa fa-check"></i><b>B</b> Review on hypothesis testing</a></li>
<li class="chapter" data-level="C" data-path="use-qual.html"><a href="use-qual.html"><i class="fa fa-check"></i><b>C</b> Use of qualitative predictors</a></li>
<li class="chapter" data-level="D" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>D</b> Model Selection</a><ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#linear-model-selection-and-best-subset-selection"><i class="fa fa-check"></i>Linear Model Selection and Best Subset Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward-stepwise-selection"><i class="fa fa-check"></i>Forward Stepwise Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward-stepwise-selection"><i class="fa fa-check"></i>Backward Stepwise Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#estimating-test-error-using-mallows-cp-aic-bic-adjusted-r-squared"><i class="fa fa-check"></i>Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#estimating-test-error-using-cross-validation"><i class="fa fa-check"></i>Estimating Test Error Using Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html"><i class="fa fa-check"></i><b>E</b> SVM tutorial in R</a><ul>
<li class="chapter" data-level="E.1" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#linear-svm-classifier"><i class="fa fa-check"></i><b>E.1</b> Linear SVM classifier</a></li>
<li class="chapter" data-level="E.2" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#nonlinear-svm"><i class="fa fa-check"></i><b>E.2</b> Nonlinear SVM</a></li>
<li class="chapter" data-level="E.3" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#examples"><i class="fa fa-check"></i><b>E.3</b> Examples</a><ul>
<li class="chapter" data-level="E.3.1" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#best-subset-selection"><i class="fa fa-check"></i><b>E.3.1</b> Best Subset Selection</a></li>
<li class="chapter" data-level="E.3.2" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#forward-stepwise-selection-and-model-selection-using-validation-set"><i class="fa fa-check"></i><b>E.3.2</b> Forward Stepwise Selection and Model Selection Using Validation Set</a></li>
<li class="chapter" data-level="E.3.3" data-path="svm-tutorial-in-r.html"><a href="svm-tutorial-in-r.html#model-selection-using-cross-validation"><i class="fa fa-check"></i><b>E.3.3</b> Model Selection Using Cross-Validation</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="svm-tutorial-in-r" class="section level1">
<h1><span class="header-section-number">E</span> SVM tutorial in R</h1>
<div id="linear-svm-classifier" class="section level2">
<h2><span class="header-section-number">E.1</span> Linear SVM classifier</h2>
<p>Lets generate some data in two dimensions, and make them a little separated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">10111</span>)
x=<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">40</span>),<span class="dv">20</span>,<span class="dv">2</span>)
y=<span class="kw">rep</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">10</span>))
x[y<span class="op">==</span><span class="dv">1</span>,]=x[y<span class="op">==</span><span class="dv">1</span>,]<span class="op">+</span><span class="dv">1</span>
<span class="kw">plot</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>,<span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-67-1.png" width="70%" style="display: block; margin: auto;" /> Now we will load the package <code>e1071</code> which contains the <code>svm</code> function we will use. We then compute the fit. Notice that we have to specify a <code>cost</code> parameter, which is a tuning parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(e1071)
<span class="co">#ans&gt; Loading required package: e1071</span>
dat=<span class="kw">data.frame</span>(x,<span class="dt">y=</span><span class="kw">as.factor</span>(y))
svmfit=<span class="kw">svm</span>(y<span class="op">~</span>.,<span class="dt">data=</span>dat,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10</span>,<span class="dt">scale=</span><span class="ot">FALSE</span>)
<span class="kw">print</span>(svmfit)
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; Call:</span>
<span class="co">#ans&gt; svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, cost = 10, </span>
<span class="co">#ans&gt;     scale = FALSE)</span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; Parameters:</span>
<span class="co">#ans&gt;    SVM-Type:  C-classification </span>
<span class="co">#ans&gt;  SVM-Kernel:  linear </span>
<span class="co">#ans&gt;        cost:  10 </span>
<span class="co">#ans&gt;       gamma:  0.5 </span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; Number of Support Vectors:  6</span>
<span class="kw">plot</span>(svmfit,dat)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-68-1.png" width="70%" style="display: block; margin: auto;" /> As mentioned in the the chapter, the plot function is somewhat crude, and plots X2 on the horizontal axis (unlike what R would do automatically for a matrix). Lets see how we might make our own plot.</p>
<p>The first thing we will do is make a grid of values for X1 and X2. We will write a function to do that, in case we want to reuse it. It uses the handy function <code>expand.grid</code>, and produces the coordinates of <code>n*n</code> points on a lattice covering the domain of <code>x</code>. Having made the lattice, we make a prediction at each point on the lattice. We then plot the lattice, color-coded according to the classification. Now we can see the decision boundary.</p>
<p>The support points (points on the margin, or on the wrong side of the margin) are indexed in the <code>$index</code> component of the fit.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svmfit=<span class="kw">svm</span>(y<span class="op">~</span>.,<span class="dt">data=</span>dat,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10</span>,<span class="dt">scale=</span><span class="ot">FALSE</span>)
make.grid=<span class="cf">function</span>(x,<span class="dt">n=</span><span class="dv">75</span>){
  grange=<span class="kw">apply</span>(x,<span class="dv">2</span>,range)
  x1=<span class="kw">seq</span>(<span class="dt">from=</span>grange[<span class="dv">1</span>,<span class="dv">1</span>],<span class="dt">to=</span>grange[<span class="dv">2</span>,<span class="dv">1</span>],<span class="dt">length=</span>n)
  x2=<span class="kw">seq</span>(<span class="dt">from=</span>grange[<span class="dv">1</span>,<span class="dv">2</span>],<span class="dt">to=</span>grange[<span class="dv">2</span>,<span class="dv">2</span>],<span class="dt">length=</span>n)
  <span class="kw">expand.grid</span>(<span class="dt">X1=</span>x1,<span class="dt">X2=</span>x2)
  }
xgrid=<span class="kw">make.grid</span>(x)
ygrid=<span class="kw">predict</span>(svmfit,xgrid)
<span class="kw">plot</span>(xgrid,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)[<span class="kw">as.numeric</span>(ygrid)],<span class="dt">pch=</span><span class="dv">20</span>,<span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="kw">points</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>,<span class="dt">pch=</span><span class="dv">19</span>)
<span class="kw">points</span>(x[svmfit<span class="op">$</span>index,],<span class="dt">pch=</span><span class="dv">5</span>,<span class="dt">cex=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-69-1.png" width="70%" style="display: block; margin: auto;" /> The <code>svm</code> function is not too friendly, in that we have to do some work to get back the linear coefficients, as described in the text. Probably the reason is that this only makes sense for linear kernels, and the function is more general. Here we will use a formula to extract the coefficients; for those interested in where this comes from, have a look in chapter 12 of ESL (“Elements of Statistical Learning”).</p>
<p>We extract the linear coefficients, and then using simple algebra, we include the decision boundary and the two margins.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta=<span class="kw">drop</span>(<span class="kw">t</span>(svmfit<span class="op">$</span>coefs)<span class="op">%*%</span>x[svmfit<span class="op">$</span>index,])
<span class="co">#ans&gt; Error in t(svmfit$coefs): objet &#39;svmfit&#39; introuvable</span>
beta0=svmfit<span class="op">$</span>rho
<span class="co">#ans&gt; Error in eval(expr, envir, enclos): objet &#39;svmfit&#39; introuvable</span>
<span class="kw">plot</span>(xgrid,<span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>)[<span class="kw">as.numeric</span>(ygrid)],<span class="dt">pch=</span><span class="dv">20</span>,<span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="co">#ans&gt; Error in plot.xy(xy, type, ...): objet &#39;ygrid&#39; introuvable</span>
<span class="kw">points</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">3</span>,<span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-70-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">points</span>(x[svmfit<span class="op">$</span>index,],<span class="dt">pch=</span><span class="dv">5</span>,<span class="dt">cex=</span><span class="dv">2</span>)
<span class="co">#ans&gt; Error in points(x[svmfit$index, ], pch = 5, cex = 2): objet &#39;svmfit&#39; introuvable</span>
<span class="kw">abline</span>(beta0<span class="op">/</span>beta[<span class="dv">2</span>],<span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>])
<span class="co">#ans&gt; Error in abline(beta0/beta[2], -beta[1]/beta[2]): objet &#39;beta0&#39; introuvable</span>
<span class="kw">abline</span>((beta0<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>beta[<span class="dv">2</span>],<span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="co">#ans&gt; Error in abline((beta0 - 1)/beta[2], -beta[1]/beta[2], lty = 2): objet &#39;beta0&#39; introuvable</span>
<span class="kw">abline</span>((beta0<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>beta[<span class="dv">2</span>],<span class="op">-</span>beta[<span class="dv">1</span>]<span class="op">/</span>beta[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="co">#ans&gt; Error in abline((beta0 + 1)/beta[2], -beta[1]/beta[2], lty = 2): objet &#39;beta0&#39; introuvable</span></code></pre></div>
<p>Just like for the other models in this book, the tuning parameter <code>C</code> has to be selected. Different values will give different solutions. Rerun the code above, but using <code>C=1</code>, and see what we mean. One can use cross-validation to do this.</p>
</div>
<div id="nonlinear-svm" class="section level2">
<h2><span class="header-section-number">E.2</span> Nonlinear SVM</h2>
<p>Instead, we will run the SVM on some data where a non-linear boundary is called for. We will use the mixture data from ESL</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;datasets/ESL.mixture.rda&quot;</span>)
<span class="kw">names</span>(ESL.mixture)
<span class="co">#ans&gt; [1] &quot;x&quot;        &quot;y&quot;        &quot;xnew&quot;     &quot;prob&quot;     &quot;marginal&quot; &quot;px1&quot;     </span>
<span class="co">#ans&gt; [7] &quot;px2&quot;      &quot;means&quot;</span>
<span class="kw">rm</span>(x,y)
<span class="kw">attach</span>(ESL.mixture)</code></pre></div>
<p>These data are also two dimensional. Lets plot them and fit a nonlinear SVM, using a radial kernel.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">1</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-72-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat=<span class="kw">data.frame</span>(<span class="dt">y=</span><span class="kw">factor</span>(y),x)
fit=<span class="kw">svm</span>(<span class="kw">factor</span>(y)<span class="op">~</span>.,<span class="dt">data=</span>dat,<span class="dt">scale=</span><span class="ot">FALSE</span>,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">cost=</span><span class="dv">5</span>)
<span class="co">#ans&gt; Error in svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = &quot;radial&quot;, : impossible de trouver la fonction &quot;svm&quot;</span></code></pre></div>
<p>Now we are going to create a grid, as before, and make predictions on the grid. These data have the grid points for each variable included on the data frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xgrid=<span class="kw">expand.grid</span>(<span class="dt">X1=</span>px1,<span class="dt">X2=</span>px2)
ygrid=<span class="kw">predict</span>(fit,xgrid)
<span class="co">#ans&gt; Error in predict(fit, xgrid): objet &#39;fit&#39; introuvable</span>
<span class="kw">plot</span>(xgrid,<span class="dt">col=</span><span class="kw">as.numeric</span>(ygrid),<span class="dt">pch=</span><span class="dv">20</span>,<span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="co">#ans&gt; Error in plot.xy(xy, type, ...): objet &#39;ygrid&#39; introuvable</span>
<span class="kw">points</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">1</span>,<span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-73-1.png" width="70%" style="display: block; margin: auto;" /> We can go further, and have the predict function produce the actual function estimates at each of our grid points. We can include the actual decision boundary on the plot by making use of the contour function. On the dataframe is also <code>prob</code>, which is the true probability of class 1 for these data, at the gridpoints. If we plot its 0.5 contour, that will give us the <em>Bayes Decision Boundary</em>, which is the best one could ever do.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">func=<span class="kw">predict</span>(fit,xgrid,<span class="dt">decision.values=</span><span class="ot">TRUE</span>)
<span class="co">#ans&gt; Error in predict(fit, xgrid, decision.values = TRUE): objet &#39;fit&#39; introuvable</span>
func=<span class="kw">attributes</span>(func)<span class="op">$</span>decision
<span class="co">#ans&gt; Error in eval(expr, envir, enclos): objet &#39;func&#39; introuvable</span>
xgrid=<span class="kw">expand.grid</span>(<span class="dt">X1=</span>px1,<span class="dt">X2=</span>px2)
ygrid=<span class="kw">predict</span>(fit,xgrid)
<span class="co">#ans&gt; Error in predict(fit, xgrid): objet &#39;fit&#39; introuvable</span>
<span class="kw">plot</span>(xgrid,<span class="dt">col=</span><span class="kw">as.numeric</span>(ygrid),<span class="dt">pch=</span><span class="dv">20</span>,<span class="dt">cex=</span>.<span class="dv">2</span>)
<span class="co">#ans&gt; Error in plot.xy(xy, type, ...): objet &#39;ygrid&#39; introuvable</span>
<span class="kw">points</span>(x,<span class="dt">col=</span>y<span class="op">+</span><span class="dv">1</span>,<span class="dt">pch=</span><span class="dv">19</span>)

<span class="kw">contour</span>(px1,px2,<span class="kw">matrix</span>(func,<span class="dv">69</span>,<span class="dv">99</span>),<span class="dt">level=</span><span class="dv">0</span>,<span class="dt">add=</span><span class="ot">TRUE</span>)
<span class="co">#ans&gt; Error in matrix(func, 69, 99): objet &#39;func&#39; introuvable</span>
<span class="kw">contour</span>(px1,px2,<span class="kw">matrix</span>(prob,<span class="dv">69</span>,<span class="dv">99</span>),<span class="dt">level=</span>.<span class="dv">5</span>,<span class="dt">add=</span><span class="ot">TRUE</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-74-1.png" width="70%" style="display: block; margin: auto;" /> We see in this case that the radial kernel has done an excellent job.</p>
</div>
<div id="examples" class="section level2">
<h2><span class="header-section-number">E.3</span> Examples</h2>
<div id="best-subset-selection" class="section level3">
<h3><span class="header-section-number">E.3.1</span> Best Subset Selection</h3>
<iframe width="560" height="349" src="https://www.youtube.com/embed/3kwdDGnV8MM" frameborder="0" allowfullscreen>
</iframe>
</div>
<div id="forward-stepwise-selection-and-model-selection-using-validation-set" class="section level3">
<h3><span class="header-section-number">E.3.2</span> Forward Stepwise Selection and Model Selection Using Validation Set</h3>
<iframe width="560" height="349" src="https://www.youtube.com/embed/mv-vdysZIb4" frameborder="0" allowfullscreen>
</iframe>
</div>
<div id="model-selection-using-cross-validation" class="section level3">
<h3><span class="header-section-number">E.3.3</span> Model Selection Using Cross-Validation</h3>
<iframe width="560" height="349" src="https://www.youtube.com/embed/F8MMHCCoALU" frameborder="0" allowfullscreen>
</iframe>
<!-- # PW5 - Annexe -->
<!-- Concernant la séance 5: J'ai remarqué que la majorité n'a pas compris ce qu'il fallait faire! Le but de la séance est d'exploiter et analyser un jeu de données. La première chose à faire est la phase descriptive où on décrit les données, les variables, quelques figures descriptives, etc.. Ensuite il faut regarder la variable cible, qu'on souhaite modéliser et prédire. Selon le type de cette variable on décide la direction de l'analyse: -->
<!-- 1- Variable cible **quantitative**: On fait la **regression**. Donc ce qui est important dans ce cas est d'étudier les corrélations entre les variables (prédicteurs) et en particulier entre les corrélations entre les prédicteurs et la variable cible. Ensuite sur le training set construire le modèle de régression. Pour cette tâche, vous pouvez commencer par un modèle qui prend tous les prédicteurs, et ensuite commencer à enlever les prédicteurs non significatives (comme on a fait en tp, c'est en fait la méthode backward). Vous pouvez tout de même pour la régression appliquer les méthodes décrites dans les vidéos du chapitre 5 pour faciliter la recherche du meilleur modèle. Une fois le modèle est trouvé, reste à faire la prédiction (sur le test set) et la performance du modèle. -->
<!-- 2- Variable cible **qualitative**: Alors on fait la **classification**. Comme la variable cible est qualitative vous pouvez étudier la relation entre les prédicteurs et cette variable (boxplots, mosaicplots, etc..).  -->
<!-- Si la variable cible a 2 classes, vous pouvez construire des modèles de classifications vus en cours, prédire, ensuite comparer la performance de vos modèles (matrices de confusion, accuracy, courbes roc, etc..). Attention, on peut suivre les méthodes expliquées dans les vidéos du chapitre 5, mais pas avec les mêmes codes! la démarche expliquée dans les vidéos est commune, mais pour la classification il faut utiliser une autre librairie que "leaps", par exemple "bestglm". -->
<!-- Si la variable cible a plus que 2 classes, pas besoin d'appliquer la régression logistique (raison expliquée dans le cours!), il suffit d'appliquer les méthodes discriminantes du chapitre 4. Attention, la courbe roc n'est pas applicable dans ce cas, on compare la performance en utilisant la matrice de confusion (au moins l'accuracy!). Mais d'autre part, vous pouvez quand même transformer le problème en deux classes, il s'agit de l'approche one-vs-all: c'est à dire considérer la classe 0 contre tous les autres classes à la fois. -->
<!-- Ps, il ne vous faut pas la correction des tps pour pouvoir faire tout ce qui est décrit ci dessus. Vous avez suffisamment des exemples dans le cous, et dans l'énoncé du tp4, vous avez un exemple détaillé avec des commentaires aussi. -->
<!-- Voilà.  -->
<!-- # References -->

<div id="refs" class="references">
<div>
<p>Dalal, Siddhartha R., Edward B. Fowlkes, and Bruce Hoadley. 1989. “Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure.” <em>Journal of the American Statistical Association</em> 84 (408): 945–57. doi:<a href="https://doi.org/10.1080/01621459.1989.10478858">10.1080/01621459.1989.10478858</a>.</p>
</div>
<div>
<p>Presidential Commission on the Space Shuttle Challenger Accident. 1986. <em>Report of the Presidential Commission on the Space Shuttle Challenger Accident (Vols. 1 &amp; 2)</em>. Washington, DC. <a href="http://history.nasa.gov/rogersrep/genindex.htm" class="uri">http://history.nasa.gov/rogersrep/genindex.htm</a>.</p>
</div>
</div>
</div>
</div>
</div>
















            </section>

          </div>
        </div>
      </div>
<a href="model-selection.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Machine-Learning.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
