<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>PW 4 | Machine Learning</title>
  <meta name="description" content="PW 4 | Machine Learning course" />
  <meta name="generator" content="bookdown 0.24.10 and GitBook 2.6.7" />

  <meta property="og:title" content="PW 4 | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="PW 4 | Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PW 4 | Machine Learning" />
  
  <meta name="twitter:description" content="PW 4 | Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2022-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4-discriminant-analysis.html"/>
<link rel="next" href="5-decision-trees-random-forests.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="book_assets/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />
<script src="book_assets/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="book_assets/plotly-binding-4.10.0/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_new.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li><a href="index.html#welcome">Welcome<span></span></a>
<ul>
<li><a href="index.html#course-overview">Course Overview<span></span></a></li>
<li><a href="index.html#course-schedule">Course Schedule<span></span></a></li>
</ul></li>
<li><a href="introduction.html#introduction">Introduction<span></span></a>
<ul>
<li><a href="introduction.html#what-is-machine-learning">What is Machine Learning ?<span></span></a></li>
<li><a href="introduction.html#supervised-learning">Supervised Learning<span></span></a></li>
<li><a href="introduction.html#unsupervised-learning">Unsupervised Learning<span></span></a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning<span></span></b></span></li>
<li class="part"><span><b>Regression<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="1-linear-regression.html"><a href="1-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-linear-regression.html"><a href="1-linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="1-linear-regression.html"><a href="1-linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="1-linear-regression.html"><a href="1-linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?<span></span></a>
<ul>
<li><a href="1-linear-regression.html#prediction">Prediction<span></span></a></li>
<li><a href="1-linear-regression.html#inference">Inference<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-linear-regression.html"><a href="1-linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression Model<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="1-linear-regression.html"><a href="1-linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.5</b> Estimating the Coefficients<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="1-linear-regression.html"><a href="1-linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.6</b> Assessing the Accuracy of the Coefficient Estimates<span></span></a>
<ul>
<li><a href="1-linear-regression.html#hypothesis-testing">Hypothesis testing<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="1-linear-regression.html"><a href="1-linear-regression.html#anova-and-model-fit"><i class="fa fa-check"></i><b>1.7</b> ANOVA and model fit<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="1-linear-regression.html"><a href="1-linear-regression.html#anova"><i class="fa fa-check"></i><b>1.7.1</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="1-linear-regression.html"><a href="1-linear-regression.html#the-r2-statistic"><i class="fa fa-check"></i><b>1.7.2</b> The <span class="math inline">\(R^2\)</span> Statistic<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="practical-work-1.html#practical-work-1">Practical Work 1<span></span></a>
<ul>
<li class="chapter" data-level="1.8" data-path="practical-work-1.html"><a href="practical-work-1.html"><i class="fa fa-check"></i><b>1.8</b> Some <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> basics<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="practical-work-1.html"><a href="practical-work-1.html#basic-commands"><i class="fa fa-check"></i><b>1.8.1</b> Basic Commands<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="practical-work-1.html"><a href="practical-work-1.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="practical-work-1.html"><a href="practical-work-1.html#matrices-data-frames-and-lists"><i class="fa fa-check"></i><b>1.8.3</b> Matrices, data frames and lists<span></span></a></li>
<li class="chapter" data-level="1.8.4" data-path="practical-work-1.html"><a href="practical-work-1.html#graphics"><i class="fa fa-check"></i><b>1.8.4</b> Graphics<span></span></a></li>
<li class="chapter" data-level="1.8.5" data-path="practical-work-1.html"><a href="practical-work-1.html#distributions"><i class="fa fa-check"></i><b>1.8.5</b> Distributions<span></span></a></li>
<li class="chapter" data-level="1.8.6" data-path="practical-work-1.html"><a href="practical-work-1.html#working-directory"><i class="fa fa-check"></i><b>1.8.6</b> Working directory<span></span></a></li>
<li class="chapter" data-level="1.8.7" data-path="practical-work-1.html"><a href="practical-work-1.html#loading-data"><i class="fa fa-check"></i><b>1.8.7</b> Loading Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="practical-work-1.html"><a href="practical-work-1.html#regression"><i class="fa fa-check"></i><b>1.9</b> Regression<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="practical-work-1.html"><a href="practical-work-1.html#the-lm-function"><i class="fa fa-check"></i><b>1.9.1</b> The <code>lm</code> function<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="practical-work-1.html"><a href="practical-work-1.html#boston"><i class="fa fa-check"></i><b>1.9.2</b> Predicting House Value: Boston dataset<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#other-consid"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#how-to-select-the-best-performing-model"><i class="fa fa-check"></i><b>2.4</b> How to select the best performing model<span></span></a>
<ul>
<li><a href="2-multiple-linear-regression.html#use-the-adjusted-r_adj2-for-multivariate-models">Use the Adjusted <span class="math inline">\(R_{adj}^2\)</span> for multivariate models<span></span></a></li>
<li><a href="2-multiple-linear-regression.html#have-a-look-at-the-residuals-or-error-terms">Have a look at the residuals or error terms<span></span></a></li>
<li><a href="2-multiple-linear-regression.html#histogram-of-residuals">Histogram of residuals<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-2.html#pw-2">PW 2<span></span></a>
<ul>
<li><a href="pw-2.html#multiple-linear-regression-1">Multiple Linear Regression<span></span></a></li>
<li><a href="pw-2.html#reporting">Reporting<span></span></a></li>
</ul></li>
<li class="part"><span><b>Classification<span></span></b></span></li>
<li class="chapter" data-level="3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logistic-regression-1"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#prediction-1"><i class="fa fa-check"></i><b>3.2.3</b> Prediction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logreg-examps"><i class="fa fa-check"></i><b>3.4</b> Example<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logreg-examps-challenger"><i class="fa fa-check"></i><b>3.4.1</b> Case study: <em>The Challenger disaster</em><span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-3.html#pw-3">PW 3<span></span></a>
<ul>
<li><a href="pw-3.html#social-networks-ads">Social Networks Ads<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html"><i class="fa fa-check"></i><b>4</b> Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#bayes-theorem"><i class="fa fa-check"></i><b>4.2</b> Bayes’ Theorem<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-for-p1"><i class="fa fa-check"></i><b>4.3</b> LDA for <span class="math inline">\(p=1\)</span><span></span></a></li>
<li class="chapter" data-level="4.4" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#estimating-the-parameters"><i class="fa fa-check"></i><b>4.4</b> Estimating the parameters<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-for-p-1"><i class="fa fa-check"></i><b>4.5</b> LDA for <span class="math inline">\(p &gt; 1\)</span><span></span></a></li>
<li class="chapter" data-level="4.6" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#making-predictions"><i class="fa fa-check"></i><b>4.6</b> Making predictions<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#other-forms-of-discriminant-analysis"><i class="fa fa-check"></i><b>4.7</b> Other forms of Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>4.7.1</b> Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>4.7.2</b> Naive Bayes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-vs-logistic-regression"><i class="fa fa-check"></i><b>4.8</b> LDA vs Logistic Regression<span></span></a></li>
</ul></li>
<li><a href="pw-4.html#pw-4">PW 4<span></span></a>
<ul>
<li><a href="pw-4.html#logistic-regression-2">Logistic Regression<span></span></a></li>
<li><a href="pw-4.html#decision-boundary-of-logistic-regression">Decision Boundary of Logistic Regression<span></span></a></li>
<li><a href="pw-4.html#linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)<span></span></a></li>
<li><a href="pw-4.html#lda-from-scratch">LDA from scratch<span></span></a></li>
<li><a href="pw-4.html#quadratic-discriminant-analysis-qda-1">Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li><a href="pw-4.html#comparison">Comparison<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-decision-trees-random-forests.html"><a href="5-decision-trees-random-forests.html"><i class="fa fa-check"></i><b>5</b> Decision Trees &amp; Random Forests<span></span></a>
<ul>
<li><a href="5-decision-trees-random-forests.html#the-basics-of-decision-trees">The Basics of Decision Trees<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#classification-trees">Classification Trees<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#bagging-random-forests">Bagging &amp; Random Forests<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#boosting">Boosting<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#trees-in-r">Trees in <code>R</code><span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#random-forests---the-first-choice-method-for-every-data-analysis">Random forests - the first-choice method for every data analysis?<span></span></a></li>
</ul></li>
<li><a href="pw-5.html#pw-5">PW 5<span></span></a>
<ul>
<li><a href="pw-5.html#regression-trees">Regression Trees<span></span></a>
<ul>
<li><a href="pw-5.html#single-tree">Single tree<span></span></a></li>
<li><a href="pw-5.html#bagging">Bagging<span></span></a></li>
<li><a href="pw-5.html#random-forests">Random Forests<span></span></a></li>
<li><a href="pw-5.html#boosting-1">Boosting<span></span></a></li>
<li><a href="pw-5.html#comparison-1">Comparison<span></span></a></li>
</ul></li>
<li><a href="pw-5.html#classification-trees-1">Classification Trees<span></span></a>
<ul>
<li><a href="pw-5.html#the-spam-dataset">The Spam dataset<span></span></a></li>
<li><a href="pw-5.html#extra-tuning">Extra: Tuning<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Dimensionality Reduction<span></span></b></span></li>
<li class="chapter" data-level="6" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html"><i class="fa fa-check"></i><b>6</b> Principal Components Analysis<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#principal-components"><i class="fa fa-check"></i><b>6.2</b> Principal Components<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#notations-and-procedure">Notations and Procedure<span></span></a></li>
<li><a href="6-principal-components-analysis.html#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span><span></span></a></li>
<li><a href="6-principal-components-analysis.html#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span><span></span></a></li>
<li><a href="6-principal-components-analysis.html#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span><span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#how-do-we-find-the-coefficients"><i class="fa fa-check"></i><b>6.3</b> How do we find the coefficients?<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#why-it-may-be-possible-to-reduce-dimensions">Why It May Be Possible to Reduce Dimensions<span></span></a></li>
<li><a href="6-principal-components-analysis.html#procedure">Procedure<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#standardization-of-the-features"><i class="fa fa-check"></i><b>6.4</b> Standardization of the features<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#projection-of-the-data"><i class="fa fa-check"></i><b>6.5</b> Projection of the data<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#scores">Scores<span></span></a></li>
<li><a href="6-principal-components-analysis.html#visualization">Visualization<span></span></a></li>
<li><a href="6-principal-components-analysis.html#extra">Extra<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#case-study"><i class="fa fa-check"></i><b>6.6</b> Case study<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#employement-in-european-countries-in-the-late-70s">Employement in European countries in the late 70s<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-6.html#pw-6">PW 6<span></span></a>
<ul>
<li><a href="pw-6.html#the-iris-dataset">The Iris Dataset<span></span></a></li>
<li><a href="pw-6.html#loading-data-1">Loading Data<span></span></a></li>
<li><a href="pw-6.html#exploratory-analysis">Exploratory analysis<span></span></a></li>
<li><a href="pw-6.html#pca-using-princomp">PCA using <code>princomp()</code><span></span></a></li>
<li><a href="pw-6.html#deeper-pca-using-factoextra-package">Deeper PCA using <code>factoextra</code> package<span></span></a></li>
<li><a href="pw-6.html#step-by-step-pca">Step-by-step PCA<span></span></a></li>
</ul></li>
<li class="part"><span><b>III Unsupervised Learning<span></span></b></span></li>
<li class="chapter" data-level="7" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html"><i class="fa fa-check"></i><b>7</b> Kmeans &amp; Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#unsupervised-learning-1"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#clustering"><i class="fa fa-check"></i><b>7.2</b> Clustering<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#introduction-4"><i class="fa fa-check"></i><b>7.3</b> Introduction<span></span></a>
<ul>
<li><a href="7-kmeans-hierarchical-clustering.html#hard-clustering">Hard clustering<span></span></a></li>
<li><a href="7-kmeans-hierarchical-clustering.html#fuzzy-clustering">Fuzzy clustering<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#k-means"><i class="fa fa-check"></i><b>7.4</b> <span class="math inline">\(k\)</span>-Means<span></span></a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#k-means-in"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(k\)</span>-means in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg><span></span></a></li>
<li class="chapter" data-level="7.4.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#cluster-validity-choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>7.4.2</b> Cluster Validity, Choosing the Number of Clusters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>7.5</b> Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#dendrogram"><i class="fa fa-check"></i><b>7.5.1</b> Dendrogram<span></span></a></li>
<li class="chapter" data-level="7.5.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#the-hierarchical-clustering-algorithm"><i class="fa fa-check"></i><b>7.5.2</b> The Hierarchical Clustering Algorithm<span></span></a></li>
<li class="chapter" data-level="7.5.3" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#hierarchical-clustering-in"><i class="fa fa-check"></i><b>7.5.3</b> Hierarchical clustering in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg><span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-7.html#pw-7">PW 7<span></span></a>
<ul>
<li><a href="pw-7.html#reporting-1">Reporting<span></span></a>
<ul>
<li><a href="pw-7.html#markdown">Markdown<span></span></a></li>
<li><a href="pw-7.html#r-markdown">R Markdown<span></span></a></li>
<li><a href="pw-7.html#the-report-to-be-submitted">The report to be submitted<span></span></a></li>
</ul></li>
<li><a href="pw-7.html#k-means-clustering"><span class="math inline">\(k\)</span>-means clustering<span></span></a>
<ul>
<li><a href="pw-7.html#pointscards"><code>pointsCards</code><span></span></a></li>
<li><a href="pw-7.html#ligue-1"><code>Ligue 1</code><span></span></a></li>
<li><a href="pw-7.html#pca"><code>PCA</code><span></span></a></li>
<li><a href="pw-7.html#implementing-k-means"><code>Implementing k-means</code><span></span></a></li>
</ul></li>
<li><a href="pw-7.html#hierarchical-clustering-1">Hierarchical clustering<span></span></a>
<ul>
<li><a href="pw-7.html#distances-dist">Distances <code>dist()</code><span></span></a></li>
<li><a href="pw-7.html#dendrogram-hclust">Dendrogram <code>hclust()</code><span></span></a></li>
<li><a href="pw-7.html#hierarchical-clustering-on-iris-dataset">Hierarchical clustering on Iris dataset<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html"><i class="fa fa-check"></i><b>8</b> Gaussian Mixture Models &amp; EM<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#the-gaussian-distribution"><i class="fa fa-check"></i><b>8.1</b> The Gaussian distribution<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#mixture-of-gaussians"><i class="fa fa-check"></i><b>8.2</b> Mixture of Gaussians<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#em-for-gaussian-mixtures"><i class="fa fa-check"></i><b>8.3</b> EM for Gaussian Mixtures<span></span></a></li>
</ul></li>
<li><a href="pw-8.html#pw-8">PW 8<span></span></a>
<ul>
<li><a href="pw-8.html#report-template">Report template<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="pw-8.html"><a href="pw-8.html"><i class="fa fa-check"></i><b>8.4</b> EM using <code>mclust</code><span></span></a>
<ul>
<li><a href="pw-8.html#gmm-vs-k-means">GMM vs <span class="math inline">\(k\)</span>-means<span></span></a></li>
<li><a href="pw-8.html#em-on-1d">EM on 1D<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="pw-8.html"><a href="pw-8.html#em-from-scratch"><i class="fa fa-check"></i><b>8.5</b> EM from scratch<span></span></a></li>
</ul></li>
<li class="part"><span><b>Hackathon<span></span></b></span></li>
<li><a href="hackathon.html#hackathon">Hackathon<span></span></a></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="A-final-grades.html"><a href="A-final-grades.html"><i class="fa fa-check"></i><b>A</b> Final Grades<span></span></a></li>
<li class="chapter" data-level="B" data-path="B-app-introRStudio.html"><a href="B-app-introRStudio.html"><i class="fa fa-check"></i><b>B</b> Introduction to <code>RStudio</code><span></span></a></li>
<li class="chapter" data-level="C" data-path="C-app-ht.html"><a href="C-app-ht.html"><i class="fa fa-check"></i><b>C</b> Review on hypothesis testing<span></span></a></li>
<li class="chapter" data-level="D" data-path="D-use-qual.html"><a href="D-use-qual.html"><i class="fa fa-check"></i><b>D</b> Use of qualitative predictors<span></span></a></li>
<li class="chapter" data-level="E" data-path="E-model-selection.html"><a href="E-model-selection.html"><i class="fa fa-check"></i><b>E</b> Model Selection<span></span></a>
<ul>
<li><a href="E-model-selection.html#linear-model-selection-and-best-subset-selection">Linear Model Selection and Best Subset Selection<span></span></a></li>
<li><a href="E-model-selection.html#forward-stepwise-selection">Forward Stepwise Selection<span></span></a></li>
<li><a href="E-model-selection.html#backward-stepwise-selection">Backward Stepwise Selection<span></span></a></li>
<li><a href="E-model-selection.html#estimating-test-error-using-mallows-cp-aic-bic-adjusted-r-squared">Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared<span></span></a></li>
<li><a href="E-model-selection.html#estimating-test-error-using-cross-validation">Estimating Test Error Using Cross-Validation<span></span></a></li>
<li><a href="E-model-selection.html#examples">Examples<span></span></a>
<ul>
<li><a href="E-model-selection.html#best-subset-selection">Best Subset Selection<span></span></a></li>
<li><a href="E-model-selection.html#forward-stepwise-selection-and-model-selection-using-validation-set">Forward Stepwise Selection and Model Selection Using Validation Set<span></span></a></li>
<li><a href="E-model-selection.html#model-selection-using-cross-validation">Model Selection Using Cross-Validation<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="F" data-path="F-references-and-credits.html"><a href="F-references-and-credits.html"><i class="fa fa-check"></i><b>F</b> References and Credits<span></span></a></li>
<li class="chapter" data-level="G" data-path="G-other-references.html"><a href="G-other-references.html"><i class="fa fa-check"></i><b>G</b> Other References<span></span></a></li>
<li><a href="main-references-credits.html#main-references-credits">Main References &amp; Credits<span></span></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pw-4" class="section level1 unnumbered hasAnchor">
<h1>PW 4<a href="pw-4.html#pw-4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>During this session we are going to continue the analysis of the <code>Social_Network_Ads</code> <a target="_blank" href="http://www.mghassany.com/MLcourse/datasets/Social_Network_Ads.csv">dataset <i class="fa fa-table" aria-hidden="true"></i></a>. Recall that this dataset contains informations of users of a social network and if they bought a specified product. Last week we built a Logistic Regression model for the variable <code>Purchased</code> in function of <code>Age</code> and <code>EstimatedSalary</code>. We will consider the same variables this week but we will fit different models using methods such as LDA, QDA, and Naive Bayes.</p>
<!-- ## Report template {-} -->
<!-- For this week, use these YAML settings for your RMarkdown file: -->
<!-- ``` -->
<!-- --- -->
<!-- title: "Week 4" -->
<!-- subtitle: "Discriminant Analysis" -->
<!-- author: LastName FirstName -->
<!-- date: "`#r format(Sys.time())`" # remove the # to show the date -->
<!-- output: -->
<!--   html_document: -->
<!--     toc: true -->
<!--     toc_depth: 2 -->
<!--     toc_float: true -->
<!--     theme: cerulean -->
<!--     highlight: espresso -->
<!-- --- -->
<!-- ``` -->
<div id="logistic-regression-2" class="section level2 unnumbered hasAnchor">
<h2>Logistic Regression<a href="pw-4.html#logistic-regression-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>1.</strong> First, let’s do the pre-processing steps you were asked to do during the last session and fit a <strong>logistic regression</strong> model. Please read and understand very well the following code (read the comments!). Then copy what is necessary for today’s session to your report (but remove my comments!).</p>
<p>If you lost the dataset, you can download it from <a target="_blank" href="http://www.mghassany.com/MLcourse/datasets/Social_Network_Ads.csv">here <i class="fa fa-table" aria-hidden="true"></i></a>.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="pw-4.html#cb40-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-2"><a href="pw-4.html#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the dataset.. I have putted it into a folder called &quot;datasets&quot;</span></span>
<span id="cb40-3"><a href="pw-4.html#cb40-3" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&#39;http://www.mghassany.com/MLcourse/datasets/Social_Network_Ads.csv&#39;</span>)</span>
<span id="cb40-4"><a href="pw-4.html#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="pw-4.html#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Describing and Exploring the dataset</span></span>
<span id="cb40-6"><a href="pw-4.html#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="pw-4.html#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dataset) <span class="co"># to show the structure of the dataset. </span></span>
<span id="cb40-8"><a href="pw-4.html#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dataset) <span class="co"># will show some statistics of every column. </span></span>
<span id="cb40-9"><a href="pw-4.html#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Remark what it shows when the column is a numerical or categorical variable.</span></span>
<span id="cb40-10"><a href="pw-4.html#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Remark that it has no sense for the variable User.ID</span></span>
<span id="cb40-11"><a href="pw-4.html#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="pw-4.html#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Age <span class="sc">~</span> Purchased, <span class="at">data=</span>dataset, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">main=</span><span class="st">&quot;Boxplot Age ~ Purchased&quot;</span>)</span>
<span id="cb40-13"><a href="pw-4.html#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># You know what is a boxplot right? I will let you interpret it.</span></span>
<span id="cb40-14"><a href="pw-4.html#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(EstimatedSalary <span class="sc">~</span> Purchased, <span class="at">data=</span>dataset,<span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb40-15"><a href="pw-4.html#cb40-15" aria-hidden="true" tabindex="-1"></a> <span class="at">main=</span><span class="st">&quot;Boxplot EstimatedSalary ~ Purchased&quot;</span>)</span>
<span id="cb40-16"><a href="pw-4.html#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Another boxplot</span></span>
<span id="cb40-17"><a href="pw-4.html#cb40-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-18"><a href="pw-4.html#cb40-18" aria-hidden="true" tabindex="-1"></a><span class="fu">aov</span>(EstimatedSalary <span class="sc">~</span>Purchased, <span class="at">data=</span>dataset)</span>
<span id="cb40-19"><a href="pw-4.html#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Anova test, but we need to show the summary of </span></span>
<span id="cb40-20"><a href="pw-4.html#cb40-20" aria-hidden="true" tabindex="-1"></a><span class="co"># it in order to see the p-value and to interpret.</span></span>
<span id="cb40-21"><a href="pw-4.html#cb40-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-22"><a href="pw-4.html#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(EstimatedSalary <span class="sc">~</span>Purchased, <span class="at">data=</span>dataset))</span>
<span id="cb40-23"><a href="pw-4.html#cb40-23" aria-hidden="true" tabindex="-1"></a><span class="co"># What do you conclude ?</span></span>
<span id="cb40-24"><a href="pw-4.html#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Now another anova test for the variable Age</span></span>
<span id="cb40-25"><a href="pw-4.html#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(Age <span class="sc">~</span>Purchased, <span class="at">data=</span>dataset))</span>
<span id="cb40-26"><a href="pw-4.html#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="pw-4.html#cb40-27" aria-hidden="true" tabindex="-1"></a><span class="co"># There is a categorical variable in the dataset, which is Gender.</span></span>
<span id="cb40-28"><a href="pw-4.html#cb40-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Of course we cannot show a boxplot of Gender and Purchased.</span></span>
<span id="cb40-29"><a href="pw-4.html#cb40-29" aria-hidden="true" tabindex="-1"></a><span class="co"># But we can show a table, or a mosaic plot, both tell the same thing.</span></span>
<span id="cb40-30"><a href="pw-4.html#cb40-30" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(dataset<span class="sc">$</span>Gender,dataset<span class="sc">$</span>Purchased)</span>
<span id="cb40-31"><a href="pw-4.html#cb40-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Remark for the function table(), that</span></span>
<span id="cb40-32"><a href="pw-4.html#cb40-32" aria-hidden="true" tabindex="-1"></a><span class="co"># in lines we have the first argument, and in columns we have the second argument.</span></span>
<span id="cb40-33"><a href="pw-4.html#cb40-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Don&#39;t forget this when you use table() to show a confusion matrix!</span></span>
<span id="cb40-34"><a href="pw-4.html#cb40-34" aria-hidden="true" tabindex="-1"></a><span class="fu">mosaicplot</span>(<span class="sc">~</span> Purchased <span class="sc">+</span> Gender, <span class="at">data=</span>dataset,</span>
<span id="cb40-35"><a href="pw-4.html#cb40-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;MosaicPlot of two categorical variables: Puchased &amp; Gender&quot;</span>,</span>
<span id="cb40-36"><a href="pw-4.html#cb40-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">color =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">las =</span> <span class="dv">1</span>)</span>
<span id="cb40-37"><a href="pw-4.html#cb40-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-38"><a href="pw-4.html#cb40-38" aria-hidden="true" tabindex="-1"></a><span class="co"># since these 2 variables are categorical, we can apply</span></span>
<span id="cb40-39"><a href="pw-4.html#cb40-39" aria-hidden="true" tabindex="-1"></a><span class="co"># a Chi-square test. The null hypothesis is the independance between</span></span>
<span id="cb40-40"><a href="pw-4.html#cb40-40" aria-hidden="true" tabindex="-1"></a><span class="co"># these variables. You will notice that p-value = 0.4562 which is higher than 0.05 (5%)</span></span>
<span id="cb40-41"><a href="pw-4.html#cb40-41" aria-hidden="true" tabindex="-1"></a><span class="co"># so we cannot reject the null hypothesis. </span></span>
<span id="cb40-42"><a href="pw-4.html#cb40-42" aria-hidden="true" tabindex="-1"></a><span class="co"># conclusion: there is no dependance between Gender and Purchased (who</span></span>
<span id="cb40-43"><a href="pw-4.html#cb40-43" aria-hidden="true" tabindex="-1"></a><span class="co"># said that women buy more than men? hah!)</span></span>
<span id="cb40-44"><a href="pw-4.html#cb40-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-45"><a href="pw-4.html#cb40-45" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(dataset<span class="sc">$</span>Purchased, dataset<span class="sc">$</span>Gender)</span>
<span id="cb40-46"><a href="pw-4.html#cb40-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-47"><a href="pw-4.html#cb40-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Let&#39;s say we want to remove the first two columns as we are not going to use them.</span></span>
<span id="cb40-48"><a href="pw-4.html#cb40-48" aria-hidden="true" tabindex="-1"></a><span class="co"># But, we can in fact use a categorical variable as a predictor in logistic regression.</span></span>
<span id="cb40-49"><a href="pw-4.html#cb40-49" aria-hidden="true" tabindex="-1"></a><span class="co"># It will treat it the same way as in regression. Check Appendix C.</span></span>
<span id="cb40-50"><a href="pw-4.html#cb40-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Try it by yourself if you would like to.</span></span>
<span id="cb40-51"><a href="pw-4.html#cb40-51" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">=</span> dataset[<span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb40-52"><a href="pw-4.html#cb40-52" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dataset) <span class="co"># show the new structure of dataset</span></span>
<span id="cb40-53"><a href="pw-4.html#cb40-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-54"><a href="pw-4.html#cb40-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-55"><a href="pw-4.html#cb40-55" aria-hidden="true" tabindex="-1"></a><span class="co"># splitting the dataset into training and testing sets</span></span>
<span id="cb40-56"><a href="pw-4.html#cb40-56" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)</span>
<span id="cb40-57"><a href="pw-4.html#cb40-57" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># CHANGE THE VALUE OF SEED. PUT YOUR STUDENT&#39;S NUMBER INSTEAD OF 123.</span></span>
<span id="cb40-58"><a href="pw-4.html#cb40-58" aria-hidden="true" tabindex="-1"></a>split <span class="ot">=</span> <span class="fu">sample.split</span>(dataset<span class="sc">$</span>Purchased, <span class="at">SplitRatio =</span> <span class="fl">0.75</span>)</span>
<span id="cb40-59"><a href="pw-4.html#cb40-59" aria-hidden="true" tabindex="-1"></a>training_set <span class="ot">=</span> <span class="fu">subset</span>(dataset, split <span class="sc">==</span> <span class="cn">TRUE</span>)</span>
<span id="cb40-60"><a href="pw-4.html#cb40-60" aria-hidden="true" tabindex="-1"></a>test_set <span class="ot">=</span> <span class="fu">subset</span>(dataset, split <span class="sc">==</span> <span class="cn">FALSE</span>)</span>
<span id="cb40-61"><a href="pw-4.html#cb40-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-62"><a href="pw-4.html#cb40-62" aria-hidden="true" tabindex="-1"></a><span class="co"># scaling</span></span>
<span id="cb40-63"><a href="pw-4.html#cb40-63" aria-hidden="true" tabindex="-1"></a><span class="co"># So here, we have two continuous predictors, Age and EstimatedSalary.</span></span>
<span id="cb40-64"><a href="pw-4.html#cb40-64" aria-hidden="true" tabindex="-1"></a><span class="co"># There is a very big difference in their scales (units).</span></span>
<span id="cb40-65"><a href="pw-4.html#cb40-65" aria-hidden="true" tabindex="-1"></a><span class="co"># That&#39;s why we scale them. But it is not always necessary.</span></span>
<span id="cb40-66"><a href="pw-4.html#cb40-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-67"><a href="pw-4.html#cb40-67" aria-hidden="true" tabindex="-1"></a>training_set[<span class="sc">-</span><span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">scale</span>(training_set[<span class="sc">-</span><span class="dv">3</span>]) <span class="co">#only first two columns</span></span>
<span id="cb40-68"><a href="pw-4.html#cb40-68" aria-hidden="true" tabindex="-1"></a>test_set[<span class="sc">-</span><span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">scale</span>(test_set[<span class="sc">-</span><span class="dv">3</span>])</span>
<span id="cb40-69"><a href="pw-4.html#cb40-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-70"><a href="pw-4.html#cb40-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that, we replace the columns of Age and EstimatedSalary in the training and</span></span>
<span id="cb40-71"><a href="pw-4.html#cb40-71" aria-hidden="true" tabindex="-1"></a><span class="co"># test sets but their scaled versions. I noticed in a lot of reports that you scaled</span></span>
<span id="cb40-72"><a href="pw-4.html#cb40-72" aria-hidden="true" tabindex="-1"></a><span class="co"># but you did not do the replacing.</span></span>
<span id="cb40-73"><a href="pw-4.html#cb40-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Note too that if you do it column by column you will have a problem because </span></span>
<span id="cb40-74"><a href="pw-4.html#cb40-74" aria-hidden="true" tabindex="-1"></a><span class="co"># it will replace the column by a matrix, you need to retransform it to a vector then.</span></span>
<span id="cb40-75"><a href="pw-4.html#cb40-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Last note, to call the columns Age and EstimatedSalary we can it like I did or </span></span>
<span id="cb40-76"><a href="pw-4.html#cb40-76" aria-hidden="true" tabindex="-1"></a><span class="co"># training_set[c(1,2)] or training_set[,c(1,2)] or training_set[,c(&quot;Age&quot;,&quot;EstimatedSalary&quot;)]</span></span>
<span id="cb40-77"><a href="pw-4.html#cb40-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-78"><a href="pw-4.html#cb40-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-79"><a href="pw-4.html#cb40-79" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic regression</span></span>
<span id="cb40-80"><a href="pw-4.html#cb40-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-81"><a href="pw-4.html#cb40-81" aria-hidden="true" tabindex="-1"></a>classifier.logreg <span class="ot">&lt;-</span> <span class="fu">glm</span>(Purchased <span class="sc">~</span> Age <span class="sc">+</span> EstimatedSalary , <span class="at">family =</span> binomial, <span class="at">data=</span>training_set)</span>
<span id="cb40-82"><a href="pw-4.html#cb40-82" aria-hidden="true" tabindex="-1"></a>classifier.logreg</span>
<span id="cb40-83"><a href="pw-4.html#cb40-83" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(classifier.logreg)</span>
<span id="cb40-84"><a href="pw-4.html#cb40-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-85"><a href="pw-4.html#cb40-85" aria-hidden="true" tabindex="-1"></a><span class="co"># prediction</span></span>
<span id="cb40-86"><a href="pw-4.html#cb40-86" aria-hidden="true" tabindex="-1"></a>pred.glm <span class="ot">=</span> <span class="fu">predict</span>(classifier.logreg, <span class="at">newdata =</span> test_set[,<span class="sc">-</span><span class="dv">3</span>], <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb40-87"><a href="pw-4.html#cb40-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Do not forget to put type response. </span></span>
<span id="cb40-88"><a href="pw-4.html#cb40-88" aria-hidden="true" tabindex="-1"></a><span class="co"># By the way, you know what you get when you do not put it, right?</span></span>
<span id="cb40-89"><a href="pw-4.html#cb40-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-90"><a href="pw-4.html#cb40-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Now let&#39;s assign observations to classes with respect to the probabilities</span></span>
<span id="cb40-91"><a href="pw-4.html#cb40-91" aria-hidden="true" tabindex="-1"></a>pred.glm_0_1 <span class="ot">=</span> <span class="fu">ifelse</span>(pred.glm <span class="sc">&gt;=</span> <span class="fl">0.5</span>, <span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb40-92"><a href="pw-4.html#cb40-92" aria-hidden="true" tabindex="-1"></a><span class="co"># I created a new vector, because we need the probabilities later for the ROC curve.</span></span>
<span id="cb40-93"><a href="pw-4.html#cb40-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-94"><a href="pw-4.html#cb40-94" aria-hidden="true" tabindex="-1"></a><span class="co"># show some values of the vectors</span></span>
<span id="cb40-95"><a href="pw-4.html#cb40-95" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred.glm)</span>
<span id="cb40-96"><a href="pw-4.html#cb40-96" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(pred.glm_0_1)</span>
<span id="cb40-97"><a href="pw-4.html#cb40-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-98"><a href="pw-4.html#cb40-98" aria-hidden="true" tabindex="-1"></a><span class="co"># confusion matrix</span></span>
<span id="cb40-99"><a href="pw-4.html#cb40-99" aria-hidden="true" tabindex="-1"></a>cm <span class="ot">=</span> <span class="fu">table</span>(test_set[,<span class="dv">3</span>], pred.glm_0_1)</span>
<span id="cb40-100"><a href="pw-4.html#cb40-100" aria-hidden="true" tabindex="-1"></a>cm</span>
<span id="cb40-101"><a href="pw-4.html#cb40-101" aria-hidden="true" tabindex="-1"></a><span class="co"># First line to store it into cm, second line to show the matrix! </span></span>
<span id="cb40-102"><a href="pw-4.html#cb40-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-103"><a href="pw-4.html#cb40-103" aria-hidden="true" tabindex="-1"></a><span class="co"># You remember my note about table() function and the order of the arguments?</span></span>
<span id="cb40-104"><a href="pw-4.html#cb40-104" aria-hidden="true" tabindex="-1"></a>cm <span class="ot">=</span> <span class="fu">table</span>(pred.glm_0_1, test_set[,<span class="dv">3</span>])</span>
<span id="cb40-105"><a href="pw-4.html#cb40-105" aria-hidden="true" tabindex="-1"></a>cm</span>
<span id="cb40-106"><a href="pw-4.html#cb40-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-107"><a href="pw-4.html#cb40-107" aria-hidden="true" tabindex="-1"></a><span class="co"># You can show the confusion matrix in a mosaic plot by the way</span></span>
<span id="cb40-108"><a href="pw-4.html#cb40-108" aria-hidden="true" tabindex="-1"></a><span class="fu">mosaicplot</span>(cm,<span class="at">col=</span><span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>,<span class="dv">2</span>)) <span class="co"># colors are random between 8 colors.</span></span>
<span id="cb40-109"><a href="pw-4.html#cb40-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-110"><a href="pw-4.html#cb40-110" aria-hidden="true" tabindex="-1"></a><span class="co"># ROC</span></span>
<span id="cb40-111"><a href="pw-4.html#cb40-111" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ROCR)</span>
<span id="cb40-112"><a href="pw-4.html#cb40-112" aria-hidden="true" tabindex="-1"></a>score <span class="ot">&lt;-</span> <span class="fu">prediction</span>(pred.glm,test_set[,<span class="dv">3</span>]) <span class="co"># we use the predicted probabilities not the 0 or 1</span></span>
<span id="cb40-113"><a href="pw-4.html#cb40-113" aria-hidden="true" tabindex="-1"></a><span class="fu">performance</span>(score,<span class="st">&quot;auc&quot;</span>) <span class="co"># y.values</span></span>
<span id="cb40-114"><a href="pw-4.html#cb40-114" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">performance</span>(score,<span class="st">&quot;tpr&quot;</span>,<span class="st">&quot;fpr&quot;</span>),<span class="at">col=</span><span class="st">&quot;green&quot;</span>)</span>
<span id="cb40-115"><a href="pw-4.html#cb40-115" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">lty=</span><span class="dv">8</span>)</span></code></pre></div>
<p>So now we have a logistic regression model stored in <code>classifier.logreg</code>. It is a model of <code>Purchased</code> in function of <code>Age</code> and <code>EstimatedSalary</code>. We will use this model to show the <strong>decision boundary</strong> in the next part of this PW. Then we will compare this model to other models obtained by Discriminant Analysis approaches.</p>
</div>
<div id="decision-boundary-of-logistic-regression" class="section level2 unnumbered hasAnchor">
<h2>Decision Boundary of Logistic Regression<a href="pw-4.html#decision-boundary-of-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now you are going to visualize the decision boundary for logistic regression.</p>
<div class="rmdinsight">
<ul>
<li>
Since the decision boundary of logistic regression is a linear
(<em>you know why right?</em>) and the dimension of the feature space is
2 (<code>Age</code> and <code>EstimatedSalary</code>), the decision
boundary in this 2-dimensional space is a line that separates the
predicted classes “0” and “1” (values of the response
<code>Purchased</code>).
</li>
<li>
For logistic regression, we predict <span class="math inline"><span class="math inline">\(y=1\)</span></span> if <span class="math inline"><span class="math inline">\(\beta^T X \geq 0\)</span></span> (right side of the line)
and <span class="math inline"><span class="math inline">\(y=0\)</span></span> if <span class="math inline"><span class="math inline">\(\beta^T X \lt 0\)</span></span> (left side of the line).
Where
</li>
</ul>
<p>
<span class="math display"><span class="math display">\[ \beta = \begin{pmatrix} \beta_0 \\
\beta_1 \\ \beta2 \end{pmatrix} \,\, \text{and} \,\, X = \begin{pmatrix}
  1 \\
  X_1 \\
  X_2
  \end{pmatrix}\]</span></span>
</p>
<p>
So we predict <span class="math inline"><span class="math inline">\(y=1\)</span></span> if <span class="math inline"><span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 \geq 0\)</span></span> which means that the equation of the decision boundary (a
line here) is <span class="math inline"><span class="math inline">\(X_2 = - \frac{\beta_1}{\beta_2}X_1 - \frac{\beta_0}{\beta_2}\)</span></span>
</p>
</div>
<p><strong>2.</strong> Plot the decision boundary obtained with logistic regression. In order to do so, calculate the intercept and the slope of the line presenting the decision boundary, then plot <code>EstimatedSalary</code> in function of <code>Age</code> (from the <code>test_set</code>) and add the line using <code>abline()</code>.</p>
<p><strong>3.</strong> In order to verify that your line (decision boundary) is well plotted, color the points on the last Figure with respect to the predicted response.</p>
<p><strong>Hints</strong>:</p>
<ul>
<li>If your predictions are stored in <code>y_pred</code>, you can do it using <code>bg = ifelse(y_pred == 1, 'color1', 'color2')</code>, and precise the argument <code>pch</code> to be 21 (you can choose pch to be a value between 21 and 25, try it).</li>
<li>Then, add the line using <code>abline()</code>, put the line width = 2 to make it more visible. Do not forget to title the Figure).</li>
</ul>
<p><strong>4.</strong> Now make the same plot but color the points with respect to their real labels (the variable <code>Purchased</code>). From this figure, count the number of the false positive predictions and compare it to the value obtained in the confusion matrix.</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level2 unnumbered hasAnchor">
<h2>Linear Discriminant Analysis (LDA)<a href="pw-4.html#linear-discriminant-analysis-lda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us apply linear discriminant analysis (LDA) now. First we will make use of the <code>lda()</code> function in the package <code>MASS</code>. Second, you are going to create the model and predict the classes by yourself without using the <code>lda()</code> function. And we will visualize the decision boundary of LDA.</p>
<p><strong>5.</strong> Fit a LDA model of <code>Purchased</code> in function of <code>Age</code> and <code>EstimatedSalary</code>. Name the model <code>classifier.lda</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="pw-4.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb41-2"><a href="pw-4.html#cb41-2" aria-hidden="true" tabindex="-1"></a>classifier.lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Purchased<span class="sc">~</span>Age<span class="sc">+</span>EstimatedSalary, <span class="at">data=</span>training_set)</span></code></pre></div>
<p><strong>6.</strong> Call <code>classifier.lda</code> and understand what does it compute.</p>
<p><strong>Plus:</strong> If you enter the following you will be returned with a list of summary information concerning the computation:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="pw-4.html#cb42-1" aria-hidden="true" tabindex="-1"></a>classifier.lda<span class="sc">$</span>prior</span>
<span id="cb42-2"><a href="pw-4.html#cb42-2" aria-hidden="true" tabindex="-1"></a>classifier.lda<span class="sc">$</span>means</span></code></pre></div>
<p><strong>7.</strong> On the test set, predict the probability of purchasing the product by the users using the model <code>classifier.lda</code>. Remark that when we predict using LDA, we obtain a list instead of a matrix, do <code>str()</code> for your predictions to see what do you get.</p>
<p><strong>Remark</strong>: we get the predicted class here, without being obligated to round the predictions as we did for logistic regression.</p>
<p><strong>8.</strong> Compute the confusion matrix and compare the predictions results obtained by LDA to the ones obtained by logistic regression. What do you remark? (<strong>Hint</strong>: compare the accuracy)</p>
<p><strong>9.</strong> Now let us plot the decision boundary obtained with LDA. You saw in the course that decision boundary for LDA represent the set of values <span class="math inline">\(x\)</span> where <span class="math inline">\(\delta_k(x) = \delta_c(x)\)</span>. Recall that
<span class="math display">\[ \delta_k(X) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}  \mu_k + \log \pi_k \]</span></p>
<p>Here in our case, we have 2 classes (<span class="math inline">\(K=2\)</span>) and 2 predictors (<span class="math inline">\(p=2\)</span>). So the decision boundary (which is linear in the case of LDA, and line in our case since <span class="math inline">\(p=2\)</span>) will verify the equation <span class="math inline">\(\delta_0(x) = \delta_1(x)\)</span> Since we have two classes “0” and “1”. In the case of LDA this leads to linear boundary and is easy to be plotted. But in more complicated cases it is difficult to manually simplify the equations and plot the decision boundary. Anyway, there is a smart method to plot (but a little bit costy) the decision boundary in <code>R</code> using the function <code>contour()</code>, the corresponding code is the following (you must adapt it and use it to plot your decision boundary):</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="pw-4.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a grid corresponding to the scales of Age and EstimatedSalary</span></span>
<span id="cb43-2"><a href="pw-4.html#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and fill this grid with lot of points</span></span>
<span id="cb43-3"><a href="pw-4.html#cb43-3" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="fu">min</span>(training_set[, <span class="dv">1</span>]) <span class="sc">-</span> <span class="dv">1</span>, <span class="fu">max</span>(training_set[, <span class="dv">1</span>]) <span class="sc">+</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb43-4"><a href="pw-4.html#cb43-4" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="fu">min</span>(training_set[, <span class="dv">2</span>]) <span class="sc">-</span> <span class="dv">1</span>, <span class="fu">max</span>(training_set[, <span class="dv">2</span>]) <span class="sc">+</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb43-5"><a href="pw-4.html#cb43-5" aria-hidden="true" tabindex="-1"></a>grid_set <span class="ot">=</span> <span class="fu">expand.grid</span>(X1, X2)</span>
<span id="cb43-6"><a href="pw-4.html#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Adapt the variable names</span></span>
<span id="cb43-7"><a href="pw-4.html#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(grid_set) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&#39;Age&#39;</span>, <span class="st">&#39;EstimatedSalary&#39;</span>)</span>
<span id="cb43-8"><a href="pw-4.html#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="pw-4.html#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># plot &#39;Estimated Salary&#39; ~ &#39;Age&#39;</span></span>
<span id="cb43-10"><a href="pw-4.html#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(test_set[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb43-11"><a href="pw-4.html#cb43-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&#39;Decision Boundary LDA&#39;</span>,</span>
<span id="cb43-12"><a href="pw-4.html#cb43-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&#39;Age&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;Estimated Salary&#39;</span>,</span>
<span id="cb43-13"><a href="pw-4.html#cb43-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">range</span>(X1), <span class="at">ylim =</span> <span class="fu">range</span>(X2))</span>
<span id="cb43-14"><a href="pw-4.html#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="pw-4.html#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="co"># color the plotted points with their real label (class)</span></span>
<span id="cb43-16"><a href="pw-4.html#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(test_set[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">bg =</span> <span class="fu">ifelse</span>(test_set[, <span class="dv">3</span>] <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&#39;green4&#39;</span>, <span class="st">&#39;red3&#39;</span>))</span>
<span id="cb43-17"><a href="pw-4.html#cb43-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-18"><a href="pw-4.html#cb43-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the points of the grid, this will take some time</span></span>
<span id="cb43-19"><a href="pw-4.html#cb43-19" aria-hidden="true" tabindex="-1"></a>pred_grid <span class="ot">=</span> <span class="fu">predict</span>(classifier.lda, <span class="at">newdata =</span> grid_set)<span class="sc">$</span>class</span>
<span id="cb43-20"><a href="pw-4.html#cb43-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-21"><a href="pw-4.html#cb43-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the predictions by a contour</span></span>
<span id="cb43-22"><a href="pw-4.html#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(X1, X2, <span class="fu">matrix</span>(<span class="fu">as.numeric</span>(pred_grid), <span class="fu">length</span>(X1), <span class="fu">length</span>(X2)), <span class="at">add =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</div>
<div id="lda-from-scratch" class="section level2 unnumbered hasAnchor">
<h2>LDA from scratch<a href="pw-4.html#lda-from-scratch" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>10.</strong> Now let us build a LDA model for our data set without using the <code>lda()</code> function. You are free to do it by creating a function or without creating one. Go back to question <strong>6</strong> and see what did you obtain by using <code>lda()</code>. It computes the prior probability of group membership and the estimated group means for each of the two groups. Additional information that is not provided, but may be important, is the single covariance matrix that is being used for the various groupings.</p>
<div class="rmdinsight">
<p>
In LDA, we compute for every observation <span class="math inline"><span class="math inline">\(x\)</span></span> its discriminant score <span class="math inline"><span class="math inline">\(\delta_k(x)\)</span></span>. Then we attribute <span class="math inline"><span class="math inline">\(x\)</span></span> to the class that has the highest <span class="math inline"><span class="math inline">\(\delta\)</span></span>. Recall that
</p>
<p>
<span class="math display"><span class="math display">\[\delta_k(x) = x^T \Sigma^{-1} \mu_k -
\frac{1}{2} \mu_k^T \Sigma^{-1}  \mu_k + \log \pi_k\]</span></span>
</p>
<p>
So to compute <span class="math inline"><span class="math inline">\(\delta_k(x)\)</span></span> we
need to estimate <span class="math inline"><span class="math inline">\(\pi_k\)</span></span>, <span class="math inline"><span class="math inline">\(\mu_k\)</span></span> and <span class="math inline"><span class="math inline">\(\Sigma\)</span></span>.
</p>
<p>
Note that <span class="math display"><span class="math display">\[x=\begin{pmatrix}
            X_1 \\
            X_2
            \end{pmatrix}\]</span></span> and here <span class="math inline"><span class="math inline">\(X_1\)</span></span>=<code>Age</code> and <span class="math inline"><span class="math inline">\(X_2\)</span></span>=<code>EstimatedSalary</code>.
</p>
</div>
<p>So let us do it step by step, first we will do the estimates:</p>
<p><strong>10.1</strong> Subset the training set into two sets: <code>class0</code> where <code>Purchased = 0</code> and <code>class1</code> where <code>Purchased = 1</code>).</p>
<p><strong>10.2</strong> Compute <span class="math inline">\(\pi_0\)</span> and <span class="math inline">\(\pi_1\)</span>.</p>
<p><span class="math display">\[\pi_i = N_i / N, \,\, \text{where} \,\, N_i \,\, \text{is the number of data points in group } i\]</span></p>
<p><strong>10.3</strong> Compute <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span>.
<span class="math display">\[\mu_0 = \begin{pmatrix}
   \mu_0(X_1) \\
   \mu_0(X_2)
   \end{pmatrix} \,\, \text{and} \,\, \mu_1 = \begin{pmatrix}
   \mu_1(X_1) \\
   \mu_1(X_2)
   \end{pmatrix}\]</span></p>
<p>where, for example, <span class="math inline">\(\mu_0(X_1)\)</span> is the mean of the variable <span class="math inline">\(X_1\)</span> in the group <span class="math inline">\(0\)</span> (the subset <code>class0</code>).</p>
<p><strong>10.4</strong> Compute <span class="math inline">\(\Sigma\)</span>. In the case of two classes like here, it is computed by calculating the following:</p>
<p><span class="math display">\[\Sigma = \frac{(N_0-1)\Sigma_0 + (N_1-1)\Sigma_1}{N_0+N_1-2}\]</span></p>
<p>where <span class="math inline">\(\Sigma_i\)</span> is the estimated covariance matrix for specific group <span class="math inline">\(i\)</span>.</p>
<p><strong>Remark:</strong> Recall that in LDA we use the same <span class="math inline">\(\Sigma\)</span>. But in QDA we do not.</p>
<p><strong>10.5.</strong> Now that we have computed all the needed estimates, we can calculate <span class="math inline">\(\delta_0(x)\)</span> and <span class="math inline">\(\delta_1(x)\)</span> for any observation <span class="math inline">\(x\)</span>. And we will attribute <span class="math inline">\(x\)</span> to the class with the highest <span class="math inline">\(\delta\)</span>. First, try it for <span class="math inline">\(x\)</span> where <span class="math inline">\(x^T=(1,1.5)\)</span>, what is class prediction for this spesific <span class="math inline">\(x\)</span>?</p>
<p><strong>10.6.</strong> Compute the discriminant scores <span class="math inline">\(\delta\)</span> for the test set (a matrix <span class="math inline">\(100\times 2\)</span>), predict the classes and compare your results with the results obtained with the <code>lda()</code> function.</p>
</div>
<div id="quadratic-discriminant-analysis-qda-1" class="section level2 unnumbered hasAnchor">
<h2>Quadratic Discriminant Analysis (QDA)<a href="pw-4.html#quadratic-discriminant-analysis-qda-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Training and assessing a QDA model in <code>R</code> is very similar in syntax to training and assessing a LDA model. The only difference is in the function name <code>qda()</code></p>
<p><strong>11.</strong> Fit a QDA model of <code>Purchased</code> in function of <code>Age</code> and <code>EstimatedSalary</code>. Name the model <code>classifier.qda</code>.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="pw-4.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># qda() is a function of library(MASS)</span></span>
<span id="cb44-2"><a href="pw-4.html#cb44-2" aria-hidden="true" tabindex="-1"></a>classifier.qda <span class="ot">&lt;-</span> <span class="fu">qda</span>(Purchased<span class="sc">~</span>., <span class="at">data =</span> training_set)</span></code></pre></div>
<p><strong>12.</strong> Make predictions on the <code>test_set</code> using the QDA model <code>classifier.qda</code>. Show the confusion matrix and compare the results with the predictions obtained using the LDA model <code>classifier.lda</code>.</p>
<p><strong>13.</strong> Plot the decision boundary obtained with QDA. Color the points with the real labels.</p>
<!-- ## Naive Bayes {-} -->
<!-- If we assume that the features are independant in each class, we use a Naive Bayes classifier. Let us test it. -->
<!-- **14.** Fit a Naive Bayes model of `Purchased` in function of `Age` and `EstimatedSalary`. Name the model `classifier.naive`. Note that you can use the function `naiveBayes()` from package `e1071`. -->
<!-- ```{r eval=F, echo=T} -->
<!-- library(e1071) # install it if necessary -->
<!-- # (install it directly from the console) -->
<!-- classifier.naive <- naiveBayes(Purchased~., data=training_set) -->
<!-- # By the way, in these functions you can manually -->
<!-- # insert the features and the response, like this: -->
<!-- classifier.naive <- naiveBayes(x=training_set[-3],y=training_set$Purchased) -->
<!-- ``` -->
<!-- **15.** Make predictions on the `test_set` using `classifier.naive`. Show the computation matrix. Do you obtain better results than in LDA and QDA ? -->
<!-- ```{r eval=F, echo=F} -->
<!-- pred.naive = predict(classifier.naive, test_set[-3]) -->
<!-- cm.naive = table(pred.naive$class,test_set[,3]) -->
<!-- ``` -->
</div>
<div id="comparison" class="section level2 unnumbered hasAnchor">
<h2>Comparison<a href="pw-4.html#comparison" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>14.</strong> In order to compare the methods we used, plot on the same Figure the ROC curve for each classifier we fitted and compare the correspondant AUC. What was the best model for this dataset? Can you justify it?</p>
<p><strong>Remark:</strong>
If you use the <code>ROCR</code> package:</p>
<ul>
<li>For Logistic regression, use the predicted probabilities in the <code>prediction()</code> (and not the round values “0” or “1”).</li>
<li>For LDA and QDA, put <code>pred.lda$posterior[,2]</code> in the <code>prediction()</code> function (those are the posterior probabilities that observations belong to class “1”).</li>
</ul>
<p class="text-right">
◼
</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-discriminant-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-decision-trees-random-forests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
