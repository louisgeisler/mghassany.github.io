<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Logistic Regression | Machine Learning</title>
  <meta name="description" content="3 Logistic Regression | Machine Learning course" />
  <meta name="generator" content="bookdown 0.24.10 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Logistic Regression | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="3 Logistic Regression | Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Logistic Regression | Machine Learning" />
  
  <meta name="twitter:description" content="3 Logistic Regression | Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2022-03-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pw-2.html"/>
<link rel="next" href="pw-3.html"/>
<script src="book_assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="book_assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="book_assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="book_assets/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />
<script src="book_assets/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="book_assets/plotly-binding-4.10.0/plotly.js"></script>
<script src="book_assets/typedarray-0.1/typedarray.min.js"></script>
<link href="book_assets/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="book_assets/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="book_assets/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="book_assets/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_new.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li><a href="index.html#welcome">Welcome<span></span></a>
<ul>
<li><a href="index.html#course-overview">Course Overview<span></span></a></li>
<li><a href="index.html#course-schedule">Course Schedule<span></span></a></li>
</ul></li>
<li><a href="introduction.html#introduction">Introduction<span></span></a>
<ul>
<li><a href="introduction.html#what-is-machine-learning">What is Machine Learning ?<span></span></a></li>
<li><a href="introduction.html#supervised-learning">Supervised Learning<span></span></a></li>
<li><a href="introduction.html#unsupervised-learning">Unsupervised Learning<span></span></a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning<span></span></b></span></li>
<li class="part"><span><b>Regression<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="1-linear-regression.html"><a href="1-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="1-linear-regression.html"><a href="1-linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="1-linear-regression.html"><a href="1-linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="1-linear-regression.html"><a href="1-linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?<span></span></a>
<ul>
<li><a href="1-linear-regression.html#prediction">Prediction<span></span></a></li>
<li><a href="1-linear-regression.html#inference">Inference<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1-linear-regression.html"><a href="1-linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression Model<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="1-linear-regression.html"><a href="1-linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.5</b> Estimating the Coefficients<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="1-linear-regression.html"><a href="1-linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.6</b> Assessing the Accuracy of the Coefficient Estimates<span></span></a>
<ul>
<li><a href="1-linear-regression.html#hypothesis-testing">Hypothesis testing<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="1-linear-regression.html"><a href="1-linear-regression.html#anova-and-model-fit"><i class="fa fa-check"></i><b>1.7</b> ANOVA and model fit<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="1-linear-regression.html"><a href="1-linear-regression.html#anova"><i class="fa fa-check"></i><b>1.7.1</b> ANOVA<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="1-linear-regression.html"><a href="1-linear-regression.html#the-r2-statistic"><i class="fa fa-check"></i><b>1.7.2</b> The <span class="math inline">\(R^2\)</span> Statistic<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="practical-work-1.html#practical-work-1">Practical Work 1<span></span></a>
<ul>
<li class="chapter" data-level="1.8" data-path="practical-work-1.html"><a href="practical-work-1.html"><i class="fa fa-check"></i><b>1.8</b> Some <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> basics<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="practical-work-1.html"><a href="practical-work-1.html#basic-commands"><i class="fa fa-check"></i><b>1.8.1</b> Basic Commands<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="practical-work-1.html"><a href="practical-work-1.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="practical-work-1.html"><a href="practical-work-1.html#matrices-data-frames-and-lists"><i class="fa fa-check"></i><b>1.8.3</b> Matrices, data frames and lists<span></span></a></li>
<li class="chapter" data-level="1.8.4" data-path="practical-work-1.html"><a href="practical-work-1.html#graphics"><i class="fa fa-check"></i><b>1.8.4</b> Graphics<span></span></a></li>
<li class="chapter" data-level="1.8.5" data-path="practical-work-1.html"><a href="practical-work-1.html#distributions"><i class="fa fa-check"></i><b>1.8.5</b> Distributions<span></span></a></li>
<li class="chapter" data-level="1.8.6" data-path="practical-work-1.html"><a href="practical-work-1.html#working-directory"><i class="fa fa-check"></i><b>1.8.6</b> Working directory<span></span></a></li>
<li class="chapter" data-level="1.8.7" data-path="practical-work-1.html"><a href="practical-work-1.html#loading-data"><i class="fa fa-check"></i><b>1.8.7</b> Loading Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="practical-work-1.html"><a href="practical-work-1.html#regression"><i class="fa fa-check"></i><b>1.9</b> Regression<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="practical-work-1.html"><a href="practical-work-1.html#the-lm-function"><i class="fa fa-check"></i><b>1.9.1</b> The <code>lm</code> function<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="practical-work-1.html"><a href="practical-work-1.html#boston"><i class="fa fa-check"></i><b>1.9.2</b> Predicting House Value: Boston dataset<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#other-consid"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-multiple-linear-regression.html"><a href="2-multiple-linear-regression.html#how-to-select-the-best-performing-model"><i class="fa fa-check"></i><b>2.4</b> How to select the best performing model<span></span></a>
<ul>
<li><a href="2-multiple-linear-regression.html#use-the-adjusted-r_adj2-for-multivariate-models">Use the Adjusted <span class="math inline">\(R_{adj}^2\)</span> for multivariate models<span></span></a></li>
<li><a href="2-multiple-linear-regression.html#have-a-look-at-the-residuals-or-error-terms">Have a look at the residuals or error terms<span></span></a></li>
<li><a href="2-multiple-linear-regression.html#histogram-of-residuals">Histogram of residuals<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-2.html#pw-2">PW 2<span></span></a>
<ul>
<li><a href="pw-2.html#multiple-linear-regression-1">Multiple Linear Regression<span></span></a></li>
<li><a href="pw-2.html#reporting">Reporting<span></span></a></li>
</ul></li>
<li class="part"><span><b>Classification<span></span></b></span></li>
<li class="chapter" data-level="3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logistic-regression-1"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#prediction-1"><i class="fa fa-check"></i><b>3.2.3</b> Prediction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logreg-examps"><i class="fa fa-check"></i><b>3.4</b> Example<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3-logistic-regression.html"><a href="3-logistic-regression.html#logreg-examps-challenger"><i class="fa fa-check"></i><b>3.4.1</b> Case study: <em>The Challenger disaster</em><span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-3.html#pw-3">PW 3<span></span></a>
<ul>
<li><a href="pw-3.html#social-networks-ads">Social Networks Ads<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html"><i class="fa fa-check"></i><b>4</b> Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#bayes-theorem"><i class="fa fa-check"></i><b>4.2</b> Bayes’ Theorem<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-for-p1"><i class="fa fa-check"></i><b>4.3</b> LDA for <span class="math inline">\(p=1\)</span><span></span></a></li>
<li class="chapter" data-level="4.4" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#estimating-the-parameters"><i class="fa fa-check"></i><b>4.4</b> Estimating the parameters<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-for-p-1"><i class="fa fa-check"></i><b>4.5</b> LDA for <span class="math inline">\(p &gt; 1\)</span><span></span></a></li>
<li class="chapter" data-level="4.6" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#making-predictions"><i class="fa fa-check"></i><b>4.6</b> Making predictions<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#other-forms-of-discriminant-analysis"><i class="fa fa-check"></i><b>4.7</b> Other forms of Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>4.7.1</b> Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>4.7.2</b> Naive Bayes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4-discriminant-analysis.html"><a href="4-discriminant-analysis.html#lda-vs-logistic-regression"><i class="fa fa-check"></i><b>4.8</b> LDA vs Logistic Regression<span></span></a></li>
</ul></li>
<li><a href="pw-4.html#pw-4">PW 4<span></span></a>
<ul>
<li><a href="pw-4.html#logistic-regression-2">Logistic Regression<span></span></a></li>
<li><a href="pw-4.html#decision-boundary-of-logistic-regression">Decision Boundary of Logistic Regression<span></span></a></li>
<li><a href="pw-4.html#linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)<span></span></a></li>
<li><a href="pw-4.html#lda-from-scratch">LDA from scratch<span></span></a></li>
<li><a href="pw-4.html#quadratic-discriminant-analysis-qda-1">Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li><a href="pw-4.html#comparison">Comparison<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-decision-trees-random-forests.html"><a href="5-decision-trees-random-forests.html"><i class="fa fa-check"></i><b>5</b> Decision Trees &amp; Random Forests<span></span></a>
<ul>
<li><a href="5-decision-trees-random-forests.html#the-basics-of-decision-trees">The Basics of Decision Trees<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#classification-trees">Classification Trees<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#bagging-random-forests">Bagging &amp; Random Forests<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#boosting">Boosting<span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#trees-in-r">Trees in <code>R</code><span></span></a></li>
<li><a href="5-decision-trees-random-forests.html#random-forests---the-first-choice-method-for-every-data-analysis">Random forests - the first-choice method for every data analysis?<span></span></a></li>
</ul></li>
<li><a href="pw-5.html#pw-5">PW 5<span></span></a>
<ul>
<li><a href="pw-5.html#regression-trees">Regression Trees<span></span></a>
<ul>
<li><a href="pw-5.html#single-tree">Single tree<span></span></a></li>
<li><a href="pw-5.html#bagging">Bagging<span></span></a></li>
<li><a href="pw-5.html#random-forests">Random Forests<span></span></a></li>
<li><a href="pw-5.html#boosting-1">Boosting<span></span></a></li>
<li><a href="pw-5.html#comparison-1">Comparison<span></span></a></li>
</ul></li>
<li><a href="pw-5.html#classification-trees-1">Classification Trees<span></span></a>
<ul>
<li><a href="pw-5.html#the-spam-dataset">The Spam dataset<span></span></a></li>
<li><a href="pw-5.html#extra-tuning">Extra: Tuning<span></span></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Dimensionality Reduction<span></span></b></span></li>
<li class="chapter" data-level="6" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html"><i class="fa fa-check"></i><b>6</b> Principal Components Analysis<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#principal-components"><i class="fa fa-check"></i><b>6.2</b> Principal Components<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#notations-and-procedure">Notations and Procedure<span></span></a></li>
<li><a href="6-principal-components-analysis.html#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span><span></span></a></li>
<li><a href="6-principal-components-analysis.html#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span><span></span></a></li>
<li><a href="6-principal-components-analysis.html#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span><span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#how-do-we-find-the-coefficients"><i class="fa fa-check"></i><b>6.3</b> How do we find the coefficients?<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#why-it-may-be-possible-to-reduce-dimensions">Why It May Be Possible to Reduce Dimensions<span></span></a></li>
<li><a href="6-principal-components-analysis.html#procedure">Procedure<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#standardization-of-the-features"><i class="fa fa-check"></i><b>6.4</b> Standardization of the features<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#projection-of-the-data"><i class="fa fa-check"></i><b>6.5</b> Projection of the data<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#scores">Scores<span></span></a></li>
<li><a href="6-principal-components-analysis.html#visualization">Visualization<span></span></a></li>
<li><a href="6-principal-components-analysis.html#extra">Extra<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-principal-components-analysis.html"><a href="6-principal-components-analysis.html#case-study"><i class="fa fa-check"></i><b>6.6</b> Case study<span></span></a>
<ul>
<li><a href="6-principal-components-analysis.html#employement-in-european-countries-in-the-late-70s">Employement in European countries in the late 70s<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-6.html#pw-6">PW 6<span></span></a>
<ul>
<li><a href="pw-6.html#the-iris-dataset">The Iris Dataset<span></span></a></li>
<li><a href="pw-6.html#loading-data-1">Loading Data<span></span></a></li>
<li><a href="pw-6.html#exploratory-analysis">Exploratory analysis<span></span></a></li>
<li><a href="pw-6.html#pca-using-princomp">PCA using <code>princomp()</code><span></span></a></li>
<li><a href="pw-6.html#deeper-pca-using-factoextra-package">Deeper PCA using <code>factoextra</code> package<span></span></a></li>
<li><a href="pw-6.html#step-by-step-pca">Step-by-step PCA<span></span></a></li>
</ul></li>
<li class="part"><span><b>III Unsupervised Learning<span></span></b></span></li>
<li class="chapter" data-level="7" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html"><i class="fa fa-check"></i><b>7</b> Kmeans &amp; Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#unsupervised-learning-1"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#clustering"><i class="fa fa-check"></i><b>7.2</b> Clustering<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#introduction-4"><i class="fa fa-check"></i><b>7.3</b> Introduction<span></span></a>
<ul>
<li><a href="7-kmeans-hierarchical-clustering.html#hard-clustering">Hard clustering<span></span></a></li>
<li><a href="7-kmeans-hierarchical-clustering.html#fuzzy-clustering">Fuzzy clustering<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#k-means"><i class="fa fa-check"></i><b>7.4</b> <span class="math inline">\(k\)</span>-Means<span></span></a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#k-means-in"><i class="fa fa-check"></i><b>7.4.1</b> <span class="math inline">\(k\)</span>-means in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg><span></span></a></li>
<li class="chapter" data-level="7.4.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#cluster-validity-choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>7.4.2</b> Cluster Validity, Choosing the Number of Clusters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>7.5</b> Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#dendrogram"><i class="fa fa-check"></i><b>7.5.1</b> Dendrogram<span></span></a></li>
<li class="chapter" data-level="7.5.2" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#the-hierarchical-clustering-algorithm"><i class="fa fa-check"></i><b>7.5.2</b> The Hierarchical Clustering Algorithm<span></span></a></li>
<li class="chapter" data-level="7.5.3" data-path="7-kmeans-hierarchical-clustering.html"><a href="7-kmeans-hierarchical-clustering.html#hierarchical-clustering-in"><i class="fa fa-check"></i><b>7.5.3</b> Hierarchical clustering in <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg><span></span></a></li>
</ul></li>
</ul></li>
<li><a href="pw-7.html#pw-7">PW 7<span></span></a>
<ul>
<li><a href="pw-7.html#reporting-1">Reporting<span></span></a>
<ul>
<li><a href="pw-7.html#markdown">Markdown<span></span></a></li>
<li><a href="pw-7.html#r-markdown">R Markdown<span></span></a></li>
<li><a href="pw-7.html#the-report-to-be-submitted">The report to be submitted<span></span></a></li>
</ul></li>
<li><a href="pw-7.html#k-means-clustering"><span class="math inline">\(k\)</span>-means clustering<span></span></a>
<ul>
<li><a href="pw-7.html#pointscards"><code>pointsCards</code><span></span></a></li>
<li><a href="pw-7.html#ligue-1"><code>Ligue 1</code><span></span></a></li>
<li><a href="pw-7.html#pca"><code>PCA</code><span></span></a></li>
<li><a href="pw-7.html#implementing-k-means"><code>Implementing k-means</code><span></span></a></li>
</ul></li>
<li><a href="pw-7.html#hierarchical-clustering-1">Hierarchical clustering<span></span></a>
<ul>
<li><a href="pw-7.html#distances-dist">Distances <code>dist()</code><span></span></a></li>
<li><a href="pw-7.html#dendrogram-hclust">Dendrogram <code>hclust()</code><span></span></a></li>
<li><a href="pw-7.html#hierarchical-clustering-on-iris-dataset">Hierarchical clustering on Iris dataset<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html"><i class="fa fa-check"></i><b>8</b> Gaussian Mixture Models &amp; EM<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#the-gaussian-distribution"><i class="fa fa-check"></i><b>8.1</b> The Gaussian distribution<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#mixture-of-gaussians"><i class="fa fa-check"></i><b>8.2</b> Mixture of Gaussians<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="8-gaussian-mixture-models-em.html"><a href="8-gaussian-mixture-models-em.html#em-for-gaussian-mixtures"><i class="fa fa-check"></i><b>8.3</b> EM for Gaussian Mixtures<span></span></a></li>
</ul></li>
<li><a href="pw-8.html#pw-8">PW 8<span></span></a>
<ul>
<li><a href="pw-8.html#report-template">Report template<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="pw-8.html"><a href="pw-8.html"><i class="fa fa-check"></i><b>8.4</b> EM using <code>mclust</code><span></span></a>
<ul>
<li><a href="pw-8.html#gmm-vs-k-means">GMM vs <span class="math inline">\(k\)</span>-means<span></span></a></li>
<li><a href="pw-8.html#em-on-1d">EM on 1D<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="pw-8.html"><a href="pw-8.html#em-from-scratch"><i class="fa fa-check"></i><b>8.5</b> EM from scratch<span></span></a></li>
</ul></li>
<li class="part"><span><b>Hackathon<span></span></b></span></li>
<li><a href="hackathon.html#hackathon">Hackathon<span></span></a></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="A-final-grades.html"><a href="A-final-grades.html"><i class="fa fa-check"></i><b>A</b> Final Grades<span></span></a></li>
<li class="chapter" data-level="B" data-path="B-app-introRStudio.html"><a href="B-app-introRStudio.html"><i class="fa fa-check"></i><b>B</b> Introduction to <code>RStudio</code><span></span></a></li>
<li class="chapter" data-level="C" data-path="C-app-ht.html"><a href="C-app-ht.html"><i class="fa fa-check"></i><b>C</b> Review on hypothesis testing<span></span></a></li>
<li class="chapter" data-level="D" data-path="D-use-qual.html"><a href="D-use-qual.html"><i class="fa fa-check"></i><b>D</b> Use of qualitative predictors<span></span></a></li>
<li class="chapter" data-level="E" data-path="E-model-selection.html"><a href="E-model-selection.html"><i class="fa fa-check"></i><b>E</b> Model Selection<span></span></a>
<ul>
<li><a href="E-model-selection.html#linear-model-selection-and-best-subset-selection">Linear Model Selection and Best Subset Selection<span></span></a></li>
<li><a href="E-model-selection.html#forward-stepwise-selection">Forward Stepwise Selection<span></span></a></li>
<li><a href="E-model-selection.html#backward-stepwise-selection">Backward Stepwise Selection<span></span></a></li>
<li><a href="E-model-selection.html#estimating-test-error-using-mallows-cp-aic-bic-adjusted-r-squared">Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared<span></span></a></li>
<li><a href="E-model-selection.html#estimating-test-error-using-cross-validation">Estimating Test Error Using Cross-Validation<span></span></a></li>
<li><a href="E-model-selection.html#examples">Examples<span></span></a>
<ul>
<li><a href="E-model-selection.html#best-subset-selection">Best Subset Selection<span></span></a></li>
<li><a href="E-model-selection.html#forward-stepwise-selection-and-model-selection-using-validation-set">Forward Stepwise Selection and Model Selection Using Validation Set<span></span></a></li>
<li><a href="E-model-selection.html#model-selection-using-cross-validation">Model Selection Using Cross-Validation<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="F" data-path="F-references-and-credits.html"><a href="F-references-and-credits.html"><i class="fa fa-check"></i><b>F</b> References and Credits<span></span></a></li>
<li class="chapter" data-level="G" data-path="G-other-references.html"><a href="G-other-references.html"><i class="fa fa-check"></i><b>G</b> Other References<span></span></a></li>
<li><a href="main-references-credits.html#main-references-credits">Main References &amp; Credits<span></span></a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Logistic Regression<a href="3-logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction<a href="3-logistic-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapters we discussed the linear regression model, which assumes that the response variable <span class="math inline">\(Y\)</span> is quantitative. But in many situations, the response variable is instead qualitative (categorical). For example, eye color is qualitative, taking on values blue, brown, or green.</p>
<p>The process for predicting qualitative responses is known as <strong><em>classification</em></strong>.</p>
<p>Given a feature vector <span class="math inline">\(X\)</span> and a qualitative response <span class="math inline">\(Y\)</span> taking values in the set <span class="math inline">\(\mathcal{C}\)</span>, the classification task is to build a function <span class="math inline">\(C(X)\)</span> that takes as input the feature vector <span class="math inline">\(X\)</span> and predicts its value for <span class="math inline">\(Y\)</span>; i.e. <span class="math inline">\(C(X) \in \mathcal{C}\)</span>. We are often more interested in estimating the probabilities that <span class="math inline">\(X\)</span> belongs to each category in <span class="math inline">\(\mathcal{C}\)</span>.</p>
<blockquote>
<p>If <span class="math inline">\(c\)</span> is a category (<span class="math inline">\(c \in \mathcal{C}\)</span>), by the probability that <span class="math inline">\(X\)</span> belongs to <span class="math inline">\(c\)</span> we mean <span class="math inline">\(p(X \in c) = \mathbb{P}(Y=c|X)\)</span>.</p>
</blockquote>
<p>In the binomial or binary logistic regression, the outcome can have only two possible types of values (e.g. “Yes” or “No”, “Success” or “Failure”). Multinomial logistic refers to cases where the outcome can have three or more possible types of values (e.g., “good” vs. “very good” vs. “best”). Generally outcome is coded as “0” and “1” in binary logistic regression.</p>
<center>
<iframe width="560" height="349" src="https://www.youtube.com/embed/SF_jZLUQndY" frameborder="0" allowfullscreen>
</iframe>
</center>
</div>
<div id="logistic-regression-1" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Logistic Regression<a href="3-logistic-regression.html#logistic-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a data set where the response falls into one of two categories, Yes or No. Rather than modeling the response <span class="math inline">\(Y\)</span> directly, logistic regression models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<div id="the-logistic-model" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> The Logistic Model<a href="3-logistic-regression.html#the-logistic-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us suppose the response has two categories and we use the generic 0/1 coding for the response. How should we model the relationship between <span class="math inline">\(p(X) = \mathbb{P}(Y = 1|X)\)</span> and <span class="math inline">\(X\)</span>?</p>
<p>The simplest situation is when <span class="math inline">\(Y\)</span> is <em>binary</em>: it can only take two values, codified for convenience as <span class="math inline">\(1\)</span> (success) and <span class="math inline">\(0\)</span> (failure).</p>
<p>More formally, a binary variable is known as a <em>Bernoulli variable</em>, which is the simplest non-trivial random variable. We say that <span class="math inline">\(Y\sim\mathrm{Ber}(p)\)</span>, <span class="math inline">\(0\leq p\leq1\)</span>, if <span class="math display">\[
Y=\left\{\begin{array}{ll}1,&amp;\text{with probability }p,\\0,&amp;\text{with probability }1-p,\end{array}\right.
\]</span> or, equivalently, if <span class="math inline">\(\mathbb{P}[Y=1]=p\)</span> and <span class="math inline">\(\mathbb{P}[Y=0]=1-p\)</span>, which can be written compactly as <span class="math display">\[\begin{aligned}
\mathbb{P}[Y=y]=p^y(1-p)^{1-y},\quad y=0,1.
\end{aligned}\]</span> Recall that a <em>binomial variable with size</em> <span class="math inline">\(n\)</span> and probability <span class="math inline">\(p\)</span>, <span class="math inline">\(\mathrm{Bi}(n,p)\)</span>, was obtained by adding <span class="math inline">\(n\)</span> independent <span class="math inline">\(\mathrm{Ber}(p)\)</span> (so <span class="math inline">\(\mathrm{Ber}(p)\)</span> is the same as <span class="math inline">\(\mathrm{Bi}(1,p)\)</span>).</p>
<div class="rmdinsight">
<p>
A Bernoulli variable <span class="math inline"><span class="math inline">\(Y\)</span></span> is
completely determined by <span class="math inline"><span class="math inline">\(p\)</span></span>.
<strong>So its mean and variance</strong>:
</p>
<ul>
<li>
<span class="math inline"><span class="math inline">\(\mathbb{E}[Y]=p\times1+(1-p)\times0=p\)</span></span>
</li>
<li>
<span class="math inline"><span class="math inline">\(\mathbb{V}\mathrm{ar}[Y]=p(1-p)\)</span></span>.
</li>
</ul>
<p>
In particular, recall that <span class="math inline"><span class="math inline">\(\mathbb{P}[Y=1]=\mathbb{E}[Y]=p\)</span></span>.
</p>
</div>
<p>Assume then that <span class="math inline">\(Y\)</span> is a binary/Bernoulli variable and that <span class="math inline">\(X\)</span> are predictors associated to them (no particular assumptions on them). The purpose in <em>logistic regression</em> is to estimate <span class="math display">\[
p(x)=\mathbb{P}[Y=1|X=x]=\mathbb{E}[Y|X=x],
\]</span> this is, how the probability of <span class="math inline">\(Y=1\)</span> is changing according to particular values, denoted by <span class="math inline">\(x\)</span>, of the random variables <span class="math inline">\(X\)</span>.</p>
<p><em>Why not linear regression?</em> A tempting possibility is to consider the model <span class="math display">\[
p(x)=\beta_0+\beta_1 x.
\]</span> However, such a model will run into problems inevitably: negative probabilities and probabilities larger than one (<span class="math inline">\(p(x) &lt; 0\)</span> for some values of <span class="math inline">\(X\)</span> and <span class="math inline">\(p(X) &gt; 1\)</span> for others). To avoid this problem, the solution is to consider a function to encapsulate the value of <span class="math inline">\(z=\beta_0+\beta_1 x\)</span>, in <span class="math inline">\(\mathbb{R}\)</span>, and map it to <span class="math inline">\([0,1]\)</span>. There are several alternatives to do so, based on distribution functions <span class="math inline">\(F:\mathbb{R}\longrightarrow[0,1]\)</span> that deliver <span class="math inline">\(y=F(z)\in[0,1]\)</span>. Many functions meet this description. In logistic regression, we use the <em>logistic function</em>,</p>
<p><span class="math display">\[ p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1+e^{\beta_0 + \beta_1 X}} \]</span></p>
<div class="rmdinsight">
<ul>
<li>
No matter what values <span class="math inline"><span class="math inline">\(\beta_0\)</span></span>,
<span class="math inline"><span class="math inline">\(\beta_1\)</span></span> or <span class="math inline"><span class="math inline">\(X\)</span></span> take, <span class="math inline"><span class="math inline">\(p(X)\)</span></span> will have values between 0 and
1.
</li>
<li>
The logistic function will always produce an <em>S-shaped</em>
curve.
</li>
<li>
The logistic <em>distribution</em> function is: <span class="math display"><span class="math display">\[F(z)=\mathrm{logistic}(z)=\frac{e^z}{1+e^z}=\frac{1}{1+e^{-z}}.\]</span></span>
</li>
</ul>
</div>
<p>After a bit of manipulation of the previous equation, we find that</p>
<p><span class="math display">\[ \frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X} \]</span></p>
<div class="rmdinsight">
<p>
The quantity <span class="math inline"><span class="math inline">\(p(X)/[1−p(X)]\)</span></span> is
called the <em>odds</em>, and can take on any value between <span class="math inline"><span class="math inline">\(0\)</span></span> and <span class="math inline"><span class="math inline">\(\infty\)</span></span>.
</p>
</div>
<p>By taking the logarithm of both sides of the equation, we arrive at<a href="main-references-credits.html#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
<p><span class="math display">\[ \log( \frac{p(X)}{1-p(X)} ) = \beta_0 + \beta_1 X \]</span></p>
<div class="rmdinsight">
<p>
The left-hand side is called the <em>log-odds</em> or <em>logit</em>.
We see that the logistic regression model has a logit that is linear in
X.
</p>
</div>
</div>
<div id="estimating-the-regression-coefficients-1" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Estimating the Regression Coefficients<a href="3-logistic-regression.html#estimating-the-regression-coefficients-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<center>
<iframe width="560" height="349" src="https://www.youtube.com/embed/0dhcIQ6bX8c" frameborder="0" allowfullscreen>
</iframe>
</center>
<p>We estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using the <em>Maximum Likelihood Estimation</em> method (MLE). The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of the response for each individual, corresponds as closely as possible to the individual’s observed response status (recall that the response <span class="math inline">\(Y\)</span> is categorical). The <em>likelihood function</em> is</p>
<p><span class="math display">\[ l(\beta_0,\beta_1) = \prod_{i=1}^n p(x_i)^{Y_i}(1-p(x_i))^{1-Y_i}. \]</span></p>
<p>This likelihood is <strong>the probability of the data based on the model</strong>. It gives the probability of the observed zeros and ones in the data. The estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen to <em>maximize</em> this likelihood function. The interpretation of the likelihood function is the following:</p>
<ul>
<li><span class="math inline">\(\prod_{i=1}^n\)</span> appears because the sample elements are assumed to be independent and we are computing the probability of observing the whole sample <span class="math inline">\((x_{1},y_1),\ldots,(x_{n},y_n)\)</span>. This probability is equal to the product of the <em>probabilities of observing each</em> <span class="math inline">\((x_{i},y_i)\)</span>.</li>
<li><span class="math inline">\(p(x_i)^{Y_i}(1-p(x_i))^{1-Y_i}\)</span> is the probability of observing <span class="math inline">\((x_{i},Y_i)\)</span>.</li>
</ul>
<div class="rmdinsight">
<p>
In the linear regression setting, the least squares approach is a
special case of maximum likelihood.
</p>
</div>
<p>We will not give mathematical details about the maximum likelihood and how to estimate the parameters. We will use <svg aria-hidden="true" role="img" viewBox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> to fit the logistic regression models (using <code>glm</code> function).</p>
<p>Use the following application (also available <a href="https://shinyserv.es/shiny/log-maximum-likelihood/" target="_blank">here <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M432,320H400a16,16,0,0,0-16,16V448H64V128H208a16,16,0,0,0,16-16V80a16,16,0,0,0-16-16H48A48,48,0,0,0,0,112V464a48,48,0,0,0,48,48H400a48,48,0,0,0,48-48V336A16,16,0,0,0,432,320ZM488,0h-128c-21.37,0-32.05,25.91-17,41l35.73,35.73L135,320.37a24,24,0,0,0,0,34L157.67,377a24,24,0,0,0,34,0L435.28,133.32,471,169c15,15,41,4.5,41-17V24A24,24,0,0,0,488,0Z"/></svg></a>) to see how the log-likelihood changes with respect to the values for <span class="math inline">\((\beta_0,\beta_1)\)</span> in three data patterns. The logistic regression fit and its dependence on <span class="math inline">\(\beta_0\)</span> (horizontal displacement) and <span class="math inline">\(\beta_1\)</span> (steepness of the curve). Recall the effect of the sign of <span class="math inline">\(\beta_1\)</span> in the curve: if positive, the logistic curve has an <span class="math inline">\(s\)</span> form; if negative, the form is a reflected <span class="math inline">\(s\)</span>.</p>
<iframe src="https://shinyserv.es/shiny/log-maximum-likelihood/?showcase=0" width="90%" height="900px" data-external="1">
</iframe>
<p>Note that the <strong>animation</strong> will not be displayed the first time it is browsed (The reason is because it is hosted at <code>https</code> websites with auto-signed SSL certificates). <strong>To see it</strong>, click on the link above. You will get a warning from your browser saying that <em>“Your connection is not private”</em>. Click in <em>“Advanced”</em> and <strong>allow an exception</strong> in your browser. The next time the animation will show up correctly.</p>
</div>
<div id="prediction-1" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Prediction<a href="3-logistic-regression.html#prediction-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<center>
<iframe width="560" height="349" src="https://www.youtube.com/embed/CBPZNizyiqA" frameborder="0" allowfullscreen>
</iframe>
</center>
<p><strong>Example</strong></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Coefficient</th>
<th>Std. error</th>
<th><span class="math inline">\(Z\)</span>-statistic</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Constant</td>
<td>-10.6513</td>
<td>0.3612</td>
<td>-29.5</td>
<td>&lt;0.0001</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X\)</span></td>
<td>0.0055</td>
<td>0.0002</td>
<td>24.9</td>
<td>&lt;0.0001</td>
</tr>
</tbody>
</table>
<p>In this example, <span class="math inline">\(\hat{\beta_0} = -10.6513\)</span> and <span class="math inline">\(\hat{\beta_1} = 0.0055\)</span>. It produces the blue curve that separates that data in the following figure,</p>
<p><img src="img/lr_example.png" /></p>
<p>As for prediction, we use the model built with the estimated parameters to predict probabilities. For example,</p>
<p>If <span class="math inline">\(X=1000\)</span>,</p>
<p><span class="math display">\[ \hat{p}(X) = \frac{e^{\hat{\beta_0} + \hat{\beta_1} X}}{1+e^{\hat{\beta_0} + \hat{\beta_1} X}} = \frac{e^{-10.6513+0.0055 \times 1000}}{1+e^{-10.6513+0.0055 \times 1000}} = 0.006\]</span></p>
<p>If <span class="math inline">\(X=2000\)</span>,</p>
<p><span class="math display">\[ \hat{p}(X) = \frac{e^{\hat{\beta_0} + \hat{\beta_1} X}}{1+e^{\hat{\beta_0} + \hat{\beta_1} X}} = \frac{e^{-10.6513+0.0055 \times 2000}}{1+e^{-10.6513+0.0055 \times 2000}} = 0.586\]</span></p>
</div>
</div>
<div id="multiple-logistic-regression" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Multiple Logistic Regression<a href="3-logistic-regression.html#multiple-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapters, we can generalize the simple logistic regression equation as follows:</p>
<p><span class="math display">\[ \log( \frac{p(X)}{1-p(X)} ) = \beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p\]</span></p>
<p>where <span class="math inline">\(X=(X_1,\ldots,X_p)\)</span> are <span class="math inline">\(p\)</span> predictors. The equation above can be rewritten as</p>
<p><span class="math display">\[ p(X) = \frac{e^{\beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p}}{1+e^{\beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p}} \]</span></p>
<p>Just as in the simple logistic regression we use the maximum likelihood method to estimate <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span>.</p>
<center>
<iframe width="560" height="349" src="https://www.youtube.com/embed/4DzxXEL-Vk4" frameborder="0" allowfullscreen>
</iframe>
</center>
</div>
<div id="logreg-examps" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Example<a href="3-logistic-regression.html#logreg-examps" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="logreg-examps-challenger" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Case study: <em>The Challenger disaster</em><a href="3-logistic-regression.html#logreg-examps-challenger" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>Challenger</em> disaster occurred on the 28th January of 1986, when the NASA Space Shuttle orbiter <em>Challenger</em> broke apart and disintegrated at 73 seconds into its flight, leading to the deaths of its seven crew members. The accident deeply shocked the US society, in part due to the attention the mission had received because of the presence of Christa McAuliffe, who would have been the first astronaut-teacher. Because of this, NASA TV broadcasted live the launch to US public schools, which resulted in millions of school children witnessing the accident. The accident had serious consequences for the NASA credibility and resulted in an interruption of 32 months in the shuttle program. The Presidential <em>Rogers Commission</em> (formed by astronaut Neil A. Armstrong and Nobel laureate Richard P. Feynman, among others) was created to investigate the disaster.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:video3"></span>
<iframe src="https://www.youtube.com/embed/fSTrmJtHLFU" width="70%" height="400px" data-external="1">
</iframe>
<p class="caption">
Figure 3.1: Challenger launch and posterior explosion, as broadcasted live by NBC in 28/01/1986.
</p>
</div>
<p>The Rogers Commission elaborated a report <span class="citation">(<a href="#ref-Roberts1986" role="doc-biblioref">Presidential Commission on the Space Shuttle Challenger Accident 1986</a>)</span> with all the findings. The commission determined that the disintegration began with the <strong>failure of an O-ring seal in the solid rocket motor due to the unusual cold temperatures (-0.6 Celsius degrees)</strong> during the launch. This failure produced a breach of burning gas through the solid rocket motor that compromised the whole shuttle structure, resulting in its disintegration due to the extreme aerodynamic forces. The <strong>problematic with O-rings was something known</strong>: the night before the launch, there was a three-hour teleconference between motor engineers and NASA management, discussing the effect of low temperature forecasted for the launch on the O-ring performance. The conclusion, influenced by Figure <a href="3-logistic-regression.html#fig:rogerts">3.2</a>a, was:</p>
<blockquote>
<p><strong>“Temperature data [are] not conclusive on predicting primary O-ring blowby.”</strong></p>
</blockquote>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rogerts"></span>
<img src="img/challenger.png" alt="Number of incidents in the O-rings (filed joints) versus temperatures. Panel a includes only flights with incidents. Panel b contains all flights (with and without incidents)." width="70%" />
<p class="caption">
Figure 3.2: Number of incidents in the O-rings (filed joints) versus temperatures. Panel <em>a</em> includes only flights with incidents. Panel <em>b</em> contains all flights (with and without incidents).
</p>
</div>
<p>The Rogers Commission noted a major flaw in Figure <a href="3-logistic-regression.html#fig:rogerts">3.2</a>a: the <strong>flights with zero incidents were excluded</strong> from the plot because <em>it was felt</em> that <strong>these flights did not contribute any information about the temperature effect</strong> (Figure <a href="3-logistic-regression.html#fig:rogerts">3.2</a>b). The Rogers Commission concluded:</p>
<blockquote>
<p><strong>“A careful analysis of the flight history of O-ring performance would have revealed the correlation of O-ring damage in low temperature”</strong>.</p>
</blockquote>
<p>The purpose of this case study, inspired by <span class="citation">(<a href="#ref-Dalal1989" role="doc-biblioref">Dalal, Fowlkes, and Hoadley 1989</a>)</span>, is to quantify what was the influence of the temperature in the probability of having at least one incident related with the O-rings. Specifically, we want to address the following questions:</p>
<ul>
<li>Q1. <em>Is the temperature associated with O-ring incidents?</em></li>
<li>Q2. <em>In which way was the temperature affecting the probability of O-ring incidents?</em></li>
<li>Q3. <em>What was the predicted probability of an incidient in an O-ring for the temperature of the launch day?</em></li>
</ul>
<p>To try to answer these questions we have the <code>challenger</code> ( <a target="_blank" href="datasets/challenger.txt"> dataset <i class="fa fa-table" aria-hidden="true"></i></a>). The dataset contains (as shown in the table below) information regarding the state of the solid rocket boosters after launch<a href="main-references-credits.html#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> for 23 flights. Each row has, among others, the following variables:</p>
<ul>
<li><code>fail.field</code>, <code>fail.nozzle</code>: binary variables indicating whether there was an incident with the O-rings in the field joints or in the nozzles of the solid rocket boosters. <code>1</code> codifies an incident and <code>0</code> its absence. On the analysis, we focus on the O-rings of the field joint as being the most determinants for the accident.</li>
<li><code>temp</code>: temperature in the day of launch. Measured in Celsius degrees.</li>
<li><code>pres.field</code>, <code>pres.nozzle</code>: leak-check pressure tests of the O-rings. These tests assured that the rings would seal the joint.</li>
</ul>

<table>
<caption>
<span id="tab:unnamed-chunk-78">Table 3.1: </span>The <code>challenger</code> dataset.
</caption>
<thead>
<tr>
<th style="text-align:left;">
flight
</th>
<th style="text-align:left;">
date
</th>
<th style="text-align:right;">
fail.field
</th>
<th style="text-align:right;">
fail.nozzle
</th>
<th style="text-align:right;">
temp
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
12/04/81
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
18.9
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
12/11/81
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21.1
</td>
</tr>
<tr>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
22/03/82
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
20.6
</td>
</tr>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
11/11/82
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
20.0
</td>
</tr>
<tr>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
04/04/83
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
19.4
</td>
</tr>
<tr>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
18/06/83
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
22.2
</td>
</tr>
<tr>
<td style="text-align:left;">
8
</td>
<td style="text-align:left;">
30/08/83
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
22.8
</td>
</tr>
<tr>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
28/11/83
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
21.1
</td>
</tr>
<tr>
<td style="text-align:left;">
41-B
</td>
<td style="text-align:left;">
03/02/84
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
13.9
</td>
</tr>
<tr>
<td style="text-align:left;">
41-C
</td>
<td style="text-align:left;">
06/04/84
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
17.2
</td>
</tr>
<tr>
<td style="text-align:left;">
41-D
</td>
<td style="text-align:left;">
30/08/84
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
21.1
</td>
</tr>
<tr>
<td style="text-align:left;">
41-G
</td>
<td style="text-align:left;">
05/10/84
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
25.6
</td>
</tr>
<tr>
<td style="text-align:left;">
51-A
</td>
<td style="text-align:left;">
08/11/84
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
19.4
</td>
</tr>
<tr>
<td style="text-align:left;">
51-C
</td>
<td style="text-align:left;">
24/01/85
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
11.7
</td>
</tr>
<tr>
<td style="text-align:left;">
51-D
</td>
<td style="text-align:left;">
12/04/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
19.4
</td>
</tr>
<tr>
<td style="text-align:left;">
51-B
</td>
<td style="text-align:left;">
29/04/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
23.9
</td>
</tr>
<tr>
<td style="text-align:left;">
51-G
</td>
<td style="text-align:left;">
17/06/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
21.1
</td>
</tr>
<tr>
<td style="text-align:left;">
51-F
</td>
<td style="text-align:left;">
29/07/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
27.2
</td>
</tr>
<tr>
<td style="text-align:left;">
51-I
</td>
<td style="text-align:left;">
27/08/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
24.4
</td>
</tr>
<tr>
<td style="text-align:left;">
51-J
</td>
<td style="text-align:left;">
03/10/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
26.1
</td>
</tr>
<tr>
<td style="text-align:left;">
61-A
</td>
<td style="text-align:left;">
30/10/85
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
23.9
</td>
</tr>
<tr>
<td style="text-align:left;">
61-B
</td>
<td style="text-align:left;">
26/11/85
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
24.4
</td>
</tr>
<tr>
<td style="text-align:left;">
61-C
</td>
<td style="text-align:left;">
12/01/86
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
14.4
</td>
</tr>
</tbody>
</table>
<p>Let’s begin the analysis by replicating Figures <a href="3-logistic-regression.html#fig:rogerts">3.2</a>a and <a href="3-logistic-regression.html#fig:rogerts">3.2</a>b and checking that linear regression is not the right tool for answering Q1–Q3. For that, we make two scatterplots of <code>nfails.field</code> (number of total incidents in the field joints) in function of <code>temp</code>, the first one excluding the launches without incidents (<code>subset = nfails.field &gt; 0</code>) and the second one for all the data.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="3-logistic-regression.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(car)</span>
<span id="cb31-2"><a href="3-logistic-regression.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">scatterplot</span>(nfails.field <span class="sc">~</span> temp, <span class="at">reg.line =</span> lm, <span class="at">smooth =</span> <span class="cn">FALSE</span>, <span class="at">spread =</span> <span class="cn">FALSE</span>,</span>
<span id="cb31-3"><a href="3-logistic-regression.html#cb31-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">boxplots =</span> <span class="cn">FALSE</span>, <span class="at">data =</span> challenger, <span class="at">subset =</span> nfails.field <span class="sc">&gt;</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-79-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="3-logistic-regression.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">scatterplot</span>(nfails.field <span class="sc">~</span> temp, <span class="at">reg.line =</span> lm, <span class="at">smooth =</span> <span class="cn">FALSE</span>, <span class="at">spread =</span> <span class="cn">FALSE</span>,</span>
<span id="cb32-2"><a href="3-logistic-regression.html#cb32-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">boxplots =</span> <span class="cn">FALSE</span>, <span class="at">data =</span> challenger)</span></code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-79-2.png" width="70%" style="display: block; margin: auto;" /></p>
<p>There is a fundamental problem in using linear regression for this data: <strong>the response is not continuous</strong>. As a consequence, there is no linearity and the errors around the mean are not normal (indeed, they are strongly non normal). We can check this with the corresponding diagnostic plots:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="3-logistic-regression.html#cb33-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(nfails.field <span class="sc">~</span> temp, <span class="at">data =</span> challenger)</span>
<span id="cb33-2"><a href="3-logistic-regression.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span>
<span id="cb33-3"><a href="3-logistic-regression.html#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod, <span class="dv">1</span>)</span>
<span id="cb33-4"><a href="3-logistic-regression.html#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod, <span class="dv">2</span>)</span></code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-80-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Although linear regression is not the adequate tool for this data, it is able to detect the obvious difference between the two plots:</p>
<ol style="list-style-type: decimal">
<li><strong>The trend for launches with incidents is flat, hence suggesting there is no dependence on the temperature</strong> (Figure <a href="3-logistic-regression.html#fig:rogerts">3.2</a>a). This was one of the arguments behind NASA’s decision of launching the rocket at a temperature of -0.6 degrees.</li>
<li>However, <strong>the trend for <em>all</em> launches indicates a clear negative dependence between temperature and number of incidents!</strong> (Figure <a href="3-logistic-regression.html#fig:rogerts">3.2</a>b). Think about it in this way: the minimum temperature for a launch without incidents ever recorded was above 18 degrees, and the Challenger was launched at -0.6 without clearly knowing the effects of such low temperatures.</li>
</ol>
<p>Instead of trying to predict the number of incidents, we will concentrate on modeling the <em>probability of expecting at least one incident given the temperature</em>, a simpler but also revealing approach. In other words, we look to estimate the following curve: <span class="math display">\[
p(x)=\mathbb{P}(\text{incident}=1|\text{temperature}=x)
\]</span> from <code>fail.field</code> and <code>temp</code>. This probability can not be properly modeled as a linear function like <span class="math inline">\(\beta_0+\beta_1x\)</span>, since inevitably will fall outside <span class="math inline">\([0,1]\)</span> for some value of <span class="math inline">\(x\)</span> (some will have negative probabilities or probabilities larger than one). The technique that solves this problem is the <strong>logistic regression</strong>. The logistic model in this case is <span class="math display">\[
\mathbb{P}(\text{incident}=1|\text{temperature}=x)=\text{logistic}\left(\beta_0+\beta_1x\right)=\frac{1}{1+e^{-(\beta_0+\beta_1x)}},
\]</span> with <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> unknown.</p>
<p>Let’s fit the model to the data by estimating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="3-logistic-regression.html#cb34-1" aria-hidden="true" tabindex="-1"></a>nasa <span class="ot">&lt;-</span> <span class="fu">glm</span>(fail.field <span class="sc">~</span> temp, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="at">data =</span> challenger)</span>
<span id="cb34-2"><a href="3-logistic-regression.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(nasa)</span>
<span id="cb34-3"><a href="3-logistic-regression.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-4"><a href="3-logistic-regression.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Call:</span></span>
<span id="cb34-5"><a href="3-logistic-regression.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; glm(formula = fail.field ~ temp, family = &quot;binomial&quot;, data = challenger)</span></span>
<span id="cb34-6"><a href="3-logistic-regression.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-7"><a href="3-logistic-regression.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Deviance Residuals: </span></span>
<span id="cb34-8"><a href="3-logistic-regression.html#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt;    Min      1Q  Median      3Q     Max  </span></span>
<span id="cb34-9"><a href="3-logistic-regression.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; -1.057  -0.757  -0.382   0.457   2.220  </span></span>
<span id="cb34-10"><a href="3-logistic-regression.html#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-11"><a href="3-logistic-regression.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Coefficients:</span></span>
<span id="cb34-12"><a href="3-logistic-regression.html#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt;             Estimate Std. Error z value Pr(&gt;|z|)  </span></span>
<span id="cb34-13"><a href="3-logistic-regression.html#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; (Intercept)    7.584      3.915    1.94    0.053 .</span></span>
<span id="cb34-14"><a href="3-logistic-regression.html#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; temp          -0.417      0.194   -2.15    0.032 *</span></span>
<span id="cb34-15"><a href="3-logistic-regression.html#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; ---</span></span>
<span id="cb34-16"><a href="3-logistic-regression.html#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb34-17"><a href="3-logistic-regression.html#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-18"><a href="3-logistic-regression.html#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb34-19"><a href="3-logistic-regression.html#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-20"><a href="3-logistic-regression.html#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt;     Null deviance: 28.267  on 22  degrees of freedom</span></span>
<span id="cb34-21"><a href="3-logistic-regression.html#cb34-21" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Residual deviance: 20.335  on 21  degrees of freedom</span></span>
<span id="cb34-22"><a href="3-logistic-regression.html#cb34-22" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; AIC: 24.33</span></span>
<span id="cb34-23"><a href="3-logistic-regression.html#cb34-23" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; </span></span>
<span id="cb34-24"><a href="3-logistic-regression.html#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; Number of Fisher Scoring iterations: 5</span></span>
<span id="cb34-25"><a href="3-logistic-regression.html#cb34-25" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(nasa)) <span class="co"># Exponentiated coefficients (&quot;odds ratios&quot;)</span></span>
<span id="cb34-26"><a href="3-logistic-regression.html#cb34-26" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; (Intercept)        temp </span></span>
<span id="cb34-27"><a href="3-logistic-regression.html#cb34-27" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt;    1965.974       0.659</span></span></code></pre></div>
<div class="rmdtip">
<p>
The <code>glm()</code> function fits <strong>g</strong>eneralized
<strong>l</strong>inear <strong>m</strong>odels, a class of models that
includes logistic regression. The syntax of the <code>glm()</code>
function is similar to that of <code>lm()</code>, except that we must
pass in the argument <code>family=binomial</code> in order to tell <span style="color: Steelblue;"><i class="fab fa-r-project" aria-hidden="true"></i>
</span> to run a logistic regression rather than some other type of
generalized linear model.
</p>
</div>
<p>The summary of the logistic model is notably different from the linear regression, as the methodology behind is quite different. Nevertheless, we have tests for the significance of each coefficient. Here we obtain that <code>temp</code> is significantly different from zero, at least at a level <span class="math inline">\(\alpha=0.05\)</span>. Therefore we can conclude that <strong>the temperature is indeed affecting the probability of an incident with the O-rings</strong> (answers Q1).</p>
<p>The coefficient of <code>temp</code>, <span class="math inline">\(\hat\beta_1\)</span>, can be regarded the “correlation between the temperature and the probability of having at least one incident”. This correlation, as evidenced by the sign of <span class="math inline">\(\hat\beta_1\)</span>, is negative. Let’s plot the fitted logistic curve to see that indeed the probability of incident and temperature are negatively correlated:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="3-logistic-regression.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data</span></span>
<span id="cb35-2"><a href="3-logistic-regression.html#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(challenger<span class="sc">$</span>temp, challenger<span class="sc">$</span>fail.field, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">30</span>), <span class="at">xlab =</span> <span class="st">&quot;Temperature&quot;</span>,</span>
<span id="cb35-3"><a href="3-logistic-regression.html#cb35-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Incident probability&quot;</span>)</span>
<span id="cb35-4"><a href="3-logistic-regression.html#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="3-logistic-regression.html#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the fitted logistic curve</span></span>
<span id="cb35-6"><a href="3-logistic-regression.html#cb35-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">30</span>, <span class="at">l =</span> <span class="dv">200</span>)</span>
<span id="cb35-7"><a href="3-logistic-regression.html#cb35-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span>(nasa<span class="sc">$</span>coefficients[<span class="dv">1</span>] <span class="sc">+</span> nasa<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">*</span> x))</span>
<span id="cb35-8"><a href="3-logistic-regression.html#cb35-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> y)</span>
<span id="cb35-9"><a href="3-logistic-regression.html#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, y, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb35-10"><a href="3-logistic-regression.html#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="3-logistic-regression.html#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co"># The Challenger</span></span>
<span id="cb35-12"><a href="3-logistic-regression.html#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(<span class="sc">-</span><span class="fl">0.6</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb35-13"><a href="3-logistic-regression.html#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(<span class="sc">-</span><span class="fl">0.6</span>, <span class="dv">1</span>, <span class="at">labels =</span> <span class="st">&quot;Challenger&quot;</span>, <span class="at">pos =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-83-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>At the sight of this curve and the summary of the model we can conclude that <strong>the temperature was increasing the probability of an O-ring incident</strong> (Q2). Indeed, the confidence intervals for the coefficients show a significative negative correlation at level <span class="math inline">\(\alpha=0.05\)</span>:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="3-logistic-regression.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(nasa, <span class="at">level =</span> <span class="fl">0.95</span>)</span>
<span id="cb36-2"><a href="3-logistic-regression.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt;              2.5 % 97.5 %</span></span>
<span id="cb36-3"><a href="3-logistic-regression.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; (Intercept)  1.336 17.783</span></span>
<span id="cb36-4"><a href="3-logistic-regression.html#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; temp        -0.924 -0.109</span></span></code></pre></div>
<p>Finally, <strong>the probability of having at least one incident with the O-rings in the launch day was</strong> <span class="math inline">\(0.9996\)</span> according to the fitted logistic model (Q3). This is easily obtained:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="3-logistic-regression.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(nasa, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">temp =</span> <span class="sc">-</span><span class="fl">0.6</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb37-2"><a href="3-logistic-regression.html#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; 1 </span></span>
<span id="cb37-3"><a href="3-logistic-regression.html#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co">#ans&gt; 1</span></span></code></pre></div>
<p>Be aware that <code>type = "response"</code> has a different meaning in logistic regression. In linear models it returns a CI for the prediction. But, <code>type = "response"</code> means that the <em>probability</em> should be returned, instead of the value of the link function, which is returned with <code>type = "link"</code> (the default).</p>
<p>Recall that there is a serious problem of <strong>extrapolation</strong> in the prediction, which makes it less precise (or more variable). But this extrapolation, together with the evidences raised by a simple analysis like we did, should have been strong arguments for postponing the launch.</p>
<p align="right">
<p>◼</p>
</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Dalal1989" class="csl-entry">
Dalal, Siddhartha R., Edward B. Fowlkes, and Bruce Hoadley. 1989. <span>“Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure.”</span> <em>Journal of the American Statistical Association</em> 84 (408): 945–57. <a href="https://doi.org/10.1080/01621459.1989.10478858">https://doi.org/10.1080/01621459.1989.10478858</a>.
</div>
<div id="ref-Roberts1986" class="csl-entry">
Presidential Commission on the Space Shuttle Challenger Accident. 1986. <em>Report of the Presidential Commission on the Space Shuttle Challenger Accident (Vols. 1 &amp; 2)</em>. Washington, DC. <a href="http://history.nasa.gov/rogersrep/genindex.htm">http://history.nasa.gov/rogersrep/genindex.htm</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>in the formula, <span class="math inline">\(\log\)</span> is the natural logarithm <span class="math inline">\(\ln\)</span><a href="3-logistic-regression.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Old Faithful, is a
hydrothermal geyser in Yellowstone National Park in the state of
Wyoming, U.S.A., and is a popular tourist attraction. Its name stems
from the supposed regularity of its eruptions. The data set comprises
272 observations, each of which represents a single eruption and
contains two variables corresponding to the duration in minutes of the
eruption, and the time until the next eruption, also in minutes.<a href="3-logistic-regression.html#fnref13" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pw-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pw-3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
