<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Linear Regression | Machine Learning</title>
  <meta name="description" content="1 Linear Regression | Machine Learning course" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Linear Regression | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="1 Linear Regression | Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Linear Regression | Machine Learning" />
  
  <meta name="twitter:description" content="1 Linear Regression | Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2019-07-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html">

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-88489172-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-88489172-1');
</script>



<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_tout_blanc.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i>Course Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i>What is Machine Learning ?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i>Supervised Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning</b></span></li>
<li class="part"><span><b>Regression</b></span></li>
<li class="chapter" data-level="1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regression.html"><a href="linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regression.html"><a href="linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i>Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression Model</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.5</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.6</b> Assessing the Accuracy of the Coefficient Estimates</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i>Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="linear-regression.html"><a href="linear-regression.html#anova-and-model-fit"><i class="fa fa-check"></i><b>1.7</b> ANOVA and model fit</a><ul>
<li class="chapter" data-level="1.7.1" data-path="linear-regression.html"><a href="linear-regression.html#anova"><i class="fa fa-check"></i><b>1.7.1</b> ANOVA</a></li>
<li class="chapter" data-level="1.7.2" data-path="linear-regression.html"><a href="linear-regression.html#the-r2-statistic"><i class="fa fa-check"></i><b>1.7.2</b> The <span class="math inline">\(R^2\)</span> Statistic</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">1</span> Linear Regression</h1>
<!-- Take a look to these slides -->
<!-- <iframe src="linear_regression.pdf" frameborder="0" width="700" height="422" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe> -->
<div id="notation" class="section level2">
<h2><span class="header-section-number">1.1</span> Notation</h2>
<p>In general, we will let <span class="math inline">\(x_{ij}\)</span> represent the value of the <span class="math inline">\(j\)</span>th variable for the <span class="math inline">\(i\)</span>th observation, where <span class="math inline">\(i=1,2,\ldots,n\)</span> and <span class="math inline">\(j=1,2,\ldots,p\)</span>.
We will use <span class="math inline">\(i\)</span> to index the samples or observations (from <span class="math inline">\(1\)</span> tp <span class="math inline">\(n\)</span>) and <span class="math inline">\(j\)</span> will be used to index the variables (or features) (from <span class="math inline">\(1\)</span> to <span class="math inline">\(p\)</span>). We let <span class="math inline">\(\textbf{X}\)</span> denote a <span class="math inline">\(n \times p\)</span> matrix whose <span class="math inline">\((i,j)\)</span>th element is <span class="math inline">\(x_{ij}\)</span>. That is,</p>
<p><span class="math display">\[ \textbf{X}  = \begin{pmatrix}
    x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1p} \\
    x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2p} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \dots  &amp; x_{np}
\end{pmatrix} \]</span></p>
<p>Note that it is useful to visualize <span class="math inline">\(\textbf{X}\)</span> as a spreadsheet of numbers with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns.
We will write the rows of <span class="math inline">\(\textbf{X}\)</span> as <span class="math inline">\(x_1 , x_2 , \ldots, x_n\)</span>. Here <span class="math inline">\(x_i\)</span> is a vector of length <span class="math inline">\(p\)</span>, containing the <span class="math inline">\(p\)</span> variable measurements for the <span class="math inline">\(i\)</span>th observation. That is,</p>
<p><span class="math display">\[ x_i = \begin{pmatrix}
    x_{i1} \\
    x_{i2} \\
    \vdots \\
    x_{ip}
\end{pmatrix}\]</span></p>
<p>(Vectors are by default represented as columns.)</p>
<p>We will write the columns of <span class="math inline">\(\textbf{X}\)</span> as <span class="math inline">\(\textbf{x}_1 , \textbf{x}_2, \ldots, \textbf{x}_p\)</span>. Each is a vector of length <span class="math inline">\(n\)</span>. That is,</p>
<p><span class="math display">\[ \textbf{x}_j = \begin{pmatrix}
    \textbf{x}_{1j} \\
    \textbf{x}_{2j} \\
    \vdots \\
    \textbf{x}_{nj}
\end{pmatrix}\]</span></p>
<p>Using this notation, the matrix <span class="math inline">\(\textbf{X}\)</span> can be written as</p>
<p><span class="math display">\[ \textbf{X} = (\textbf{x}_1  \textbf{x}_2 \ldots \textbf{x}_p) \]</span></p>
<p>or</p>
<p><span class="math display">\[ \textbf{X} = \begin{pmatrix}
    x_{1}^T \\
    x_{2}^T \\
    \vdots \\
    x_{n}^T
\end{pmatrix}\]</span></p>
<p>The <span class="math inline">\(^T\)</span> notation denotes the transpose of a matrix or vector.</p>
<p>We use <span class="math inline">\(y_i\)</span> to denote the <span class="math inline">\(i\)</span>th observation of the variable on which we wish to make predictions. We write the set of all <span class="math inline">\(n\)</span> observations in vector form as</p>
<p><span class="math display">\[ \textbf{y} = \begin{pmatrix}
    y_{1}^T \\
    y_{2}^T \\
    \vdots \\
    y_{n}^T
\end{pmatrix}\]</span></p>
<p>Then the observed data consists of <span class="math inline">\(\{(x_1, y_1), (x_2 , y_2 ), \ldots , (x_n , y_n )\}\)</span>, where
each <span class="math inline">\(x_i\)</span> is a vector of length <span class="math inline">\(p\)</span>. (If <span class="math inline">\(p = 1\)</span>, then <span class="math inline">\(x_i\)</span> is simply a scalar).</p>
</div>
<div id="model-representation" class="section level2">
<h2><span class="header-section-number">1.2</span> Model Representation</h2>
<p>Let’s consider the example about predicting housing prices. We’re going to use this data set as an example,</p>
<p><img src="img/mr1.png" /></p>
<p>Suppose that there is a person trying to sell a house of size 1250 square feet and he wants to know how much he might be able to sell the house for. One thing we could do is fit a model. Maybe fit a straight line to this data. Looks something like this,</p>
<p><img src="img/mr2.png" /></p>
<p>and based on that, maybe he can sell the house for around $220,000. Recall that this is an example of a supervised learning algorithm. And it’s supervised learning because we’re given the “right answer” for each of our examples. More precisely, this is an example of a regression problem where the term regression refers to the fact that we are predicting a real-valued output namely the price.</p>
<p>More formally, in supervised learning, we have a data set and this data set is called a <strong>training set</strong>. So for housing prices example, we have a training set of different housing prices and our job is to learn from this data how to predict prices of the houses.</p>
<p>Let’s define some notation from this data set:</p>
<ul>
<li>The size of the house is the input variable.</li>
<li>The house price is the output variable.</li>
<li>The input variables are typically denoted using the variable symbol <span class="math inline">\(X\)</span>,</li>
<li>The inputs go by different names, such as <em>predictors</em>, <em>independent variables</em>, <em>features</em>, or sometimes just <em>variables</em>.</li>
<li>The output variable is often called the <em>response</em>, <em>dependent variable</em> or <em>target</em>, and is typically denoted using the symbol <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\((x_i,y_i)\)</span> is the <span class="math inline">\(i\)</span>th training example.</li>
<li>The set of <span class="math inline">\(\{(x_i, y_i)\}\)</span> is the training set.</li>
<li><span class="math inline">\(n\)</span> is the number of training examples.</li>
</ul>
<p>So here’s how this supervised learning algorithm works. Suppose that we observe a quantitative response <span class="math inline">\(Y\)</span> and <span class="math inline">\(p\)</span> different predictors, <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> . We assume that there is some relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1 , X_2 ,\ldots, X_p)\)</span>, which can be written in the very general form</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<center>
<img src="img/mr3.png" />
</center>
<p>Here <span class="math inline">\(f\)</span> is some fixed but unknown function of <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> , and <span class="math inline">\(\epsilon\)</span> is a random error term, which is independent of <span class="math inline">\(X\)</span> and has mean zero. The <span class="math inline">\(f\)</span> function is also called <em>hypothesis</em> in Machine Learning. In general, the function <span class="math inline">\(f\)</span> may involve more than one input variable. In essence, Supervised Learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p>
</div>
<div id="why-estimate-f" class="section level2">
<h2><span class="header-section-number">1.3</span> Why Estimate <span class="math inline">\(f\)</span> ?</h2>
<p>There are two main reasons that we may wish to estimate <span class="math inline">\(f\)</span>: <em>prediction</em>
and <em>inference</em>.</p>
<div id="prediction" class="section level3 unnumbered">
<h3>Prediction</h3>
<p>In many situations, a set of inputs <span class="math inline">\(X\)</span> are readily available, but the output <span class="math inline">\(Y\)</span> cannot be easily obtained. In this setting, since the error term averages to zero, we can predict <span class="math inline">\(Y\)</span> using</p>
<p><span class="math display">\[ \hat{Y} = \hat{f}(X) \]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> represents our estimate for <span class="math inline">\(f\)</span>, and <span class="math inline">\(\hat{Y}\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>. Like in the example above about predicting housing prices.</p>
<p>We can measure the accuracy of <span class="math inline">\(\hat{Y}\)</span> by using a <strong>cost function</strong>. In the regression models, the most commonly-used measure is the <em>mean squared error</em> (MSE), given by</p>
<p><span class="math display">\[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2\]</span></p>
</div>
<div id="inference" class="section level3 unnumbered">
<h3>Inference</h3>
<p>We are often interested in understanding the way that <span class="math inline">\(Y\)</span> is affected as <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> change. In this situation we wish to estimate <span class="math inline">\(f\)</span> , but our goal is not necessarily to make predictions for <span class="math inline">\(Y\)</span>. We instead want to understand the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or more specifically, to understand how <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span>. In this case, one may be interested in answering the following questions:</p>
<ul>
<li>Which predictors are associated with the response?</li>
<li>What is the relationship between the response and each predictor?</li>
<li>Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</li>
</ul>
</div>
</div>
<div id="simple-linear-regression-model" class="section level2">
<h2><span class="header-section-number">1.4</span> Simple Linear Regression Model</h2>
<p><em>Simple linear regression</em> is a very straightforward approach for predicting a quantitative response <span class="math inline">\(Y\)</span> on the basis of a single predictor variable <span class="math inline">\(X\)</span>. It assumes that there is approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Mathematically, we can write this linear relationship as</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 X + \epsilon \]</span>
<span class="math display">\[Y \approx \beta_0 + \beta_1 X\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are two unknown constants that represent the <em>intercept</em> and <em>slope</em>, also known as <strong><em>coefficients</em></strong> or <em>parameters</em>, and <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<p>Given some estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> for the model coefficients, we predict future inputs <span class="math inline">\(x\)</span> using</p>
<p><span class="math display">\[\hat{y} = \hat{\beta_0} + \hat{\beta_1} x\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> indicates a prediction of <span class="math inline">\(Y\)</span> on the basis of <span class="math inline">\(X = x\)</span>.
The <em>hat</em> symbol, <span class="math inline">\(\hat{}\)</span>, denotes an estimated value.</p>
</div>
<div id="estimating-the-coefficients" class="section level2">
<h2><span class="header-section-number">1.5</span> Estimating the Coefficients</h2>
<p>Let <span class="math inline">\(\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i\)</span> be the prediction for <span class="math inline">\(Y\)</span> based on the <span class="math inline">\(i\)</span>th value of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> represents the <span class="math inline">\(i\)</span>th <strong><em>residual</em></strong>.</p>
<p>We define the <strong><em>Residual Sum of Squares</em></strong> (<strong>RSS</strong>)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> as</p>
<p><span class="math display">\[  \begin{aligned}
RSS &amp;= e_1^2 + e_2^2 + \ldots + e_n^2 \\
    &amp;= \sum_{i=1}^{n} e_i^2
 \end{aligned}  \]</span></p>
<p>or equivantly as</p>
<p><span class="math display">\[ \begin{aligned}
RSS &amp;= (y_1 - \hat{\beta_0} - \hat{\beta_1} x_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1} x_2)^2 + \ldots + (y_n - \hat{\beta_0} - \hat{\beta_1} x_n)^2 \\
    &amp;= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2
\end{aligned} \]</span></p>
<p>The <em>least squares</em> approach chooses <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> to minimize the RSS. The minimizing values can be shown to be<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display">\[  \begin{aligned}
\hat{\beta_1} &amp;=  \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})  }{\sum_{i=1}^{n} (x_i - \bar{x})^2 } = \frac{s_{xy}}{s_x^2} = (s_x^2)^{-1} s_{xy} \\
\text{and} \\
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1} \bar{x}
\end{aligned}  \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i\)</span> is the <em>sample mean</em>.</li>
<li><span class="math inline">\(s_x^2=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\)</span> is the <em>sample variance</em>. The sample standard deviation is <span class="math inline">\(s_x=\sqrt{s_x^2}\)</span>.</li>
<li><span class="math inline">\(s_{xy}=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})\)</span> is the <em>sample covariance</em>. It measures the degree of linear association between <span class="math inline">\(x_1,\ldots,x_n\)</span> and <span class="math inline">\(y_1,\ldots,y_n\)</span>. Once scaled by <span class="math inline">\(s_xs_y\)</span>, it gives the <em>sample correlation coefficient</em>, <span class="math inline">\(r_{xy}=\frac{s_{xy}}{s_xs_y}\)</span>.</li>
</ul>
<div class="rmdinsight">
<p>
1- To find the optimal estimates for <span class="math inline"><span class="math inline">\(\beta_0\)</span></span> and <span class="math inline"><span class="math inline">\(\beta_1\)</span></span> we need a choice-criterion. In the case of the <em>least squares</em> approach (more precisely, the <em>ordinary least squares OLS</em>) this criterion is the residual sum of squares <em>RSS</em>: we calculate <span class="math inline"><span class="math inline">\(\beta_0\)</span></span> and <span class="math inline"><span class="math inline">\(\beta_1\)</span></span> that minimise the <em>RSS</em>.
</p>
<p>
2- Minimizing the <em>RSS</em> function requires to calculate the first order derivatives with respect to <span class="math inline"><span class="math inline">\(\beta_0\)</span></span> and <span class="math inline"><span class="math inline">\(\beta_1\)</span></span> and set them to zero.
</p>
<p>
3- Click <a target="_blank" href="https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/least-squares-regression/v/introduction-to-residuals-and-least-squares-regression"> here <i class="fa fa-video-camera" aria-hidden="true"></i></a> and watch the video to understand more about the residuals and least squares.
</p>
<p>
4- Click <a href="https://egarpor.shinyapps.io/least-squares/" target="_blank">here <i class="fa fa-external-link" aria-hidden="true"></i></a> to see the influence of the distance employed in the sum of squares. Try to minimize the sum of squares for the different datasets. The choices of intercept and slope that minimize the sum of squared distances for a kind of distance are not the optimal for a different kind of distance.
</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimates" class="section level2">
<h2><span class="header-section-number">1.6</span> Assessing the Accuracy of the Coefficient Estimates</h2>
<p>The standard error of an estimator reflects how it varies under repeated sampling. We have</p>
<p><span class="math display">\[ \text{SE}(\hat{\beta_1})^2 =  \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \]</span></p>
<p><span class="math display">\[ \text{SE}(\hat{\beta_0})^2 = \sigma^2 \bigg[ \frac{1}{n} +  \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \bigg] \]</span></p>
<p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p>
<p>In general, <span class="math inline">\(\sigma^2\)</span> is unknown, but can be estimated from the data. The estimate of <span class="math inline">\(\sigma\)</span> is known as the <em>residual standard error</em>, and is given by</p>
<p><span class="math display">\[ \text{RSE} = \sqrt{\frac{\text{RSS}}{(n-2)}} \]</span></p>
<p>These standard errors can be used to compute <em>confidence intervals</em>. A <span class="math inline">\(95\%\)</span> confidence interval is defined as a range of values such that with <span class="math inline">\(95\%\)</span> probability, the range will contain the true unknown value of the parameter. It has the form</p>
<p><span class="math display">\[ \hat{\beta_1} \pm 2 \cdot \text{SE}(\hat{\beta_1}) \]</span></p>
<p>That is, there is approximately a <span class="math inline">\(95\%\)</span> chance that the interval</p>
<p><span class="math display">\[ \bigg[  \hat{\beta_1} - 2 \cdot \text{SE}(\hat{\beta_1}), \hat{\beta_1} + 2 \cdot \text{SE}(\hat{\beta_1})   \bigg] \]</span></p>
<p>will contain the true value of <span class="math inline">\(\beta_1\)</span>. Similarly, a confidence interval for <span class="math inline">\(\beta_0\)</span> approximately takes the form</p>
<p><span class="math display">\[ \hat{\beta_0} \pm 2 \cdot \text{SE}(\hat{\beta_0}) \]</span></p>
<div id="hypothesis-testing" class="section level3 unnumbered">
<h3>Hypothesis testing</h3>
<p>Standard errors can also be used to perform <em>hypothesis tests</em> on the coefficients. The most common hypothesis test involves testing the <em>null hypothesis</em> of</p>
<p><span class="math display">\[ H_0 : \text{There is no relationship between} \, X \, \text{and} \, Y \]</span></p>
<p>versus the <em>alternative hypothesis</em></p>
<p><span class="math display">\[ H_1 : \text{There is some relationship between} \, X \, \text{and} \, Y \]</span></p>
<p>Mathematically, this corresponds to testing</p>
<p><span class="math display">\[ H_0 : \beta_1 = 0 \]</span></p>
<p>versus</p>
<p><span class="math display">\[ H_1 : \beta_1 \neq 0 \]</span></p>
<p>since if <span class="math inline">\(\beta_1 = 0\)</span> then the simple linear regression model reduces to <span class="math inline">\(Y = \beta_0 + \epsilon\)</span>, and <span class="math inline">\(X\)</span> is not associated with <span class="math inline">\(Y\)</span>.</p>
<p>To test the null hypothesis <span class="math inline">\(H_0\)</span>, we compute a <strong><em>t-statistic</em></strong>, given by</p>
<p><span class="math display">\[ t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})} \]</span></p>
<p>This will have a <span class="math inline">\(t\)</span>-distribution (<em>Student</em>) with <span class="math inline">\(n-2\)</span> degrees of freedom, assuming <span class="math inline">\(\beta_1=0\)</span>.</p>
<p>Using statistical software, it is easy to compute the probability of observing any value equal to <span class="math inline">\(|t|\)</span> or larger. We call this probability the <strong><em>p-value</em></strong>.</p>
<p>If p-value is small enough (typically under <span class="math inline">\(0.01\)</span> (<span class="math inline">\(1\%\)</span> error) or <span class="math inline">\(0.05\)</span> (<span class="math inline">\(5\%\)</span> error)) we reject the null hypothesis, that is we declare a relationship to exist between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<div id="anova-and-model-fit" class="section level2">
<h2><span class="header-section-number">1.7</span> ANOVA and model fit</h2>
<div id="anova" class="section level3">
<h3><span class="header-section-number">1.7.1</span> ANOVA</h3>
<p>In this section we will see how the variance of <span class="math inline">\(Y\)</span> is decomposed into two parts, each one corresponding to the regression and to the error, respectively. This decomposition is called the <em>ANalysis Of VAriance</em> (ANOVA).</p>
<p>Before explaining ANOVA, it is important to recall an interesting result: <em>the mean of the fitted values <span class="math inline">\(\hat Y_1,\ldots,\hat Y_n\)</span> is the mean of <span class="math inline">\(Y_1,\ldots, Y_n\)</span></em>. This is easily seen if we plug-in the expression of <span class="math inline">\(\hat\beta_0\)</span>:
<span class="math display">\[\begin{align*}
\frac{1}{n}\sum_{i=1}^n \hat Y_i=\frac{1}{n}\sum_{i=1}^n \left(\hat \beta_0+\hat\beta_1X_i\right)=\hat \beta_0+\hat\beta_1\bar X=\left(\bar Y - \hat\beta_1\bar X \right) + \hat\beta_1\bar X=\bar Y.
\end{align*}\]</span>
The ANOVA decomposition considers the following measures of variation related with the response:</p>
<ul>
<li><span class="math inline">\(\text{SST}=\text{TSS}=\sum_{i=1}^n\left(Y_i-\bar Y\right)^2\)</span>, the <strong>Total Sum of Squares</strong>. This is the <em>total variation</em> of <span class="math inline">\(Y_1,\ldots,Y_n\)</span>, since <span class="math inline">\(\text{SST}=ns_y^2\)</span>, where <span class="math inline">\(s_y^2\)</span> is the sample variance of <span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</li>
<li><span class="math inline">\(\text{SSR}=\text{ESS}=\sum_{i=1}^n\left(\hat Y_i-\bar Y\right)^2\)</span>, the <strong>Regression Sum of Squares</strong> or <strong>Explained Sum of Squares</strong><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. This is the variation explained by the regression line, that is, <em>the variation from <span class="math inline">\(\bar Y\)</span> that is explained by the estimated conditional mean <span class="math inline">\(\hat Y_i=\hat\beta_0+\hat\beta_1X_i\)</span></em>. <span class="math inline">\(\text{SSR}=ns_{\hat y}^2\)</span>, where <span class="math inline">\(s_{\hat y}^2\)</span> is the sample variance of <span class="math inline">\(\hat Y_1,\ldots,\hat Y_n\)</span>.</li>
<li><span class="math inline">\(\text{SSE}=\text{RSS}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2\)</span>, the <strong>Sum of Squared Errors</strong> or <strong>Residual Sum of Squares</strong><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Is the variation around the conditional mean. Recall that <span class="math inline">\(\text{SSE}=\sum_{i=1}^n \hat\varepsilon_i^2=(n-2)\hat\sigma^2\)</span>, where <span class="math inline">\(\hat\sigma^2\)</span> is the sample variance of <span class="math inline">\(\hat \varepsilon_1,\ldots,\hat \varepsilon_n\)</span>.</li>
</ul>
<p>The ANOVA decomposition is
<span class="math display">\[\begin{align*}
\underbrace{\text{SST}}_{\text{Variation of }Y_i&#39;s} = \underbrace{\text{SSR}}_{\text{Variation of }\hat Y_i&#39;s} + \underbrace{\text{SSE}}_{\text{Variation of }\hat \varepsilon_i&#39;s}
\end{align*}\]</span></p>
<p>The graphical interpretation of this equation is shown in the following figures.</p>
<div class="figure" style="text-align: center"><span id="fig:anova"></span>
<img src="img/anova.png" alt="Visualization of the ANOVA decomposition. SST measures the variation of $Y_1,\ldots,Y_n$ with respect to $\bar Y$. SST measures the variation with respect to the conditional means, $\hat \beta_0+\hat\beta_1X_i$. SSE collects the variation of the residuals." width="70%" />
<p class="caption">
Figure 1.1: Visualization of the ANOVA decomposition. SST measures the variation of <span class="math inline">\(Y_1,\ldots,Y_n\)</span> with respect to <span class="math inline">\(\bar Y\)</span>. SST measures the variation with respect to the conditional means, <span class="math inline">\(\hat \beta_0+\hat\beta_1X_i\)</span>. SSE collects the variation of the residuals.
</p>
</div>
<div class="rmdinsight">
<p>
Below the ANOVA decomposition and its dependence on <span class="math inline"><span class="math inline">\(\sigma^2\)</span></span> and <span class="math inline"><span class="math inline">\(\hat\sigma^2\)</span></span>. Application is also available <a href="https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova/" target="_blank">here</a>.
</p>
<p>
Note that the <strong>animation</strong> will not be displayed the first time it is browsed (The reason is because it is hosted at <code>https</code> websites with auto-signed SSL certificates). <strong>To see it</strong>, click on the link above. You will get a warning from your browser saying that <em>“Your connection is not private”</em>. Click in <em>“Advanced”</em> and <strong>allow an exception</strong> in your browser. The next time the animation will show up correctly.
</p>
</div>
<iframe src="https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova/?showcase=0" width="90%" height="900px">
</iframe>
<p>The ANOVA table summarizes the decomposition of the variance. Here is given in the layout employed by <code>R</code>.</p>
<table>
<colgroup>
<col width="3%" />
<col width="19%" />
<col width="12%" />
<col width="14%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Degrees of freedom</th>
<th>Sum Squares</th>
<th>Mean Squares</th>
<th><span class="math inline">\(F\)</span>-value</th>
<th><span class="math inline">\(p\)</span>-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Predictor</td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(\text{SSR}=\sum_{i=1}^n\left(\hat Y_i-\bar Y\right)^2\)</span></td>
<td><span class="math inline">\(\text{MSR}=\frac{\text{SSR}}{1}\)</span></td>
<td><span class="math inline">\(\frac{\text{SSR}/1}{\text{SSE}/(n-2)}\)</span></td>
<td><span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td>Residuals</td>
<td><span class="math inline">\(n - 2\)</span></td>
<td><span class="math inline">\(\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2\)</span></td>
<td><span class="math inline">\(\text{MSE}=\frac{\text{SSE}}{n-2}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(n-1\)</span></td>
<td><span class="math inline">\(\text{SST}=\sum_{i=1}^n\left(Y_i-\bar Y\right)^2\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The <code>anova</code> function in <code>R</code> takes a model as an input and returns the ANOVA table.</p>
<p>The “<span class="math inline">\(F\)</span>-value” of the ANOVA table represents the value of the <span class="math inline">\(F\)</span>-statistic <span class="math inline">\(\frac{\text{SSR}/1}{\text{SSE}/(n-2)}\)</span>. This statistic is employed to test
<span class="math display">\[\begin{align*}
H_0:\beta_1=0\quad\text{vs.}\quad H_1:\beta_1\neq 0,
\end{align*}\]</span>
that is, the hypothesis of no linear dependence of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. The result of this test is completely equivalent to the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\beta_1\)</span> that we saw previously in the Hypothesis testing (this is something <em>specific for simple linear regression</em> – the <span class="math inline">\(F\)</span>-test will not be equivalent to the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\beta_1\)</span> in the Mulitple Linear Regression).</p>
<p>It happens that
<span class="math display">\[\begin{align*}
F=\frac{\text{SSR}/1}{\text{SSE}/(n-2)}\stackrel{H_0}{\sim} F_{1,n-2},
\end{align*}\]</span>
where <span class="math inline">\(F_{1,n-2}\)</span> is the <em>Snedecor’s <span class="math inline">\(F\)</span> distribution</em><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> with <span class="math inline">\(1\)</span> and <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<p>If <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(F\)</span> is expected to be <em>small</em> since SSR will be close to zero. The <span class="math inline">\(p\)</span>-value of this test is the same as the <span class="math inline">\(p\)</span>-value of the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(H_0:\beta_1=0\)</span>.</p>
</div>
<div id="the-r2-statistic" class="section level3">
<h3><span class="header-section-number">1.7.2</span> The <span class="math inline">\(R^2\)</span> Statistic</h3>
<p>To calculate <span class="math inline">\(R^2\)</span>, we use the formula</p>
<p><span class="math display">\[ R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1- \frac{\text{RSS}}{\text{TSS}} \]</span></p>
<p>where <span class="math inline">\(\text{TSS} = \sum (y_i - \bar{y})^2\)</span> is the <em>total sum of squares</em>.</p>
<p><span class="math inline">\(R^2\)</span> measures the <em>proportion of variability in</em> <span class="math inline">\(Y\)</span> <em>that can be explained using</em> <span class="math inline">\(X\)</span>. An <span class="math inline">\(R^2\)</span> statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong, or the inherent error <span class="math inline">\(\sigma^2\)</span> is high, or both.</p>
<p>It can be shown that in this simple linear linear regression setting that <span class="math inline">\(R^2 = r^2\)</span>, where <span class="math inline">\(r\)</span> is the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[ r = \frac{cov(X,Y)}{\sigma_X \sigma_Y} \]</span></p>
<div class="rmdcaution">
<p>
<span class="math inline"><span class="math inline">\(R^2\)</span></span> does not measure the correctness of a linear model but its <strong>usefulness</strong> (for prediction, for <em>explaining the variance</em> of <span class="math inline"><span class="math inline">\(Y\)</span></span>), assuming the model is correct.
</p>
<p>
Trusting blindly the <span class="math inline"><span class="math inline">\(R^2\)</span></span> can lead to catastrophic conclusions, since the model may not be correct.
</p>
</div>
So remember:
<div class="rmdinsight">
<p>
A large <span class="math inline"><span class="math inline">\(R^2\)</span></span> means <em>nothing</em> if the <strong>assumptions of the model do not hold</strong>. <span class="math inline"><span class="math inline">\(R^2\)</span></span> is the proportion of variance of <span class="math inline"><span class="math inline">\(Y\)</span></span> explained by <span class="math inline"><span class="math inline">\(X\)</span></span>, but, of course, <em>only when the linear model is correct</em>.
</p>
</div>
<p align="right">
◼
</p>

</div>
</div>
</div>










<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Also known as <strong>SSE</strong>: Sum of Squared Errors.<a href="linear-regression.html#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>They are unique and always exist. They can be obtained by solving <span class="math inline">\(\frac{\partial}{\partial \beta_0}\text{RSS}(\beta_0,\beta_1)=0\)</span> and <span class="math inline">\(\frac{\partial}{\partial \beta_1}\text{RSS}(\beta_0,\beta_1)=0\)</span>.<a href="linear-regression.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Recall that SSR is different from RSS (Residual Sum of Squares)<a href="linear-regression.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>Recall that SSE and RSS (for <span class="math inline">\((\hat \beta_0,\hat \beta_1)\)</span>) are just different names for referring to the same quantity: <span class="math inline">\(\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2=\sum_{i=1}^n\left(Y_i-\hat \beta_0-\hat \beta_1X_i\right)^2=\mathrm{RSS}\left(\hat \beta_0,\hat \beta_1\right)\)</span>.<a href="linear-regression.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>The <span class="math inline">\(F_{n,m}\)</span> distribution arises as the quotient of two independent random variables <span class="math inline">\(\chi^2_n\)</span> and <span class="math inline">\(\chi^2_m\)</span>, <span class="math inline">\(\frac{\chi^2_n/n}{\chi^2_m/m}\)</span>.<a href="linear-regression.html#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Machine-Learning.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
