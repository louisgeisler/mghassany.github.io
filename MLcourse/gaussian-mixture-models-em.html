<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Gaussian Mixture Models &amp; EM | Machine Learning</title>
  <meta name="description" content="8 Gaussian Mixture Models &amp; EM | Machine Learning course" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Gaussian Mixture Models &amp; EM | Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="8 Gaussian Mixture Models &amp; EM | Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Gaussian Mixture Models &amp; EM | Machine Learning" />
  
  <meta name="twitter:description" content="8 Gaussian Mixture Models &amp; EM | Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2019-11-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pw-7.html"/>
<link rel="next" href="pw-8.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="book_assets/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-88489172-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-88489172-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_tout_blanc.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-overview"><i class="fa fa-check"></i>Course Overview</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i>What is Machine Learning ?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i>Supervised Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a></li>
</ul></li>
<li class="part"><span><b>I Supervised Learning</b></span></li>
<li class="part"><span><b>Regression</b></span></li>
<li class="chapter" data-level="1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regression.html"><a href="linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regression.html"><a href="linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i>Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression Model</a></li>
<li class="chapter" data-level="1.5" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.5</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="1.6" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.6</b> Assessing the Accuracy of the Coefficient Estimates</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#hypothesis-testing"><i class="fa fa-check"></i>Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="linear-regression.html"><a href="linear-regression.html#anova-and-model-fit"><i class="fa fa-check"></i><b>1.7</b> ANOVA and model fit</a><ul>
<li class="chapter" data-level="1.7.1" data-path="linear-regression.html"><a href="linear-regression.html#anova"><i class="fa fa-check"></i><b>1.7.1</b> ANOVA</a></li>
<li class="chapter" data-level="1.7.2" data-path="linear-regression.html"><a href="linear-regression.html#the-r2-statistic"><i class="fa fa-check"></i><b>1.7.2</b> The <span class="math inline">\(R^2\)</span> Statistic</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-work-1.html"><a href="practical-work-1.html"><i class="fa fa-check"></i>Practical Work 1</a><ul>
<li class="chapter" data-level="1.8" data-path="practical-work-1.html"><a href="practical-work-1.html#some-basics"><i class="fa fa-check"></i><b>1.8</b> Some <svg style="height:0.8em;top:.04em;position:relative;fill:steelblue;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg> basics</a><ul>
<li class="chapter" data-level="1.8.1" data-path="practical-work-1.html"><a href="practical-work-1.html#basic-commands"><i class="fa fa-check"></i><b>1.8.1</b> Basic Commands</a></li>
<li class="chapter" data-level="1.8.2" data-path="practical-work-1.html"><a href="practical-work-1.html#vectors"><i class="fa fa-check"></i><b>1.8.2</b> Vectors</a></li>
<li class="chapter" data-level="1.8.3" data-path="practical-work-1.html"><a href="practical-work-1.html#matrices-data-frames-and-lists"><i class="fa fa-check"></i><b>1.8.3</b> Matrices, data frames and lists</a></li>
<li class="chapter" data-level="1.8.4" data-path="practical-work-1.html"><a href="practical-work-1.html#graphics"><i class="fa fa-check"></i><b>1.8.4</b> Graphics</a></li>
<li class="chapter" data-level="1.8.5" data-path="practical-work-1.html"><a href="practical-work-1.html#distributions"><i class="fa fa-check"></i><b>1.8.5</b> Distributions</a></li>
<li class="chapter" data-level="1.8.6" data-path="practical-work-1.html"><a href="practical-work-1.html#working-directory"><i class="fa fa-check"></i><b>1.8.6</b> Working directory</a></li>
<li class="chapter" data-level="1.8.7" data-path="practical-work-1.html"><a href="practical-work-1.html#loading-data"><i class="fa fa-check"></i><b>1.8.7</b> Loading Data</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="practical-work-1.html"><a href="practical-work-1.html#regression"><i class="fa fa-check"></i><b>1.9</b> Regression</a><ul>
<li class="chapter" data-level="1.9.1" data-path="practical-work-1.html"><a href="practical-work-1.html#the-lm-function"><i class="fa fa-check"></i><b>1.9.1</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="practical-work-1.html"><a href="practical-work-1.html#boston"><i class="fa fa-check"></i><b>1.9.2</b> Predicting House Value: Boston dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-consid"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#how-to-select-the-best-performing-model"><i class="fa fa-check"></i><b>2.4</b> How to select the best performing model</a><ul>
<li><a href="multiple-linear-regression.html#use-the-adjusted-r_adj2-for-univariate-models">Use the Adjusted <span class="math inline">\(R_{adj}^2\)</span> for univariate models</a></li>
<li class="chapter" data-level="" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#have-a-look-at-the-residuals-or-error-terms"><i class="fa fa-check"></i>Have a look at the residuals or error terms</a></li>
<li class="chapter" data-level="" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#histogram-of-residuals"><i class="fa fa-check"></i>Histogram of residuals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html"><i class="fa fa-check"></i>PW 2</a><ul>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#multiple-linear-regression-1"><i class="fa fa-check"></i>Multiple Linear Regression</a></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#reporting"><i class="fa fa-check"></i>Reporting</a></li>
</ul></li>
<li class="part"><span><b>Classification</b></span></li>
<li class="chapter" data-level="3" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-1"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="3.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="3.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#prediction-1"><i class="fa fa-check"></i><b>3.2.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="3.4" data-path="logistic-regression.html"><a href="logistic-regression.html#logreg-examps"><i class="fa fa-check"></i><b>3.4</b> Example</a><ul>
<li class="chapter" data-level="3.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logreg-examps-challenger"><i class="fa fa-check"></i><b>3.4.1</b> Case study: <em>The Challenger disaster</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html"><i class="fa fa-check"></i>PW 3</a><ul>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html#social-networks-ads"><i class="fa fa-check"></i>Social Networks Ads</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>4</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-theorem"><i class="fa fa-check"></i><b>4.2</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="4.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#lda-for-p1"><i class="fa fa-check"></i><b>4.3</b> LDA for <span class="math inline">\(p=1\)</span></a></li>
<li class="chapter" data-level="4.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#estimating-the-parameters"><i class="fa fa-check"></i><b>4.4</b> Estimating the parameters</a></li>
<li class="chapter" data-level="4.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#lda-for-p-1"><i class="fa fa-check"></i><b>4.5</b> LDA for <span class="math inline">\(p &gt; 1\)</span></a></li>
<li class="chapter" data-level="4.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#making-predictions"><i class="fa fa-check"></i><b>4.6</b> Making predictions</a></li>
<li class="chapter" data-level="4.7" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#other-forms-of-discriminant-analysis"><i class="fa fa-check"></i><b>4.7</b> Other forms of Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.7.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>4.7.1</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="4.7.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#naive-bayes"><i class="fa fa-check"></i><b>4.7.2</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#lda-vs-logistic-regression"><i class="fa fa-check"></i><b>4.8</b> LDA vs Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html"><i class="fa fa-check"></i>PW 4</a><ul>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#logistic-regression-2"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#decision-boundary-of-logistic-regression"><i class="fa fa-check"></i>Decision Boundary of Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i>Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#lda-from-scratch"><i class="fa fa-check"></i>LDA from scratch</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#quadratic-discriminant-analysis-qda-1"><i class="fa fa-check"></i>Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#comparison"><i class="fa fa-check"></i>Comparison</a></li>
</ul></li>
<li class="part"><span><b>Special Session</b></span></li>
<li class="chapter" data-level="5" data-path="session-5.html"><a href="session-5.html"><i class="fa fa-check"></i><b>5</b> Session 5</a></li>
<li class="part"><span><b>II Dimensionality Reduction</b></span></li>
<li class="chapter" data-level="6" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html"><i class="fa fa-check"></i><b>6</b> Principal Components Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#principal-components"><i class="fa fa-check"></i><b>6.2</b> Principal Components</a><ul>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#notations-and-procedure"><i class="fa fa-check"></i>Notations and Procedure</a></li>
<li><a href="principal-components-analysis.html#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></a></li>
<li><a href="principal-components-analysis.html#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></a></li>
<li><a href="principal-components-analysis.html#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#how-do-we-find-the-coefficients"><i class="fa fa-check"></i><b>6.3</b> How do we find the coefficients?</a><ul>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#why-it-may-be-possible-to-reduce-dimensions"><i class="fa fa-check"></i>Why It May Be Possible to Reduce Dimensions</a></li>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#procedure"><i class="fa fa-check"></i>Procedure</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#standardization-of-the-features"><i class="fa fa-check"></i><b>6.4</b> Standardization of the features</a></li>
<li class="chapter" data-level="6.5" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#projection-of-the-data"><i class="fa fa-check"></i><b>6.5</b> Projection of the data</a><ul>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#scores"><i class="fa fa-check"></i>Scores</a></li>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#visualization"><i class="fa fa-check"></i>Visualization</a></li>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#extra"><i class="fa fa-check"></i>Extra</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#case-study"><i class="fa fa-check"></i><b>6.6</b> Case study</a><ul>
<li class="chapter" data-level="" data-path="principal-components-analysis.html"><a href="principal-components-analysis.html#employement-in-european-countries-in-the-late-70s"><i class="fa fa-check"></i>Employement in European countries in the late 70s</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html"><i class="fa fa-check"></i>PW 6</a><ul>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#the-iris-dataset"><i class="fa fa-check"></i>The Iris Dataset</a></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#loading-data-1"><i class="fa fa-check"></i>Loading Data</a></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#exploratory-analysis"><i class="fa fa-check"></i>Exploratory analysis</a></li>
<li><a href="pw-6.html#pca-using-princomp">PCA using <code>princomp()</code></a></li>
<li><a href="pw-6.html#deeper-pca-using-factoextra-package">Deeper PCA using <code>factoextra</code> package</a></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#step-by-step-pca"><i class="fa fa-check"></i>Step-by-step PCA</a></li>
</ul></li>
<li class="part"><span><b>III Unsupervised Learning</b></span></li>
<li class="chapter" data-level="7" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering.html"><a href="clustering.html#unsupervised-learning-1"><i class="fa fa-check"></i><b>7.1</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="7.2" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>7.2</b> Clustering</a></li>
<li class="chapter" data-level="7.3" data-path="clustering.html"><a href="clustering.html#introduction-4"><i class="fa fa-check"></i><b>7.3</b> Introduction</a><ul>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#hard-clustering"><i class="fa fa-check"></i>Hard clustering</a></li>
<li class="chapter" data-level="" data-path="clustering.html"><a href="clustering.html#fuzzy-clustering"><i class="fa fa-check"></i>Fuzzy clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>7.4</b> <span class="math inline">\(k\)</span>-Means</a></li>
<li class="chapter" data-level="7.5" data-path="clustering.html"><a href="clustering.html#k-means-in"><i class="fa fa-check"></i><b>7.5</b> <span class="math inline">\(k\)</span>-means in <svg style="height:0.8em;top:.04em;position:relative;fill:steelblue;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg></a><ul>
<li class="chapter" data-level="7.5.1" data-path="clustering.html"><a href="clustering.html#cluster-validity-choosing-the-number-of-clusters"><i class="fa fa-check"></i><b>7.5.1</b> Cluster Validity, Choosing the Number of Clusters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html"><i class="fa fa-check"></i>PW 7</a><ul>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#reporting-1"><i class="fa fa-check"></i>Reporting</a><ul>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#markdown"><i class="fa fa-check"></i>Markdown</a></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#r-markdown"><i class="fa fa-check"></i>R Markdown</a></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#the-report-to-be-submitted"><i class="fa fa-check"></i>The report to be submitted</a></li>
</ul></li>
<li><a href="pw-7.html#k-means-clustering"><span class="math inline">\(k\)</span>-means clustering</a><ul>
<li><a href="pw-7.html#pointscards"><code>pointsCards</code></a></li>
<li><a href="pw-7.html#ligue-1"><code>Ligue 1</code></a></li>
<li><a href="pw-7.html#pca"><code>PCA</code></a></li>
<li><a href="pw-7.html#implementing-k-means"><code>Implementing k-means</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="gaussian-mixture-models-em.html"><a href="gaussian-mixture-models-em.html"><i class="fa fa-check"></i><b>8</b> Gaussian Mixture Models &amp; EM</a><ul>
<li class="chapter" data-level="8.1" data-path="gaussian-mixture-models-em.html"><a href="gaussian-mixture-models-em.html#the-gaussian-distribution"><i class="fa fa-check"></i><b>8.1</b> The Gaussian distribution</a></li>
<li class="chapter" data-level="8.2" data-path="gaussian-mixture-models-em.html"><a href="gaussian-mixture-models-em.html#mixture-of-gaussians"><i class="fa fa-check"></i><b>8.2</b> Mixture of Gaussians</a></li>
<li class="chapter" data-level="8.3" data-path="gaussian-mixture-models-em.html"><a href="gaussian-mixture-models-em.html#em-for-gaussian-mixtures"><i class="fa fa-check"></i><b>8.3</b> EM for Gaussian Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html"><i class="fa fa-check"></i>PW 8</a><ul>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#report-template"><i class="fa fa-check"></i>Report template</a></li>
<li class="chapter" data-level="8.4" data-path="pw-8.html"><a href="pw-8.html#em-using-mclust"><i class="fa fa-check"></i><b>8.4</b> EM using <code>mclust</code></a><ul>
<li><a href="pw-8.html#gmm-vs-k-means">GMM vs <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#em-on-1d"><i class="fa fa-check"></i>EM on 1D</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="pw-8.html"><a href="pw-8.html#em-from-scratch"><i class="fa fa-check"></i><b>8.5</b> EM from scratch</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>9</b> Hierarchical Clustering</a><ul>
<li class="chapter" data-level="9.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#dendrogram"><i class="fa fa-check"></i><b>9.1</b> Dendrogram</a></li>
<li class="chapter" data-level="9.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#the-hierarchical-clustering-algorithm"><i class="fa fa-check"></i><b>9.2</b> The Hierarchical Clustering Algorithm</a></li>
<li class="chapter" data-level="9.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#hierarchical-clustering-in-r"><i class="fa fa-check"></i><b>9.3</b> Hierarchical clustering in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-9.html"><a href="pw-9.html"><i class="fa fa-check"></i>PW 9</a><ul>
<li><a href="pw-9.html#distances-dist">Distances <code>dist()</code></a></li>
<li><a href="pw-9.html#dendrogram-hclust">Dendrogram <code>hclust()</code></a></li>
<li class="chapter" data-level="" data-path="pw-9.html"><a href="pw-9.html#hierarchical-clustering-on-iris-dataset"><i class="fa fa-check"></i>Hierarchical clustering on Iris dataset</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="app-introRStudio.html"><a href="app-introRStudio.html"><i class="fa fa-check"></i><b>A</b> Introduction to <code>RStudio</code></a></li>
<li class="chapter" data-level="B" data-path="app-ht.html"><a href="app-ht.html"><i class="fa fa-check"></i><b>B</b> Review on hypothesis testing</a></li>
<li class="chapter" data-level="C" data-path="use-qual.html"><a href="use-qual.html"><i class="fa fa-check"></i><b>C</b> Use of qualitative predictors</a></li>
<li class="chapter" data-level="D" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>D</b> Model Selection</a><ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#linear-model-selection-and-best-subset-selection"><i class="fa fa-check"></i>Linear Model Selection and Best Subset Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward-stepwise-selection"><i class="fa fa-check"></i>Forward Stepwise Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward-stepwise-selection"><i class="fa fa-check"></i>Backward Stepwise Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#estimating-test-error-using-mallows-cp-aic-bic-adjusted-r-squared"><i class="fa fa-check"></i>Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#estimating-test-error-using-cross-validation"><i class="fa fa-check"></i>Estimating Test Error Using Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#examples"><i class="fa fa-check"></i>Examples</a><ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#best-subset-selection"><i class="fa fa-check"></i>Best Subset Selection</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward-stepwise-selection-and-model-selection-using-validation-set"><i class="fa fa-check"></i>Forward Stepwise Selection and Model Selection Using Validation Set</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#model-selection-using-cross-validation"><i class="fa fa-check"></i>Model Selection Using Cross-Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="E" data-path="references-and-credits.html"><a href="references-and-credits.html"><i class="fa fa-check"></i><b>E</b> References and Credits</a></li>
<li class="chapter" data-level="F" data-path="other-references.html"><a href="other-references.html"><i class="fa fa-check"></i><b>F</b> Other References</a></li>
<li class="chapter" data-level="" data-path="main-references-credits.html"><a href="main-references-credits.html"><i class="fa fa-check"></i>Main References &amp; Credits</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gaussian-mixture-models-em" class="section level1">
<h1><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</h1>
<!-- https://www.youtube.com/watch?v=qMTuMa86NzU -->
<!-- To label equation use \begin{equation} and (\#eq:normaldist1) -->
<!-- To cite equation use \@ref(eq:normal01) -->
<p>In the previous chapter we saw the <span class="math inline">\(k\)</span>-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In <span class="math inline">\(k\)</span>-means, a cluster is described only by its centroid. This is not too flexible, as we may have problems with clusters that are <em>overlapping</em>, or ones that are not of <em>circular shape</em>.</p>
<p>In this chapter, we will introduce a model-based clustering technique, which is <strong>E</strong>xpectation <strong>M</strong>aximization (EM). We will apply it using Gaussian Mixture Models (GMM).</p>
<p>With EM Clustering, we can go a step further and describe each cluster by its centroid (mean), covariance (so that we can have elliptical clusters), and weight (the size of the cluster). The probability that a point belongs to a cluster is now given by a multivariate Gaussian probability distribution (multivariate - depending on multiple variables). That also means that we can calculate the probability of a point being under a Gaussian ‘bell’, i.e. the probability of a point belonging to a cluster. A comparison between the performances of <span class="math inline">\(k\)</span>-means and EM clustering on an artificial dataset is shown in Figure <a href="gaussian-mixture-models-em.html#fig:ClusterAnalysisMouse">8.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ClusterAnalysisMouse"></span>
<img src="img/ClusterAnalysis_Mouse.png" alt="Comparison of $k$-means and EM on artificial data called [Mouse](https://elki-project.github.io/datasets/) dataset. Using the Variances, the EM algorithm can describe the normal distributions exact, while $k$-means splits the data in [Voronoi](https://bit.ly/1rVyJkt)-Cells." width="80%" />
<p class="caption">
Figure 8.1: Comparison of <span class="math inline">\(k\)</span>-means and EM on artificial data called <a href="https://elki-project.github.io/datasets/">Mouse</a> dataset. Using the Variances, the EM algorithm can describe the normal distributions exact, while <span class="math inline">\(k\)</span>-means splits the data in <a href="https://bit.ly/1rVyJkt">Voronoi</a>-Cells.
</p>
</div>
<p>We start this chapter by reminding what is a Gaussian distribution, then introduce the Mixture of Gaussians and finish by explaining the Expectation-Maximization algorithm.</p>
<div id="the-gaussian-distribution" class="section level2">
<h2><span class="header-section-number">8.1</span> The Gaussian distribution</h2>
<p>The Gaussian, also known as the normal distribution, is a widely used
model for the distribution of continuous variables. In the case of a
single variable <span class="math inline">\(x\)</span>, the Gaussian distribution can be written in the
form</p>
<p><span class="math display" id="eq:normaldist1">\[\begin{equation}
\mathcal{N}(x|m,\sigma^2)=\frac{1}{(2 \pi \sigma^2 )^{1/2}} \exp \left\lbrace - \frac{1}{2 \sigma^2} (x-m)^2\right\rbrace
\tag{8.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>For a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(X\)</span>, the multivariate Gaussian distribution
take the form</p>
<p><span class="math display" id="eq:normaldist2">\[\begin{equation}
\mathcal{N}(X|\mu,\Sigma)=\frac{1}{(2 \pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp \left\lbrace - \frac{1}{2} (X-\mu)^T \Sigma^{-1} (X-\mu) \right\rbrace
\tag{8.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is a <span class="math inline">\(D\)</span>-dimensional mean vector, <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(D\times D\)</span>
covariance matrix, and <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of <span class="math inline">\(\Sigma\)</span>.</p>
<!-- ```{r} -->
<!-- #          Normal Distribution PDF -->
<!-- #range -->
<!-- x=seq(-5,5,length=300) -->
<!-- #plot each curve -->
<!-- plot(x,dnorm(x,mean=0,sd=sqrt(.2)),type="l",lwd=2,col="blue",main='Normal Distribution PDF',xlim=c(-5,5),ylim=c(0,1),xlab='X', -->
<!-- ylab=latex2exp::TeX('$N(x|m,\\sigma^2)$')) -->
<!-- curve(dnorm(x,mean=0,sd=1), add=TRUE,type="l",lwd=2,col="red") -->
<!-- curve(dnorm(x,mean=0,sd=sqrt(5)), add=TRUE,type="l",lwd=2,col="brown") -->
<!-- curve(dnorm(x,mean=-2,sd=sqrt(.5)), add=TRUE,type="l",lwd=2,col="green") -->
<!-- legend(3,0.9,inset = 0, -->
<!--         legend = latex2exp::TeX(c("$\\mu = 0$, $\\sigma^2 = 0.2$", "$\\mu = 0$, $\\sigma^2 = 1$","$\\mu = 0$, $\\sigma^2 = 5.0$","$\\mu = -2.0$, $\\sigma^2 = 0.5$")),  -->
<!--         col=c("blue","red","brown","green"), lty=1:4, cex=.5, horiz = TRUE) -->
<!-- ``` -->
<p>The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, when we consider the sum of multiple random variables. The <em>central limit theorem</em> (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases.</p>
</div>
<div id="mixture-of-gaussians" class="section level2">
<h2><span class="header-section-number">8.2</span> Mixture of Gaussians</h2>
<p>While the Gaussian distribution has some important analytical properties, it suffers from
significant limitations when it comes to modeling real data sets.
Consider the example shown in Figure <a href="gaussian-mixture-models-em.html#fig:gaussianfaithful">8.2</a> applied on the
’Old Faithful’ data set, this data set comprises 272 measurements of the eruption of the <a href="https://en.wikipedia.org/wiki/Old_Faithful">Old Faithful geyser</a> at Yellowstone National Park in the USA. Each measurement comprises the duration of the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the
data set. Such superpositions, formed by taking linear combinations of
more basic distributions such as Gaussians, can be formulated as
probabilistic models known as <em>mixture distributions</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:gaussianfaithful"></span>
<img src="img/gaussian_faithful.png" alt="Plots of the ’old faithful’ data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data" width="80%" />
<p class="caption">
Figure 8.2: Plots of the ’old faithful’ data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data
</p>
</div>
<p>In Figure <a href="gaussian-mixture-models-em.html#fig:gaussianmixture">8.3</a> we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.</p>
<div class="figure" style="text-align: center"><span id="fig:gaussianmixture"></span>
<img src="img/gaussian_mixture.png" alt="Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red." width="70%" />
<p class="caption">
Figure 8.3: Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red.
</p>
</div>
<p>We therefore consider a superposition of <span class="math inline">\(K\)</span> Gaussian densities of the
form</p>
<p><span class="math display">\[\label{eq:gaussian}
p(x)= \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>which is called a <em>mixture of Gaussians</em>. Each Gaussian density
<span class="math inline">\(\mathcal{N}(x| \mu_k, \Sigma_k)\)</span> is called a <em>component</em> of the mixture
and has its own mean <span class="math inline">\(\mu_k\)</span> and covariance <span class="math inline">\(\Sigma_k\)</span>.</p>
<p>The parameters <span class="math inline">\(\pi_k\)</span> are called <em>mixing coefficients</em>. They verify the
conditions
<span class="math display">\[\sum_{k=1}^{K} \pi_k = 1 \quad \text{and} \quad 0 \leq \pi_k \leq 1\]</span></p>
<p>In order to find an equivalent formulation of the Gaussian mixture
involving an explicit <strong>latent</strong> variable, we introduce a
<span class="math inline">\(K\)</span>-dimensional binary random variable <span class="math inline">\(z\)</span> having a 1-of-<span class="math inline">\(K\)</span>
representation in which a particular element <span class="math inline">\(z_k\)</span> is equal to 1 and all
other elements are equal to 0. The values of <span class="math inline">\(z_k\)</span> therefore satisfy
<span class="math inline">\(z_k \in \{0,1\}\)</span> and <span class="math inline">\(\sum_k z_k =1\)</span>, and we see that there are <span class="math inline">\(K\)</span>
possible states for the vector <span class="math inline">\(z\)</span> according to which element is
nonzero. The marginal distribution over <span class="math inline">\(z\)</span> is specified in terms of the
mixing coefficients <span class="math inline">\(\pi_k\)</span> , such that <span class="math display">\[p(z_k=1)=\pi_k\]</span></p>
<div class="rmdinsight">
<p>
A <a href="https://en.wikipedia.org/wiki/Latent_variable">latent variable</a> is a variable that is not directly measurable, but its value can be inferred by taking other measurements.
</p>
<p>
This happens a lot in machine learning, robotics, statistics and other fields. For example, you may not be able to directly quantify intelligence (it’s not a countable thing like the number of brain cells you have), but we think it exists and we can run experiments that may tell us about intelligence. So your intelligence is a latent variable that affects your performance on multiple tasks even though it can not be directly measured (<a href="https://www.quora.com/What-is-a-latent-variable">link</a>).
</p>
</div>
<p>The conditional distribution of <span class="math inline">\(x\)</span> given a particular value for <span class="math inline">\(z\)</span> is
a Gaussian</p>
<p><span class="math display">\[p(x|z_k=1)= \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>The joint distribution is given by <span class="math inline">\(p(z)p(x|z)\)</span>, and the marginal
distribution of <span class="math inline">\(x\)</span> is then obtained by summing the joint distribution
over all possible states of <span class="math inline">\(z\)</span> to give</p>
<p><span class="math display">\[p(x)= \sum_z p(z)p(x|z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>Now, we are able to work with the joint distribution <span class="math inline">\(p(x|z)\)</span> instead of
the marginal distribution <span class="math inline">\(p(x)\)</span>. This leads to significant
simplification, most notably through the introduction of the
Expectation-Maximization (EM) algorithm.</p>
<p>Another quantity that play an important role is the conditional
probability of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>. We shall use <span class="math inline">\(r(z_k)\)</span> to denote
<span class="math inline">\(p(z_k = 1|x)\)</span>, whose value can be found using Bayes’ theorem</p>
<p><span class="math display" id="eq:responsibilities">\[\begin{align}
r(z_k)= p(z_k = 1|x) &amp;= \frac{ p(z_k = 1) p(x|(z_k=1)}{\displaystyle \sum_{j=1}^{K} p(z_j = 1) p(x|(z_j=1)} \nonumber \\
&amp;= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)} 
\tag{8.3}
\end{align}\]</span></p>
<p>We shall view <span class="math inline">\(\pi_k\)</span> as the prior probability of <span class="math inline">\(z_k = 1\)</span>, and the
quantity <span class="math inline">\(r(z_k)\)</span> as the corresponding posterior probability once we
have observed <span class="math inline">\(x\)</span>. As we shall see in next section, <span class="math inline">\(r(z_k)\)</span> can also be
viewed as the <em>responsibility</em> that component <span class="math inline">\(k\)</span> takes for ’explaining’
the observation <span class="math inline">\(x\)</span>.</p>
<div class="rmdtip">
<p>
Doesn’t this reminds you of the Equation <a href="discriminant-analysis.html#eq:bayes">4.1</a> when we used Bayes’ theorm for Classification? where <span class="math inline"><span class="math inline">\(p_k(x)\)</span></span> was the <em>posterior</em> probability that an observation <span class="math inline"><span class="math inline">\(X=x\)</span></span> belongs to <span class="math inline"><span class="math inline">\(k\)</span></span>-th class. The difference is that here the data is unlabled (we have no class), so we create a <strong>latent</strong> (hidden, unobserved) variable <span class="math inline"><span class="math inline">\(z\)</span></span> that will play a similar role.
</p>
</div>
<p>In Figure <a href="gaussian-mixture-models-em.html#fig:gaussianmixture500samples">8.4</a> the role of the responsibilities is illustrated on a sample of 500 points drawn from a mixture of three Gaussians.</p>
<div class="figure" style="text-align: center"><span id="fig:gaussianmixture500samples"></span>
<img src="img/gaussianmixture_500samples.png" alt="Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution $p(z)p(x|z)$ in which the three states of $z$, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution $p(x)$, which is obtained by simply ignoring the values of $z$ and just plotting the $x$ values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities $r(z_{nk})$ associated with data point $x_n$, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by $r(z_{nk})$ for $k = 1,2,3$, respectively." width="90%" />
<p class="caption">
Figure 8.4: Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution <span class="math inline">\(p(z)p(x|z)\)</span> in which the three states of <span class="math inline">\(z\)</span>, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution <span class="math inline">\(p(x)\)</span>, which is obtained by simply ignoring the values of <span class="math inline">\(z\)</span> and just plotting the <span class="math inline">\(x\)</span> values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities <span class="math inline">\(r(z_{nk})\)</span> associated with data point <span class="math inline">\(x_n\)</span>, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by <span class="math inline">\(r(z_{nk})\)</span> for <span class="math inline">\(k = 1,2,3\)</span>, respectively.
</p>
</div>
<p>So the form of the Gaussian mixture distribution is governed by the parameters <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where we have used the notation <span class="math inline">\(\pi=\{\pi_1,\ldots,\pi_K\}\)</span>, <span class="math inline">\(\mu=\{\mu_1,\ldots,\mu_K\}\)</span> and <span class="math inline">\(\Sigma=\{\Sigma_1,\ldots,\Sigma_K\}\)</span>. One way to set the values of these parameters is to use maximum likelihood. The log of the likelihood function is given by</p>
<p><span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span>
We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over <span class="math inline">\(k\)</span> inside the logarithm. As a result, the maximum likelihood solution for the
parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques. Alternatively we can employ a powerful framework called <strong>E</strong>xpectation <strong>M</strong>aximization, which
will be discussed in this chapter.</p>
</div>
<div id="em-for-gaussian-mixtures" class="section level2">
<h2><span class="header-section-number">8.3</span> EM for Gaussian Mixtures</h2>
<p>Suppose we have a data set of observations <span class="math inline">\(\{x_1, \ldots, x_N\}\)</span>, which
gives a data set <span class="math inline">\(X\)</span> of size <span class="math inline">\(N \times D\)</span>, and we wish to model this data using a mixture of
Gaussians. Similarly, the corresponding latent variable are denoted by
a <span class="math inline">\(N \times K\)</span> matrix <span class="math inline">\(Z\)</span> with rows <span class="math inline">\(z_n^K\)</span>.</p>
<div class="rmdinsight">
<p>
Recall that the objective is to estimate the parameters <span class="math inline"><span class="math inline">\(\pi\)</span></span>, <span class="math inline"><span class="math inline">\(\mu\)</span></span> and <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> in order to estimate the posterior probabilities (named also <em>responsibilities</em>, called <span class="math inline"><span class="math inline">\(\, r(z_k)\)</span></span> in this chapter). To do so, we find the estimators that maximize the log of the likelihood function.
</p>
</div>
<p>If we assume that the data points are i.i.d. (independent and
identically distributed), then we can calculate the log of the
likelihood function, which is given by</p>
<p><span class="math display" id="eq:gaussianlikelihood">\[\begin{equation}
\tag{8.4}
\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace
\end{equation}\]</span></p>
<p>An elegant and powerful method for finding maximum likelihood solutions
for this models with latent variables is called the
<strong>E</strong>xpectation <strong>M</strong>aximization algorithm, or EM algorithm.</p>
<p>Setting the derivatives of <span class="math inline">\(\ln p(X|\pi,\mu,\Sigma)\)</span> in
<a href="gaussian-mixture-models-em.html#eq:gaussianlikelihood">(8.4)</a> respectively with respect to the
<span class="math inline">\(\mu_k,\Sigma_k\)</span> and <span class="math inline">\(\pi_k\)</span> to zero, we obtain</p>
<p><span class="math display" id="eq:means">\[\begin{equation}
\tag{8.5}
\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n
\end{equation}\]</span></p>
<p>where we define <span class="math display">\[N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p>
<p>We can interpret <span class="math inline">\(N_k\)</span> as the effective number of points assigned to cluster <span class="math inline">\(k\)</span>. Note
carefully the form of this solution. We see that the mean <span class="math inline">\(\mu_k\)</span> for the <span class="math inline">\(k\)</span>-th Gaussian
component is obtained by taking a weighted mean of all of the points in the data set,
in which the weighting factor for data point <span class="math inline">\(x_n\)</span> is given by the posterior probability
<span class="math inline">\(r(z_{nk})\)</span> that component <span class="math inline">\(k\)</span> was responsible for generating <span class="math inline">\(x_n\)</span>.</p>
<p>As for <span class="math inline">\(\sigma_k\)</span> we obtain</p>
<p><span class="math display" id="eq:sigma">\[\begin{equation}
\tag{8.6}
\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T
\end{equation}\]</span></p>
<p>which has the same form as the corresponding result for a single Gaussian fitted to
the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points
associated with the corresponding component.</p>
<p>Finally, for the mixing coefficients <span class="math inline">\(\pi_k\)</span> we obtain</p>
<p><span class="math display" id="eq:pi">\[\begin{equation}
\tag{8.7}
\pi_k=\frac{N_k}{N}
\end{equation}\]</span></p>
<p>so that the mixing coefficient for the <span class="math inline">\(k\)</span>-th component is given by the average responsibility which that component takes for explaining the data points.</p>
<p>We first choose some initial values for the means, covariances, and
mixing coefficients. Then we alternate between the following two updates
that we shall call the <strong>E</strong> step and the <strong>M</strong> step. In the <em>expectation</em> step,
or E step, we use the current values for the parameters to evaluate the
posterior probabilities, or responsibilities, given by Eq.
<a href="gaussian-mixture-models-em.html#eq:responsibilities">(8.3)</a>. We then use these probabilities in the
<em>maximization</em> step, or M step, to re-estimate the means, covariances,
and mixing coefficients using the results in Equations <a href="gaussian-mixture-models-em.html#eq:means">(8.5)</a>,
<a href="gaussian-mixture-models-em.html#eq:sigma">(8.6)</a> and <a href="gaussian-mixture-models-em.html#eq:pi">(8.7)</a>. The algorithm of EM for mixtures of
Gaussians is shown in the following Algorithm:</p>
<table>
<colgroup>
<col width="37%" />
<col width="62%" />
</colgroup>
<thead>
<tr class="header">
<th>The EM for Gaussian mixtures</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data</strong>:</td>
<td><span class="math inline">\(\mathbf{X}= \{x_{kd}, \,\,\,\, k=1,\ldots,N, d=1,\ldots,D\}\)</span>
where <span class="math inline">\(D\)</span> is the dimension of the feature space. <span class="math inline">\(Z\)</span> the latent variables matrix.</td>
</tr>
<tr class="even">
<td><strong>Result</strong>:</td>
<td>Posterior probabilities <span class="math inline">\(r(z_{nk})\)</span> and the model parameters <span class="math inline">\(\mu,\Sigma\)</span> and <span class="math inline">\(\pi\)</span>.</td>
</tr>
<tr class="odd">
<td><strong>Initialization</strong>:</td>
<td><ul>
<li>Choose a value for <span class="math inline">\(K\)</span>, <span class="math inline">\(1 &lt; K &lt; N\)</span>.</li>
<li>Initialize the means <span class="math inline">\(\mu_k\)</span>, the covariances <span class="math inline">\(\Sigma_k\)</span> and mixing coefficients <span class="math inline">\(\pi_k\)</span> randomly.</li>
<li>Evaluate the initial value of the log likelihood.</li>
</ul></td>
</tr>
<tr class="even">
<td><strong>Learning</strong>: <strong>repeat</strong></td>
<td><p><strong>E step</strong>:</p>
<ul>
<li>Evaluate the responsibilities using the current parameter values:
<span class="math display">\[r(z_{nk})= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)}\]</span></li>
</ul>
<p><strong>M step</strong>:</p>
<ul>
<li><p>Re-estimate the parameters using the current responsibilities:
<span class="math display">\[\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n\]</span>
<span class="math display">\[\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T\]</span>
<span class="math display">\[\pi_k=\frac{N_k}{N}\]</span>
<span class="math display">\[\text{where} \quad N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p></li>
<li><p>Evaluate the log likelihood:
<span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span>
Until convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to E step.</p></li>
</ul></td>
</tr>
</tbody>
</table>
<p>The EM algorithm for a mixture of two Gaussians applied to the rescaled
Old Faithful data set is illustrated in Figure <a href="gaussian-mixture-models-em.html#fig:emfaithful">8.5</a>. In
plot (a) we see the initial configuration, the Gaussian component are
shown as blue and red circles. Plot (b) shows the result of the initial
E step where we update the responsibilities. Plot (c) shows the M step
where we update the parameters. Plots (d), (e), and (f) show the results
after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the
algorithm is close to convergence.</p>
<div class="figure" style="text-align: center"><span id="fig:emfaithful"></span>
<img src="img/em_faithful.png" alt="Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used." width="80%" />
<p class="caption">
Figure 8.5: Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used.
</p>
</div>
<!-- 
## The EM Algorithm in General

In this section, we present the general view of the EM algorithm. The
goal of the EM algorithm is to find maximum likelihood solutions for
models having latent variables. We denote $X$ the data matrix, $Z$ the
latent variables matrix. Let us denote $\theta$ the set of all model
parameters. Then the log likelihood function is given by

$$\ln p(X|\theta) = \ln \left\lbrace \sum_Z p(X,Z|\theta) \right\rbrace$$

Note that if the latent variables are continuous we get similar
equations, we only replace the over $Z$ with an integral.

The presence of the sum prevents the logarithm from acting directly on
the joint distribution, resulting in complicated expressions for the
maximum likelihood solution.

Suppose that, for each observation in $X$, we were told the
corresponding value of the latent variable $Z$. We shall call $\{X,Z\}$
the *complete* data set, and we shall refer to the actual observed data
$X$ as *incomplete*. The likelihood function for the complete data set
simply takes the form $\ln p(X,Z|\theta)$, and we shall suppose that
maximization of this complete-data log likelihood function is
straightforward.

**Initialization**:\
-Choose an initial setting for the parameters $\theta^{\text{old}}$.\
**Learning**: repeat\
Until convergence of either the parameters or the log likelihood. If the
convergence criterion is not satisfied then let $$\theta^{\text{old}}
\leftarrow \theta^{\text{new}}$$ and return to the E step.

In practice, however, we are not given the complete data set $\{X,Z\}$,
but only the incomplete data $X$. Our state of knowledge of the values
of the latent variables in $Z$ is given only by the posterior
distribution $p(Z|X, \theta)$. Because we cannot use the complete-data
log likelihood, we consider instead its expected value under the
posterior distribution of the latent variable, which corresponds to the
E step of the EM algorithm. In the subsequent M step, we maximize this
expectation. If the current estimate for the parameters is denoted
$\theta^{\text{old}}$, then a pair of successive E and M steps gives
rise to a revised estimate $\theta^{\text{new}}$. The algorithm is
initialized by choosing some starting value for the parameters
$\theta_0$.

In the E step, we use the current parameter values $\theta^{\text{old}}$
to find the posterior distribution of the latent variables given by
$p(Z|X, \theta^{\text{old}})$. We then use this posterior distribution
to find the expectation of the complete-data log likelihood evaluated
for some general parameter value $\theta$. This expectation, denoted
$\mathcal{Q}(\theta,\theta^{\text{old}})$, is given by

$$\label{eq:complete-likelihood}
 \mathcal{Q}(\theta,\theta^{\text{old}}) = \sum_Z p(Z|X, \theta^{\text{old}}) \ln p(X,Z|\theta)$$

In the M step, we determine the revised parameter estimate
$\theta^{\text{new}}$ by maximizing this function

$$\theta^{\text{new}} = \argmax_{\theta}  \mathcal{Q}(\theta,\theta^{\text{old}})$$

Note that in the definition of
$\mathcal{Q}(\theta,\theta^{\text{old}})$, the logarithm acts directly
on the joint distribution $p(X,Z|\theta)$, so the corresponding M-step
maximization will, by supposition, be tractable.

The general EM algorithm is summarized in Algorithm \[algo:em:general\].
It has the property that each cycle of EM will increase the
incomplete-data log likelihood (unless it is already at a local
maximum). -->
<div class="rmdinsight">
<p>
Summary of this chapter:
</p>
<ul>
<li>
Gaussian Mixture Models (GMM) take a Gaussian and add another Gaussian(s).
</li>
<li>
This allows to model more complex data.
</li>
<li>
We fit a GMM with the Expectation-Maximization (EM) algorithm.
</li>
<li>
Expectation-Maximization (EM) algorithm is a series of steps to find good parameter estimates when there are latent variables.
</li>
<li>
EM steps:
<ol style="list-style-type: decimal">
<li>
Initialize t he parameter estimates.
</li>
<li>
Given the current parameter estimates, find the minimum log likelihood for <span class="math inline"><span class="math inline">\(Z\)</span></span> (data + latent variables).
</li>
<li>
Givent the current data, find better parameter estimates.
</li>
<li>
Repeat steps 2 &amp; 3.
</li>
</ol>
</li>
</ul>
</div>
<p align="right">
◼
</p>

</div>
</div>
<h1>Discussion</h1>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://mlcourse.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="pw-7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pw-8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
