<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 3 Estimation ponctuelle | Statistique Inférentielle</title>
  <meta name="description" content="Cours de Statistique Inférentielle A3 ESILV" />
  <meta name="generator" content="bookdown 0.13.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 3 Estimation ponctuelle | Statistique Inférentielle" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Cours de Statistique Inférentielle A3 ESILV" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 3 Estimation ponctuelle | Statistique Inférentielle" />
  
  <meta name="twitter:description" content="Cours de Statistique Inférentielle A3 ESILV" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2019-10-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="tp-illustration-numerique-des-theoremes-limites-avec-r.html"/>
<link rel="next" href="intervalle-de-confiance.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="book_assets/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script async defer src="https://hypothes.is/embed.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_tout_blanc.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Statistique Inférentielle</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#plan-du-cours"><i class="fa fa-check"></i>Plan du cours</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#la-demarche-statistique"><i class="fa fa-check"></i>La démarche statistique</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#objectifs-et-plan-du-cours"><i class="fa fa-check"></i>Objectifs et plan du cours</a></li>
</ul></li>
<li class="part"><span><b>Statistique descriptive</b></span></li>
<li class="chapter" data-level="1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html"><i class="fa fa-check"></i><b>1</b> Statistique descriptive</a><ul>
<li class="chapter" data-level="1.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#terminologie"><i class="fa fa-check"></i><b>1.1</b> Terminologie</a></li>
<li class="chapter" data-level="1.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#statistique-et-probabilites"><i class="fa fa-check"></i><b>1.2</b> Statistique et Probabilités</a></li>
<li class="chapter" data-level="1.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#description-dune-serie-de-valeurs"><i class="fa fa-check"></i><b>1.3</b> Description d’une série de valeurs</a></li>
<li class="chapter" data-level="1.4" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#representations-graphiques"><i class="fa fa-check"></i><b>1.4</b> Représentations graphiques</a><ul>
<li class="chapter" data-level="1.4.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#variable-qualitative"><i class="fa fa-check"></i><b>1.4.1</b> Variable qualitative</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#variable-quantitative"><i class="fa fa-check"></i><b>1.4.2</b> Variable quantitative</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#indicateurs-statistiques"><i class="fa fa-check"></i><b>1.5</b> Indicateurs statistiques</a><ul>
<li class="chapter" data-level="1.5.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#tendance-centrale"><i class="fa fa-check"></i><b>1.5.1</b> Tendance centrale</a></li>
<li class="chapter" data-level="1.5.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#dispersion-variabilite"><i class="fa fa-check"></i><b>1.5.2</b> Dispersion (variabilité)</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#statistique-descriptive-bidimensionnelle"><i class="fa fa-check"></i><b>1.6</b> Statistique descriptive bidimensionnelle</a><ul>
<li class="chapter" data-level="" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#deux-variables-quantitatives"><i class="fa fa-check"></i>Deux variables quantitatives</a></li>
<li class="chapter" data-level="" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#une-variable-quantitative-et-une-qualitative"><i class="fa fa-check"></i>Une variable quantitative et une qualitative</a></li>
<li class="chapter" data-level="" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#deux-variables-qualitatives"><i class="fa fa-check"></i>Deux variables qualitatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html"><i class="fa fa-check"></i>Exercices</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html"><i class="fa fa-check"></i>TP Statistique descriptive avec R</a><ul>
<li><a href="tp-statistique-descriptive-avec-r.html#quest-ce-que-cest-que">Qu’est-ce que c’est que <svg style="height:0.8em;top:.04em;position:relative;fill:steelblue;" viewBox="0 0 581 512"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"/></svg>?</a><ul>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#rstudio"><i class="fa fa-check"></i>RStudio</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#ere-partie-donnees-quantitatives-discretes"><i class="fa fa-check"></i>1ère partie: Données quantitatives discrètes</a><ul>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#effectifs-et-frequence"><i class="fa fa-check"></i>Effectifs et fréquence</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#mesures-de-tendance-centrale"><i class="fa fa-check"></i>Mesures de tendance centrale</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#indicateurs-de-dispersion"><i class="fa fa-check"></i>Indicateurs de dispersion</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#representations-graphiques-1"><i class="fa fa-check"></i>Représentations graphiques</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#eme-partie-analyse-descriptive"><i class="fa fa-check"></i>2ème partie : Analyse descriptive</a><ul>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#donnees-utilisees"><i class="fa fa-check"></i>Données utilisées</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#definition-du-repertoire-de-travail"><i class="fa fa-check"></i>Définition du répertoire de travail</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#chargement-des-donnees"><i class="fa fa-check"></i>Chargement des données</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#analyse-descriptive-univariee"><i class="fa fa-check"></i>Analyse descriptive univariée</a></li>
<li class="chapter" data-level="" data-path="tp-statistique-descriptive-avec-r.html"><a href="tp-statistique-descriptive-avec-r.html#analyse-descriptive-bivariee"><i class="fa fa-check"></i>Analyse descriptive bivariée</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Statistique Inférentielle</b></span></li>
<li class="chapter" data-level="2" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html"><i class="fa fa-check"></i><b>2</b> Échantillonnage et Théorèmes limites</a><ul>
<li class="chapter" data-level="2.1" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#echantillonnage"><i class="fa fa-check"></i><b>2.1</b> Échantillonnage</a></li>
<li class="chapter" data-level="2.2" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#la-statistique-overlinex_n"><i class="fa fa-check"></i><b>2.2</b> La statistique <span class="math inline">\(\overline{X}_n\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#theoremes-limites"><i class="fa fa-check"></i><b>2.3</b> Théorèmes limites</a><ul>
<li class="chapter" data-level="2.3.1" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#loi-faible-des-grands-nombres"><i class="fa fa-check"></i><b>2.3.1</b> Loi faible des grands nombres</a></li>
<li class="chapter" data-level="2.3.2" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#loi-forte-des-grands-nombres"><i class="fa fa-check"></i><b>2.3.2</b> Loi forte des grands nombres</a></li>
<li class="chapter" data-level="" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#illustration-de-la-loi-des-grands-nombres"><i class="fa fa-check"></i>Illustration de la loi des grands nombres</a></li>
<li class="chapter" data-level="2.3.3" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#theoreme-central-limite"><i class="fa fa-check"></i><b>2.3.3</b> Théorème central limite</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#loi-dun-pourcentage"><i class="fa fa-check"></i><b>2.4</b> Loi d’un pourcentage</a></li>
<li class="chapter" data-level="2.5" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#etude-de-la-statistique-s2"><i class="fa fa-check"></i><b>2.5</b> Etude de la statistique <span class="math inline">\(S^2\)</span></a></li>
<li class="chapter" data-level="2.6" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#introduction-a-lestimation"><i class="fa fa-check"></i><b>2.6</b> Introduction à l’estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercices-1.html"><a href="exercices-1.html"><i class="fa fa-check"></i>Exercices</a></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><i class="fa fa-check"></i>TP illustration numérique des théorèmes limites avec R</a><ul>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#realisations-de-variables-aleatoires"><i class="fa fa-check"></i>Réalisations de variables aléatoires</a></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#fonction-de-densite"><i class="fa fa-check"></i>Fonction de densité</a></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#fonction-de-repartition"><i class="fa fa-check"></i>Fonction de répartition</a></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#illustrations-de-theoremes-limites"><i class="fa fa-check"></i>Illustrations de théorèmes limites</a><ul>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#loi-forte-de-grands-nombres"><i class="fa fa-check"></i>Loi forte de grands nombres</a></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#theoreme-central-limite-1"><i class="fa fa-check"></i>Théorème central limite</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="tp-illustration-numerique-des-theoremes-limites-avec-r.html"><a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html#bonus"><i class="fa fa-check"></i>Bonus</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html"><i class="fa fa-check"></i><b>3</b> Estimation ponctuelle</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#methodes-destimation"><i class="fa fa-check"></i><b>3.2</b> Méthodes d’estimation</a></li>
<li class="chapter" data-level="3.3" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-methode-des-moments"><i class="fa fa-check"></i><b>3.3</b> La méthode des moments</a><ul>
<li class="chapter" data-level="3.3.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#lestimateur-des-moments-emm"><i class="fa fa-check"></i><b>3.3.1</b> L’estimateur des moments (EMM)</a></li>
<li class="chapter" data-level="3.3.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#exemples"><i class="fa fa-check"></i><b>3.3.2</b> Exemples</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-methode-du-maximum-de-vraisemblance"><i class="fa fa-check"></i><b>3.4</b> La méthode du maximum de vraisemblance</a><ul>
<li class="chapter" data-level="3.4.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-fonction-de-vraisemblance"><i class="fa fa-check"></i><b>3.4.1</b> La fonction de vraisemblance</a></li>
<li class="chapter" data-level="3.4.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#lestimateur-de-maximum-de-vraisemblance-emv"><i class="fa fa-check"></i><b>3.4.2</b> L’estimateur de maximum de vraisemblance (EMV)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#qualite-dun-estimateur"><i class="fa fa-check"></i><b>3.5</b> Qualité d’un estimateur</a><ul>
<li class="chapter" data-level="3.5.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#estimateur-sans-biais-et-de-variance-minimale-esbvm"><i class="fa fa-check"></i><b>3.5.1</b> Estimateur sans biais et de variance minimale (ESBVM)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#proprietes-des-estimateurs-des-moments-emm"><i class="fa fa-check"></i><b>3.6</b> Propriétés des estimateurs des moments (EMM)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="intervalle-de-confiance.html"><a href="intervalle-de-confiance.html"><i class="fa fa-check"></i><b>4</b> Intervalle de confiance</a></li>
<li class="chapter" data-level="5" data-path="tests-dhpotheses.html"><a href="tests-dhpotheses.html"><i class="fa fa-check"></i><b>5</b> Tests d’hpothèses</a></li>
<li class="part"><span><b>Rappel Probabilités</b></span></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html"><i class="fa fa-check"></i>Variables Aléatoires Discrètes</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#notions-de-probabilites"><i class="fa fa-check"></i>Notions de Probabilités</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#espace-probabilisable"><i class="fa fa-check"></i>Espace Probabilisable</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#notions-sur-les-evenements"><i class="fa fa-check"></i>Notions sur les Evénements</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#espace-probabilise"><i class="fa fa-check"></i>Espace Probabilisé</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#probabilite-proprietes"><i class="fa fa-check"></i>Probabilité: Propriétés</a></li>
<li><a href="variables-aleatoires-discretes.html#probabilite-uniforme-sur-omega-fini"><span>Probabilité uniforme sur <span class="math inline">\(\Omega\)</span> fini</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#probabilite-conditionnelle"><i class="fa fa-check"></i>Probabilité conditionnelle</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#formule-de-bayes"><i class="fa fa-check"></i>Formule de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#notion-de-variable-aleatoire-reelle-v.a.r."><i class="fa fa-check"></i>Notion de variable aléatoire réelle (v.a.r.)</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#variables-aleatoires-discretes-1"><i class="fa fa-check"></i>Variables aléatoires discrètes</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#definition-loi-de-probabilite"><i class="fa fa-check"></i>Définition, loi de probabilité</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#fonction-de-repartition-dune-variable-aleatoire-discrete"><i class="fa fa-check"></i>Fonction de répartition d’une variable aléatoire discrète</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#moments-dune-variable-aleatoire-discrete"><i class="fa fa-check"></i>Moments d’une variable aléatoire discrète</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#esperance-mathematique"><i class="fa fa-check"></i>Espérance mathématique</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#esperance-dune-fonction-dune-variable-aleatoire"><i class="fa fa-check"></i>Espérance d’une fonction d’une variable aléatoire</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#variance"><i class="fa fa-check"></i>Variance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#ecart-type"><i class="fa fa-check"></i>Ecart-type</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#moments-non-centres-et-centres"><i class="fa fa-check"></i>Moments non centrés et centrés</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#couple-de-variables-aleatoires-discretes"><i class="fa fa-check"></i>Couple de variables aléatoires discrètes</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#table-de-probabilite-conjointe"><i class="fa fa-check"></i>Table de probabilité conjointe</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-marginales"><i class="fa fa-check"></i>Lois marginales</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-conditionnelles"><i class="fa fa-check"></i>Lois conditionnelles</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#independance-de-variables-aleatoires"><i class="fa fa-check"></i>Indépendance de variables aléatoires</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#coefficient-de-correlation-lineaire-1"><i class="fa fa-check"></i>Coefficient de corrélation linéaire</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-usuelles-discretes"><i class="fa fa-check"></i>Lois usuelles discrètes</a><ul>
<li><a href="variables-aleatoires-discretes.html#loi-uniforme-discrete-mathcalun">Loi uniforme discrète <span class="math inline">\(\mathcal{U}(n)\)</span></a></li>
<li><a href="variables-aleatoires-discretes.html#loi-de-bernoulli-mathcalbp">Loi de Bernoulli <span class="math inline">\(\mathcal{B}(p)\)</span></a></li>
<li><a href="variables-aleatoires-discretes.html#loi-binomiale-mathcalbnp">Loi Binomiale <span class="math inline">\(\mathcal{B}(n,p)\)</span></a></li>
<li><a href="variables-aleatoires-discretes.html#loi-de-poisson-mathcalplambda">Loi de Poisson <span class="math inline">\(\mathcal{P}(\lambda)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#approximation-dune-loi-binomiale"><i class="fa fa-check"></i>Approximation d’une loi binomiale</a></li>
<li><a href="variables-aleatoires-discretes.html#loi-geometrique-ou-de-pascal-mathcalgp">Loi Géométrique ou de Pascal <span class="math inline">\(\mathcal{G}(p)\)</span></a></li>
<li><a href="variables-aleatoires-discretes.html#loi-binomiale-negative-mathcalbnrp">Loi Binomiale Négative <span class="math inline">\(\mathcal{BN}(r,p)\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html"><i class="fa fa-check"></i>Variables Aléatoires Continues</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densite-dune-variable-aleatoire-continue"><i class="fa fa-check"></i>Densité d’une variable aléatoire continue</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#fonction-de-repartition-dune-v.a.c"><i class="fa fa-check"></i>Fonction de répartition d’une v.a.c</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#fonction-dune-variable-aleatoire-continue"><i class="fa fa-check"></i>Fonction d’une variable aléatoire continue</a><ul>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxaxb">Calcul de densités pour <span class="math inline">\(h(X)=aX+b\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxx2">Calcul de densités pour <span class="math inline">\(h(X)=X^2\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxex">Calcul de densités pour <span class="math inline">\(h(X)=e^X\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-et-variance-de-variables-aleatoires-continues"><i class="fa fa-check"></i>Espérance et variance de variables aléatoires continues</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-dune-v.a.c"><i class="fa fa-check"></i>Espérance d’une v.a.c</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#variance-dune-v.a.c"><i class="fa fa-check"></i>Variance d’une v.a.c</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#lois-usuelles-de-v.a.c"><i class="fa fa-check"></i>Lois usuelles de v.a.c</a><ul>
<li><a href="variables-aleatoires-continues.html#loi-uniforme-uab">Loi uniforme <span class="math inline">\(U(a,b)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-exponentielle-mathcalelambda">Loi exponentielle <span class="math inline">\(\mathcal{E}(\lambda)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-normale-ou-de-laplace-gauss-mathcalnmusigma2">Loi Normale ou de Laplace-Gauss <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#etude-de-la-densite-de-la-loi-normale"><i class="fa fa-check"></i>Étude de la densité de la loi Normale</a></li>
<li><a href="variables-aleatoires-continues.html#loi-normale-centree-reduite-mathcaln01">Loi Normale centrée réduite <span class="math inline">\(\mathcal{N}(0,1)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#relation-entre-loi-normale-et-loi-normale-centree-reduite"><i class="fa fa-check"></i>Relation entre loi normale et loi normale centrée réduite</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#calcul-des-probabilites-dune-loi-normale"><i class="fa fa-check"></i>Calcul des probabilités d’une loi normale</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#approximation-normale-dune-repartition-binomiale"><i class="fa fa-check"></i>Approximation normale d’une répartition binomiale</a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-chi2-de-pearson">Loi de <span class="math inline">\(\chi^{2}\)</span> de Pearson</a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-student-stn">Loi de Student <span class="math inline">\(St(n)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-fisher-snedecor-mathcalfnm">Loi de Fisher-Snedecor <span class="math inline">\(\mathcal{F}(n,m)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#couple-de-variables-aleatoires-continues"><i class="fa fa-check"></i>Couple de variables aléatoires continues</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densite-conjointe"><i class="fa fa-check"></i>Densité conjointe</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densites-marginales"><i class="fa fa-check"></i>Densités marginales</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-dune-fonction-du-couple"><i class="fa fa-check"></i>Espérance d’une fonction du couple</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#independance"><i class="fa fa-check"></i>Indépendance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#distribution-conditionnelle"><i class="fa fa-check"></i>Distribution conditionnelle</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Annexe</b></span></li>
<li class="chapter" data-level="A" data-path="app-introRStudio.html"><a href="app-introRStudio.html"><i class="fa fa-check"></i><b>A</b> Introduction to <code>RStudio</code></a></li>
<li class="chapter" data-level="B" data-path="tab-normale.html"><a href="tab-normale.html"><i class="fa fa-check"></i><b>B</b> Table de la loi Normale centrée réduite</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistique Inférentielle</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div class = rmdreview>
Vous êtes invités à annoter le contenu de ce cours. Les annotations peuvent être des corrections typographiques, des propositions ou des questions. Pour ajouter des annotations, <span style="background-color: #3297FD; color: white">choisissez le text</span> que vous voulez commenter et cliquez sur <i class="h-icon-annotate"></i>. Pour accéder aux annotations crées par d'autres personnes, cliquez <i class="h-icon-chevron-left"></i> sur le coin supérieur de la page <i class="fa fa-arrow-circle-right  fa-rotate-315" aria-hidden="true"></i>.
</div>
<div id="estimation-ponctuelle" class="section level1">
<h1><span class="header-section-number">Chapitre 3</span> Estimation ponctuelle</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>Dans ce chapitre, on suppose que les données <span class="math inline">\(x_1,\ldots,x_n\)</span> sont <span class="math inline">\(n\)</span> réalisations indépendantes d’une même variable aléatoire sous-jacente <span class="math inline">\(X\)</span>. Il est équivalent de supposer que <span class="math inline">\(x_1,\ldots,x_n\)</span> sont les réalisations de variables aléatoires <span class="math inline">\(X_1,\ldots,X_n\)</span> indépendantes et de même loi. Nous adopterons ici la seconde formulation, qui est plus pratique à manipuler.</p>
<p>Les techniques de statistique descriptive, comme l’histogramme ou le graphe de probabilités, permettent de faire des hypothèses sur la nature de la loi de probabilité des <span class="math inline">\(X_i\)</span>. Des techniques statistiques plus sophistiquées, les tests d’adéquation, permettent de valider ou pas ces hypothèses.</p>
<p>On supposera ici que ces techniques ont permis d’adopter une famille de lois de probabilité bien précise (par exemple, loi normale, loi de Poisson, etc.) pour la loi des <span class="math inline">\(X_i\)</span>, mais que la valeur du ou des paramètres de cette loi est inconnue.</p>
<p>On notera <span class="math inline">\(\theta\)</span> le paramètre inconnu. Le problème traité dans ce chapitre est celui de
l’estimation du paramètre <span class="math inline">\(\theta\)</span>. Comme on l’a déjà dit, il s’agit de donner, au vu des observations <span class="math inline">\(x_1,\ldots,x_n\)</span>, une approximation ou une évaluation de <span class="math inline">\(\theta\)</span> que l’on espère la plus proche possible de la vraie valeur inconnue. On pourra proposer une unique valeur vraisemblable pour <span class="math inline">\(\theta\)</span> (estimation ponctuelle, dans ce chapitre) ou un ensemble de valeurs vraisemblables (estimation ensembliste ou région de confiance, dans le chapitre suivant).</p>
<p>On notera <span class="math inline">\(F(x;\theta)\)</span> la fonction de répartition des <span class="math inline">\(X_i\)</span>. Pour les variables aléatoires discrètes on notera <span class="math inline">\(P(X = x;\theta)\)</span> les probabilités élémentaires, et pour les variables aléatoires continues on notera <span class="math inline">\(f(x;\theta)\)</span> la densité. Par exemple, quand X est de loi exponentielle exp(<span class="math inline">\(\lambda\)</span>), on aura <span class="math inline">\(F(x;\lambda) = 1 − e^{−\lambda x}\)</span> et <span class="math inline">\(f(x;\lambda) = \lambda e^{−\lambda x}\)</span>.</p>
</div>
<div id="methodes-destimation" class="section level2">
<h2><span class="header-section-number">3.2</span> Méthodes d’estimation</h2>
<p>Il existe de nombreuses méthodes pour estimer un paramètre <span class="math inline">\(\theta\)</span>.
Dans cette section, nous ne nous intéressons qu’aux deux méthodes d’estimation les
plus usuelles, la méthode des moments et la méthode du maximum de vraisemblance.</p>
<p>Mais il faut d’abord définir précisément ce que sont une estimation et surtout un
estimateur.</p>
<p>Pour estimer <span class="math inline">\(\theta\)</span> on ne dispose que des données <span class="math inline">\(x_1,\ldots,x_n\)</span>, donc une estimation de <span class="math inline">\(\theta\)</span> sera une fonction de ces observations.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-74" class="definition"><strong>Définition 3.1  (Définition d’une statistique)  </strong></span>Une <strong>statistique</strong> <span class="math inline">\(t\)</span> est une fonction des observations <span class="math inline">\(x_1,\ldots,x_n\)</span> :</p>
<span class="math display">\[\begin{align}
t: \, &amp; \mathbb{R}^n \rightarrow \mathbb{R}^m  \\
      &amp; (x_1,\ldots,x_n) \rightarrow t(x_1,\ldots,x_n)
\end{align}\]</span>
</div>

<p>Par exemple, <span class="math inline">\(\overline{x}_n = \frac{1}{n} \sum_{i=1}^n x_i, x_1^2, (x_1,x_3+x_4,2 \ln x_6)\)</span> sont des statistiques.</p>
<p>Puisque les observations <span class="math inline">\(x_1,\ldots,x_n\)</span> sont des réalisations des variables aléatoires <span class="math inline">\(X_1,\ldots,X_n\)</span>, la quantité calculable à partir des observations <span class="math inline">\(t(x_1,\ldots,x_n)\)</span> est une réalisation de la variable aléatoire <span class="math inline">\(t(X_1,\ldots,X_n)\)</span>. Et on retrouve par exemple le fait que <span class="math inline">\(\overline{x}_n = \frac{1}{n} \sum_{i=1}^n x_i\)</span> est une réalisation de <span class="math inline">\(\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\)</span>.</p>
<p>Pour simplifier les écritures, on note souvent <span class="math inline">\(t_n = t(x_1,\ldots,x_n)\)</span> et <span class="math inline">\(T_n = t(X_1,\ldots,X_n)\)</span>.</p>
<p>Par abus, on donne le même nom de statistique aux deux quantités, mais dans une perspective d’estimation, on va nommer différemment <span class="math inline">\(t_n\)</span> et <span class="math inline">\(T_n\)</span>.</p>

<div class="definition">
<span id="def:unnamed-chunk-75" class="definition"><strong>Définition 3.2  (Définition d’un estimateur)  </strong></span>Un estimateur d’une grandeur <span class="math inline">\(\theta\)</span> est une statistique <span class="math inline">\(T_n\)</span> à valeurs dans
l’ensemble des valeurs possibles de <span class="math inline">\(\theta\)</span>. Une estimation de <span class="math inline">\(\theta\)</span> est une réalisation <span class="math inline">\(t_n\)</span> de l’estimateur <span class="math inline">\(T_n\)</span>.
</div>

<p>Un estimateur est donc une variable aléatoire, alors qu’une estimation est une valeur
déterministe. Dans l’exemple des ampoules, l’estimateur de <span class="math inline">\(\lambda\)</span> est <span class="math inline">\(1/\overline{X}_n\)</span> et l’estimation de <span class="math inline">\(\lambda\)</span> est <span class="math inline">\(0.012\)</span>.</p>
</div>
<div id="la-methode-des-moments" class="section level2">
<h2><span class="header-section-number">3.3</span> La méthode des moments</h2>
<div id="lestimateur-des-moments-emm" class="section level3">
<h3><span class="header-section-number">3.3.1</span> L’estimateur des moments (EMM)</h3>
<p>C’est la méthode la plus naturelle.
L’idée de base est d’estimer une espérance mathématique par une moyenne empirique,
une variance par une variance empirique, etc…</p>
<p>Si le paramètre à estimer est l’espérance de la loi des <span class="math inline">\(X_i\)</span>, alors on peut l’estimer par la moyenne empirique de l’échantillon. Autrement dit, si <span class="math inline">\(\theta = E(X)\)</span>, alors l’estimateur de <span class="math inline">\(\theta\)</span> par la méthode des moments (EMM) est <span class="math inline">\(\hat{\theta}_n=\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\)</span>.</p>
<p>Plus généralement, pour <span class="math inline">\(\theta \in \mathbb{R}\)</span>, si <span class="math inline">\(E(X) = \phi(\theta)\)</span>, où <span class="math inline">\(\phi\)</span> est une fonction inversible, alors l’estimateur de <span class="math inline">\(\theta\)</span> par la méthode des moments est <span class="math inline">\(\hat{\theta}_n = \phi^{-1} (\overline{X}_n)\)</span>.</p>
<p>De la même manière, on estime la variance de la loi des <span class="math inline">\(X_i\)</span> par la variance empirique
de l’échantillon <span class="math inline">\(S_n^2= \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}_n^2\)</span>.</p>
</div>
<div id="exemples" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Exemples</h3>
<div id="exemple-1-loi-de-bernoulli" class="section level4">
<h4><span class="header-section-number">3.3.2.1</span> Exemple 1: loi de Bernoulli</h4>
<p>Si <span class="math inline">\(X_1,\ldots,X_n\)</span> sont indépendantes et de même loi de Bernoulli <span class="math inline">\(\mathcal{B}(p)\)</span>, <span class="math inline">\(E(X) = p\)</span>. Donc
l’estimateur de <span class="math inline">\(p\)</span> par la méthode des moments est <span class="math inline">\(\hat{p}_n = \overline{X}_n\)</span>. Cet estimateur n’est autre que la proportion de 1 dans l’échantillon. On retrouve donc le principe d’estimation d’une
probabilité par une proportion.</p>
</div>
<div id="exemple-2-loi-exponentielle" class="section level4">
<h4><span class="header-section-number">3.3.2.2</span> Exemple 2: loi exponentielle</h4>
<p>Si <span class="math inline">\(X_1,\ldots,X_n\)</span> sont indépendantes et de même loi exponentielle <span class="math inline">\(\mathcal{E}(\lambda)\)</span>, <span class="math inline">\(E(X) = 1/\lambda\)</span>. Donc l’estimateur de <span class="math inline">\(\lambda\)</span> par la méthode des moments est <span class="math inline">\(\hat{\lambda}_n = 1/\overline{X}_n\)</span>.</p>
</div>
<div id="exemple-3-loi-normale" class="section level4">
<h4><span class="header-section-number">3.3.2.3</span> Exemple 3: loi normale</h4>
<p>Si <span class="math inline">\(X_1,\ldots,X_n\)</span> sont indépendantes et de même loi normale <span class="math inline">\(\mathcal{N}(m,\sigma^2 )\)</span>, <span class="math inline">\(E(X) = m\)</span> et
<span class="math inline">\(V(X) = \sigma^2\)</span>, donc les estimateurs de <span class="math inline">\(m\)</span> et <span class="math inline">\(\sigma^2\)</span> par la méthode des moments sont <span class="math inline">\(\hat{m} = \overline{X}_n\)</span> et <span class="math inline">\(\hat{\sigma}^2=S_n^2\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
</div>
<div id="exemple-4-loi-gamma" class="section level4">
<h4><span class="header-section-number">3.3.2.4</span> Exemple 4: loi gamma</h4>
<p>Si <span class="math inline">\(X_1,\ldots,X_n\)</span> sont indépendantes et de même loi gamma <span class="math inline">\(\Gamma(a,\lambda)\)</span>, <span class="math inline">\(E(X) = a/\lambda\)</span> et
<span class="math inline">\(V(X) = a/\lambda^2\)</span>. On en déduit facilement que :</p>
<p><span class="math display">\[ \lambda = \frac{E(X)}{V(X)} \quad \text{et} \quad a = \frac{[E(X)]^2}{V(X)}\]</span></p>
<p>Donc les EMM de <span class="math inline">\(a\)</span> et <span class="math inline">\(\lambda\)</span> sont:</p>
<p><span class="math display">\[ \hat{\lambda} = \frac{\overline{X}_n}{S_n^2} \quad \text{et} \quad a = \frac{\overline{X}_n^2}{S_n^2}\]</span></p>
</div>
</div>
</div>
<div id="la-methode-du-maximum-de-vraisemblance" class="section level2">
<h2><span class="header-section-number">3.4</span> La méthode du maximum de vraisemblance</h2>
<div id="la-fonction-de-vraisemblance" class="section level3">
<h3><span class="header-section-number">3.4.1</span> La fonction de vraisemblance</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-76" class="definition"><strong>Définition 3.3  </strong></span>Quand les observations sont toutes discrètes ou toutes continues, on appelle fonction de vraisemblance (ou plus simplement vraisemblance) pour l’échantillon <span class="math inline">\(x_1,\ldots,x_n\)</span>, la fonction du paramètre <span class="math inline">\(\theta\)</span> :</p>
<span class="math display">\[\begin{equation*}
\mathcal{L}(\theta; x_1,\ldots,x_n) = \left\lbrace
      \begin{array}{ll}
      P(X_1=x_1,\ldots,X_n=x_n; \theta)  &amp; \text{si les} \, X_i \, \text{sont discrètes}\\
      f_{X_1,\ldots,X_n}(x_1,\ldots,x_n;\theta) &amp; \text{si les} \, X_i \, \text{sont continues}
      \end{array}
\right.
\end{equation*}\]</span>
</div>

<p>Dans tous les exemples que nous traiterons ici, les <span class="math inline">\(X_i\)</span> sont indépendantes et de même
loi. Dans ce cas, la fonction de vraisemblance s’écrit:</p>
<p><span class="math display">\[\begin{equation*}
\mathcal{L}(\theta; x_1,\ldots,x_n) = \left\lbrace
      \begin{array}{ll}
      \displaystyle \prod_{i=1}^n P(X_i=x_i; \theta) = \prod_{i=1}^n P(X=x_i; \theta) &amp; \text{si les} \, X_i \, \text{sont discrètes}\\
      \displaystyle \prod_{i=1}^n f_{X_i}(x_i;\theta) = \prod_{i=1}^n f(x_i;\theta) &amp; \text{si les} \, X_i \, \text{sont continues}
      \end{array}
\right.
\end{equation*}\]</span></p>

<div class="rmdtip">
<strong><em>Remarque</em></strong>: La probabilité et la densité utilisées dans cette définition sont des fonctions
des observations <span class="math inline">\(x_1,\ldots,x_n\)</span>, dépendant du paramètre <span class="math inline">\(\theta\)</span>. A l’inverse, la fonction de vraisemblance est considérée comme une fonction de <span class="math inline">\(\theta\)</span> dépendant des observations <span class="math inline">\(x_1,\ldots,x_n\)</span>,
ce qui permet, par exemple, de dériver cette fonction par rapport à <span class="math inline">\(\theta\)</span>.
</div>

<div id="exemple-introductif" class="section level4">
<h4><span class="header-section-number">3.4.1.1</span> Exemple introductif</h4>
<p>Dans cet exemple, <span class="math inline">\(n = 1\)</span>. On considère que l’on sait que <span class="math inline">\(X_1\)</span> est de loi binomiale
<span class="math inline">\(\mathcal{B}(15,p)\)</span>, avec <span class="math inline">\(p\)</span> inconnu. On observe <span class="math inline">\(x_1 = 5\)</span> et on cherche à estimer <span class="math inline">\(p\)</span>. La fonction de
vraisemblance est :</p>
<p><span class="math display">\[\mathcal{L}(p;5) = P(X_1 = 5;p) = C_{15}^5 p^5 (1-p)^{15-5}\]</span></p>
<p>C’est la probabilité d’avoir observé un 5 quand la valeur du paramètre est <span class="math inline">\(p\)</span>. Calculons-là pour quelques valeurs de <span class="math inline">\(p\)</span>.</p>
<!-- |$p$                |0.1  |0.2 |0.3  |0.4  |0.5  |0.6  |0.7   |0.8      |0.9       | -->
<!-- |:------------------|:----|:---|:----|:----|:----|:----|:-----|:--------|:---------| -->
<!-- |$\mathcal{L}(p;5)$ |0.01 |0.1 |0.21 |0.19 |0.09 |0.02 |0.003 |$10^{4}$ |$210^{7}$ | -->
<table>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.2\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.3\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.4\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.5\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.6\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.7\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.8\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.9\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\mathcal{L}(p;5)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.01\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.10\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.21\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.19\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.09\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.02\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(0.003\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(10^{-4}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(210^{-7}\)</span>
</td>
</tr>
</tbody>
</table>
<p>On tire de cette table que quand <span class="math inline">\(p = 0.8\)</span>, c’est-à-dire quand <span class="math inline">\(X_1\)</span> est de loi <span class="math inline">\(\mathcal{B}(15,0.8)\)</span>,
il n’y a qu’une chance sur <span class="math inline">\(10000\)</span> d’observer <span class="math inline">\(x_1 = 5\)</span>. En revanche, il y a <span class="math inline">\(21\%\)</span> de chances
d’observer un <span class="math inline">\(5\)</span> quand <span class="math inline">\(p = 0.3\)</span>. Il est donc beaucoup plus vraisemblable que <span class="math inline">\(p\)</span> soit égal
à <span class="math inline">\(0.3\)</span> plutôt qu’à <span class="math inline">\(0.8\)</span>. En suivant ce raisonnement, on aboutit à dire que la valeur la plus
vraisemblable de <span class="math inline">\(p\)</span> est celle pour laquelle la probabilité d’observer un <span class="math inline">\(5\)</span> est maximale.
C’est donc la valeur de <span class="math inline">\(p\)</span> qui maximise la fonction de vraisemblance.</p>
<p>Pour la calculer, on peut annuler la dérivée de la vraisemblance. Mais on remarque que
la vraisemblance est un <strong>produit</strong>. Comme il est plus commode de maximiser (ou de dériver)
une somme qu’un produit, on utilise le fait que <strong>la valeur qui rend maximale une fonction rend aussi maximal son logarithme</strong>. On va donc plutôt maximiser le logarithme de la
fonction de vraisemblance, qu’on appelle la <strong><em>log-vraisemblance</em></strong>. Pour notre exemple, la
log-vraisemblance vaut:</p>
<p><span class="math display">\[\ln \mathcal{L}(p;x_1)=\ln C_{15}^{x_1} + x_1 \ln p + (15-x_1) \ln (1-p)\]</span>
Sa dérivée est:</p>
<p><span class="math display">\[ \frac{\partial }{\partial p } \ln \mathcal{L}(p;x_1)= \frac{x_1}{p} - \frac{15-x_1}{1-p} = \frac{x_1 - 15 p}{p(1-p)} \]</span>
qui s’annule pour <span class="math inline">\(p = \frac{x_1}{15} = \frac{5}{15} = \frac{1}{3}\)</span>.</p>
<p>Donc la valeur la plus vraisemblable de <span class="math inline">\(p\)</span> est <span class="math inline">\(\frac{1}{3}\)</span>. La vraisemblance maximale est <span class="math inline">\(\mathcal{L}(\frac{1}{3};5) = 21.4\%\)</span>.</p>
</div>
</div>
<div id="lestimateur-de-maximum-de-vraisemblance-emv" class="section level3">
<h3><span class="header-section-number">3.4.2</span> L’estimateur de maximum de vraisemblance (EMV)</h3>
<p>En suivant le raisonnement précédent, pour <span class="math inline">\(n\)</span> quelconque, il est logique de dire que
la valeur la plus vraisemblable de <span class="math inline">\(\theta\)</span> est la valeur pour laquelle la probabilité d’observer
<span class="math inline">\(x_1 ,\ldots,x_n\)</span> est la plus forte possible. Cela revient à faire comme si c’était l’éventualité la
plus probable qui s’était produite au cours de l’expérience.</p>

<div class="definition">
<span id="def:unnamed-chunk-79" class="definition"><strong>Définition 3.4  </strong></span>L’estimation de maximum de vraisemblance de <span class="math inline">\(\theta\)</span> est la valeur <span class="math inline">\(\hat{\theta}_n\)</span> de <span class="math inline">\(\theta\)</span> qui rend maximale la fonction de vraisemblance <span class="math inline">\(\mathcal{L}(\theta;x_1 ,\ldots,x_n)\)</span>. L’estimateur de
maximum de vraisemblance (EMV) de <span class="math inline">\(\theta\)</span> est la variable aléatoire correspondante.
</div>

<p>Comme dans l’exemple, dans la plupart des cas, la fonction de vraisemblance s’exprime
comme un produit. Donc $ _n$ sera en général calculé en maximisant la log-vraisemblance:</p>
<p><span class="math display">\[ \hat{\theta}_{n}=\arg \max _{\theta} \ln \mathcal{L}\left(\theta ; x_{1}, \ldots, x_{n}\right) \]</span></p>
<p>Quand <span class="math inline">\(\theta = (\theta_1 ,\ldots,\theta_d ) \in \mathbb{R}^d\)</span> et que toutes les dérivées partielles ci-dessous existent, <span class="math inline">\(\hat{\theta}_{n}\)</span> est solution du système d’équations appelées équations de vraisemblance:</p>
<p><span class="math display">\[ \forall j \in\{1, \ldots, d\}, \quad \frac{\partial}{\partial \theta_{j}} \ln \mathcal{L}\left(\theta ; x_{1}, \ldots, x_{n}\right)=0 \]</span></p>

<div class="rmdinsight">
A priori, une solution de ce système d’équations pourrait être un minimum de la vraisemblance. Mais on peut montrer que la nature d’une fonction de vraisemblance fait que
c’est bien un maximum que l’on obtient.
</div>

<p>Il est fréquent que le système des équations de vraisemblance n’ait pas de solution
explicite. Dans ce cas, on le résoud par des méthodes numériques, comme la méthode de
Newton-Raphson (<a href="https://www.lyceedadultes.fr/sitepedagogique/documents/math/mathTermS/04_continuite_derivabilite_fonction/04_cours_algorithme_newton.pdf" target="_blank"><i class="fas  fa-external-link-alt "></i>1</a>, <a href="http://dridk.me/newton-raphson.html" target="_blank"><i class="fas  fa-external-link-alt "></i>2</a>). En <code>R</code>, la maximisation numérique peut se faire à l’aide de la commande
<code>optim</code>.</p>
<div id="exemples-1" class="section level4">
<h4><span class="header-section-number">3.4.2.1</span> Exemples</h4>
<div id="exemple-1-loi-de-bernoulli-1" class="section level5">
<h5><span class="header-section-number">3.4.2.1.1</span> Exemple 1: loi de Bernoulli</h5>
<p>Soit les <span class="math inline">\(X_i\)</span>, sont de loi <span class="math inline">\(\mathcal{B}(p)\)</span>, on a:</p>
<p><span class="math display">\[P\left(X_{i}=x_{i} ; p\right)=\left\{\begin{array}{cc}{p} &amp; {\text { si } x_{i}=1} \\ {1-p} &amp; {\text { si } x_{i}=0}\end{array}=p^{x_{i}}(1-p)^{1-x_{i}}\right.\]</span>
Donc la fonction de vraisemblance est:</p>
<p><span class="math display">\[\mathcal{L}\left(p ; x_{1}, \ldots, x_{n}\right)=\prod_{i=1}^{n} P\left(X_{i}=x_{i} ; p\right)=\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}=p^{\sum_{i=1}^{n} x_{i}}(1-p)^{\sum_{i=1}^{n}\left(1-x_{i}\right)}\]</span></p>
<p>D’où <span class="math inline">\(\ln \mathcal{L}\left(p ; x_{1}, \ldots, x_{n}\right)=\left(\sum_{i=1}^{n} x_{i}\right) \ln p+\left(n-\sum_{i=1}^{n} x_{i}\right) \ln (1-p)\)</span>.</p>
<p>Alors <span class="math inline">\(\frac{\partial}{\partial p} \ln \mathcal{L}\left(p ; x_{1}, \ldots, x_{n}\right)=\frac{\sum_{i=1}^{n} x_{i}}{p}-\frac{n-\sum_{i=1}^{n} x_{i}}{1-p}=\frac{\sum_{i=1}^{n} x_{i}-n p}{p(1-p)}\)</span>, qui s’annule pour <span class="math inline">\(p=\frac{1}{n} \sum_{i=1}^{n} x_{i}=\overline{x}_{n}\)</span>. Par conséquent, l’EMV de <span class="math inline">\(p\)</span> est <span class="math inline">\(\hat{p}_n= \overline{X}_n\)</span>.</p>
</div>
<div id="exemple-2-loi-exponentielle-1" class="section level5">
<h5><span class="header-section-number">3.4.2.1.2</span> Exemple 2: loi exponentielle</h5>
<p>Si les <span class="math inline">\(X_i\)</span> sont de loi <span class="math inline">\(\mathcal{E}(\lambda)\)</span>, la fonction de vraisemblance est:</p>
<p><span class="math display">\[\mathcal{L}\left(\lambda ; x_{1}, \ldots, x_{n}\right)=\prod_{i=1}^{n} f_{X_{i}}\left(x_{i} ; \lambda\right)=\prod_{i=1}^{n} \lambda e^{-\lambda x_{i}}=\lambda^{n} e^{-\lambda \sum_{i=1}^{n} x_{i}}\]</span>
D’où <span class="math inline">\(\ln \mathcal{L}\left(\lambda ; x_{1}, \ldots, x_{n}\right)=n \ln \lambda-\lambda \sum_{i=1}^{n} x_{i}\)</span>.
Alors <span class="math inline">\(\frac{\partial}{\partial \lambda} \ln \mathcal{L}\left(\lambda ; x_{1}, \ldots, x_{n}\right)=\frac{n}{\lambda}-\sum_{i=1}^{n} x_{i}\)</span>, qui s’annule pour <span class="math inline">\(\lambda=\frac{n}{\sum_{i=1}^{n} x_{i}}=\frac{1}{\overline{x}_{n}}\)</span>.</p>
<p>Par conséquent, l’EMV de <span class="math inline">\(\lambda\)</span> est <span class="math inline">\(\hat{\lambda}_n = \frac{1}{\overline{X}_n}\)</span>.</p>
</div>
<div id="exemple-3-loi-normale-1" class="section level5">
<h5><span class="header-section-number">3.4.2.1.3</span> Exemple 3: loi normale</h5>
<p>Si les <span class="math inline">\(X_i\)</span> sont de loi <span class="math inline">\(\mathcal{N}(m,\sigma^2)\)</span>, la fonction de vraisemblance est:</p>
<p><span class="math display">\[\begin{aligned} \mathcal{L}\left(m, \sigma^{2} ; x_{1}, \ldots, x_{n}\right) &amp;=\prod_{i=1}^{n} f_{X_{i}}\left(x_{i} ; m, \sigma^{2}\right)=\prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{\left(x_{i}-m\right)^{2}}{2 \sigma^{2}}} \\ &amp;=\frac{1}{(\sigma \sqrt{2 \pi})^{n}} e^{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-m\right)^{2}} \end{aligned}\]</span></p>
<p>D’où <span class="math inline">\(\ln \mathcal{L}\left(m, \sigma^{2} ; x_{1}, \ldots, x_{n}\right)=-\frac{n}{2} \ln \sigma^{2}-\frac{n}{2} \ln 2 \pi-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-m\right)^{2}\)</span>.</p>
<p>On doit annuler les dérivées partielles de ce logarithme par rapport à <span class="math inline">\(m\)</span> et <span class="math inline">\(\sigma^2\)</span>. On a:</p>
<ul>
<li><span class="math inline">\(\frac{\partial}{\partial m} \ln \mathcal{L}\left(m, \sigma^{2} ; x_{1}, \ldots, x_{n}\right)=-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}-2\left(x_{i}-m\right)=\frac{1}{\sigma^{2}}\left(\sum_{i=1}^{n} x_{i}-n m\right)\)</span>, qui s’annule pour <span class="math inline">\(m=\frac{1}{n} \sum_{i=1}^{n} x_{i}=\overline{x}_{n}\)</span>.</li>
<li><span class="math inline">\(\frac{\partial}{\partial \sigma^{2}} \ln \mathcal{L}\left(m, \sigma^{2} ; x_{1}, \ldots, x_{n}\right)=-\frac{n}{2 \sigma^{2}}+\frac{1}{2 \sigma^{4}} \sum_{i=1}^{n}\left(x_{i}-m\right)^{2}\)</span>, qui s’annule pour <span class="math inline">\(\sigma^2=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-m\right)^{2}\)</span>.</li>
</ul>
<p><span class="math inline">\(\hat{m}_n\)</span> et <span class="math inline">\(\hat{\sigma}_n^2\)</span> sont les valeurs de <span class="math inline">\(m\)</span> et <span class="math inline">\(\sigma^2\)</span> qui vérifient les deux conditions en même temps. On a donc <span class="math inline">\(\hat{m}_{n}=\overline{X}_{n}\)</span> et <span class="math inline">\(\hat{\sigma}_{n}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}_{n}\right)^{2}=S_{n}^{2}\)</span>.</p>

<div class="rmdtip">
Remarque: Dans ces les trois exemples, la méthode des moments et la méthode
du maximum de vraisemblance donnent les mêmes résultats. Ce n’est le cas que pour
quelques lois de probabilité parmi les plus élémentaires. En fait, dans la plupart des cas,
les deux méthodes fournissent des estimateurs différents. C’est le cas de la loi gamma. Cela amène à se poser la question de la qualité et de l’optimalité d’un estimateur, ce qui fait l’objet de la section suivante.
</div>

</div>
</div>
</div>
</div>
<div id="qualite-dun-estimateur" class="section level2">
<h2><span class="header-section-number">3.5</span> Qualité d’un estimateur</h2>
<p>En toute généralité, <span class="math inline">\(\theta\)</span> peut-être un paramètre à plusieurs dimensions, mais on supposera dans toute cette section et dans la suivante que <span class="math inline">\(\theta\)</span> est un réel. Cela signifie par exemple que, quand <span class="math inline">\(X\)</span> est de loi normale <span class="math inline">\(\mathcal{N}(m,\sigma^2)\)</span>, on s’intéressera séparément à la qualité des estimateurs de <span class="math inline">\(m\)</span> et de <span class="math inline">\(\sigma^2\)</span>. Les estimateurs <span class="math inline">\(T_n\)</span> considérés ici seront donc des variables aléatoires réelles. Pour <span class="math inline">\(\theta \in \mathbb{R}^d, d \geq 2\)</span>, toutes les notions de ces sections sont généralisables, mais la complexité des résultats augmente notablement. Par exemple, la notion de variance est remplacée par celle de matrice de covariance.</p>
<div id="estimateur-sans-biais-et-de-variance-minimale-esbvm" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Estimateur sans biais et de variance minimale (ESBVM)</h3>
<p>Un estimateur <span class="math inline">\(T_n\)</span> de <span class="math inline">\(\theta\)</span> sera un bon estimateur s’il est suffisamment proche, en un certain sens, de <span class="math inline">\(\theta\)</span>. Il faut donc définir une mesure de l’écart entre <span class="math inline">\(\theta\)</span> et <span class="math inline">\(T_n\)</span>. On appelle cette mesure le <strong>risque</strong> de l’estimateur. On a intérêt à ce que le risque d’un estimateur soit le plus petit possible.</p>
<p>Par exemple, les risques <span class="math inline">\(T_n - \theta, |T_n - \theta|, (T_n - \theta)^2\)</span> expriment bien un écart entre <span class="math inline">\(T_n\)</span> et <span class="math inline">\(\theta\)</span>. Mais comme il est plus facile d’utiliser des quantités déterministes que des quantités aléatoires, on s’intéresse en priorité aux espérances des quantités précédentes. En particulier:</p>

<div class="definition">
<p><span id="def:unnamed-chunk-82" class="definition"><strong>Définition 3.5  </strong></span>Le <strong>biais</strong> de <span class="math inline">\(T_n\)</span> est <span class="math inline">\(E(T_n)-\theta\)</span>.</p>
Le <strong>risque quadratique</strong> ou <strong>erreur quadratique moyenne</strong> est:
<span class="math display">\[ EQM(T_n)=E[(T_n-\theta)^2]\]</span>
</div>

<p>Dans le cas du biais, le rique peut être nul:</p>

<div class="definition">
<span id="def:unnamed-chunk-83" class="definition"><strong>Définition 3.6  </strong></span>Un <strong>estimateur</strong> <span class="math inline">\(T_n\)</span> de <span class="math inline">\(\theta\)</span> est <strong><em>sans biais</em></strong> si et seulement si <span class="math inline">\(E(T_n) = \theta\)</span>. Il est <strong><em>biaisé</em></strong> si et seulement si <span class="math inline">\(E(T_n) \neq \theta\)</span>.
</div>

<p>Le biais mesure une erreur systématique d’estimation de <span class="math inline">\(\theta\)</span> par <span class="math inline">\(T_n\)</span>. Par exemple, si <span class="math inline">\(E(T_n)-\theta &lt; 0\)</span>, cela signifie que <span class="math inline">\(T_n\)</span> aura tendance à <em>sous-estimer</em> <span class="math inline">\(\theta\)</span>.</p>
<p>L’erreur quadratique moyenne s’écrit:</p>
<p><span class="math display">\[\begin{aligned}
E Q M\left(T_{n}\right) &amp; = E\left[\left(T_{n}-\theta\right)^{2}\right]=E\left[\left(T_{n}-E\left(T_{n}\right)+E\left(T_{n}\right)-\theta\right)^{2}\right] \\
                        &amp; = E\left[\left(T_{n}-E\left(T_{n}\right)\right)^{2}\right]+2 E\left[T_{n}-E\left(T_{n}\right)\right] E\left[E\left(T_{n}\right)-\theta\right]+E\left[\left(E\left(T_{n}\right)-\theta\right)^{2}\right] \\
                        &amp; = \operatorname{Var}\left(T_{n}\right)+\left[E\left(T_{n}\right)-\theta\right]^{2} \\
                        &amp; = \text { Variance de l&#39;estimateur }+\text { carré de son biais }
\end{aligned}\]</span></p>
<p>Si <span class="math inline">\(T_n\)</span> est un estimateur sans biais, <span class="math inline">\(EQM(T_n ) = Var(T_n )\)</span>. On a donc intérêt à ce qu’un
estimateur soit sans biais et de faible variance. Par ailleurs, on en déduit immédiatement
que de deux estimateurs sans biais, le meilleur est celui qui a la plus petite variance.</p>
<p>La variance d’un estimateur mesure sa variabilité. Si l’estimateur est sans biais, cette
variabilité est autour de <span class="math inline">\(\theta\)</span>. Si on veut estimer correctement <span class="math inline">\(\theta\)</span>, il ne faut pas que cette
variabilité soit trop forte.</p>
<p>En pratique, si on observe plusieurs jeux de données similaires, on obtient une estimation de <span class="math inline">\(\theta\)</span> pour chacun d’entre eux. Alors si l’estimateur est de faible variance, ces
estimations seront toutes proches les unes des autres, et s’il est sans biais leur moyenne
sera très proche de <span class="math inline">\(\theta\)</span>.</p>
<p>Il est logique de s’attendre à ce que, plus la taille des données augmente, plus on
a d’information sur le phénomène aléatoire observé, donc meilleure sera l’estimation. En
théorie, avec une observation infinie, on devrait pouvoir estimer <span class="math inline">\(\theta\)</span> sans aucune erreur. On
peut traduire cette affirmation par le fait que le risque de l’estimateur <span class="math inline">\(T_n\)</span> doit tendre vers
<span class="math inline">\(0\)</span> quand la taille <span class="math inline">\(n\)</span> de l’échantillon tend vers l’infini. Cela revient à dire que l’estimateur
<span class="math inline">\(T_n\)</span> doit converger, en un certain sens, vers <span class="math inline">\(\theta\)</span>.</p>
<p>Il s’agit en fait d’étudier la convergence de la suite de variables aléatoires <span class="math inline">\(\{T_n\}_{n \geq 1}\)</span> vers
la constante <span class="math inline">\(\theta\)</span>. On sait qu’il existe plusieurs types de convergence de suites de variables
aléatoires. On peut étudier la convergence presque sûre ou la convergence en probabilité,
mais on s’intéresse en général à la convergence en moyenne quadratique (ou convergence
dans <span class="math inline">\(L^2\)</span>).</p>

<div class="definition">
<p><span id="def:unnamed-chunk-84" class="definition"><strong>Définition 3.7  </strong></span>L’estimateur <span class="math inline">\(T_n\)</span> <strong>converge en moyenne quadratique</strong> vers <span class="math inline">\(\theta\)</span> si et seulement si son erreur quadratique moyenne tend vers 0 quand <span class="math inline">\(n\)</span> tend vers l’infini:</p>
<span class="math display">\[T_{n} \stackrel{M Q}{\longrightarrow} \theta \Leftrightarrow \lim _{n \rightarrow \infty} E\left[\left(T_{n}-\theta\right)^{2}\right]=0\]</span>
</div>

<p>Si <span class="math inline">\(T_n\)</span> est sans biais, il sera convergent en moyenne quadratique si et seulement si sa variance tend vers 0 quand <span class="math inline">\(n\)</span> tend vers l’infini.</p>
<p>Finalement, on considèrera que le meilleur estimateur possible de <span class="math inline">\(\theta\)</span> est un <strong><em>estimateur sans biais et de variance minimale (ESBVM)</em></strong>. Un tel estimateur n’existe pas forcément.</p>
</div>
</div>
<div id="proprietes-des-estimateurs-des-moments-emm" class="section level2">
<h2><span class="header-section-number">3.6</span> Propriétés des estimateurs des moments (EMM)</h2>
<p>Si <span class="math inline">\(\theta = E(X)\)</span>, alors l’EMM de <span class="math inline">\(\theta\)</span> est <span class="math inline">\(\hat{\theta}_n = \overline{X}_n\)</span>. La justification de cette méthode est la loi des grands nombres, qui dit que <span class="math inline">\(\overline{X}_n\)</span> converge presque sûrement vers <span class="math inline">\(E(X)\)</span>. Donc, si <span class="math inline">\(\theta = E(X)\)</span>, <span class="math inline">\(\overline{X}_n\)</span> est une estimateur de <span class="math inline">\(\theta\)</span> convergent presque sûrement. Autrement dit, si on a beaucoup d’observations, on peut estimer une espérance par une moyenne empirique.</p>
<p>On peut en fait montrer facilement que <span class="math inline">\(\overline{X}_n\)</span> est un bon estimateur de <span class="math inline">\(\theta = E(X)\)</span>, sans utiliser la loi des grands nombres:</p>
<p><span class="math display">\[
E\left(\overline{X}_{n}\right)=E\big[\frac{1}{n} \sum_{i=1}^{n} X_{i}\big]=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}\right)=\frac{1}{n} n \theta=\theta
\]</span></p>
<p>Donc <span class="math inline">\(\overline{X}_{n}\)</span> est un estimateur sans biais de <span class="math inline">\(\theta = E(X)\)</span>.</p>
<p>La variance de <span class="math inline">\(\overline{X}_{n}\)</span> est:</p>
<p><span class="math display">\[
\operatorname{Var}\left(\overline{X}_{n}\right)=\operatorname{Var}\left[\frac{1}{n} \sum_{i=1}^{n} X_{i}\right]=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}\left(X_{i}\right)=\frac{\operatorname{Var}(X)}{n}
\]</span>
car les <span class="math inline">\(X_i\)</span> sont indépendantes, donc la variance de leur somme est égale à la somme de leurs variances, qui sont toutes égales à <span class="math inline">\(Var(X)\)</span>. <span class="math inline">\(Var(\overline{X}_{n})\)</span> tend vers 0 quand <span class="math inline">\(n\)</span> tend vers l’infini. Par conséquent:</p>
<p><strong>Propriété:</strong> La moyenne empirique <span class="math inline">\(\overline{X}_{n}\)</span> est un estimateur sans biais et convergent en moyenne quadratique de <span class="math inline">\(E(X)\)</span>.</p>
<p>On considère maintenant l’estimation de la variance de la loi des <span class="math inline">\(X_i\)</span> par la variance empirique de l’échantillon <span class="math inline">\(S_{n}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}_{n}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}-\overline{X}_{n}^{2}\)</span>.</p>
<p>Déterminons le biais de cet estimateur.</p>
<p><span class="math display">\[
\begin{aligned} E\left(S_{n}^{2}\right) &amp;=E\left[\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}-\overline{X}_{n}^{2}\right]=\frac{1}{n} \sum_{i=1}^{n} E\left(X_{i}^{2}\right)-E\left(\overline{X}_{n}^{2}\right)=E\left(X^{2}\right)-E\left(\overline{X}_{n}^{2}\right) \\ &amp;=\operatorname{Var}(X)+E(X)^{2}-\operatorname{Var}\left(\overline{X}_{n}\right)-E\left(\overline{X}_{n}\right)^{2} \\ &amp;=\operatorname{Var}(X)+E(X)^{2}-\frac{\operatorname{Var}(X)}{n}-E(X)^{2}=\left(1-\frac{1}{n}\right) \operatorname{Var}(X) \\ &amp;=\frac{n-1}{n} \operatorname{Var}(X) \neq \operatorname{Var}(X) \end{aligned}
\]</span></p>
<p>Donc contrairement à ce qu’on pourrait croire, la variance empirique <span class="math inline">\(S_n^2\)</span> n’est pas un estimateur sans biais de <span class="math inline">\(Var(X)\)</span>. Cet estimateur n’est qu’asymptotiquement sans biais.</p>
<p>En revanche, on voit que <span class="math inline">\(E\left(\frac{n}{n-1} S_{n}^{2}\right)=\frac{n}{n-1} E\left(S_{n}^{2}\right)=\operatorname{Var}(X)\)</span>. On pose donc <span class="math inline">\(S_{n}^{\prime 2}=\frac{n}{n-1} S_{n}^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\big(X_{i}-\overline{X}_{n}\big)^{2}\)</span>. <span class="math inline">\(S_{n}^{\prime 2}\)</span> est appelée <strong>variance estimée</strong> de l’échantillon. Le résultat précédent montre que c’est un estimateur sans biais de <span class="math inline">\(Var(X)\)</span>.</p>
<p>Par ailleurs, on montre que</p>
<p><span class="math display">\[
\operatorname{Var}\left(S_{n}^{\prime 2}\right)=\frac{1}{n(n-1)}\left[(n-1) E\left[(X-E(X))^{4}\right]-(n-3) \operatorname{Var}(X)^{2}\right]
\]</span>
qui tend vers 0 quand <span class="math inline">\(n\)</span> tend vers l’infini. Par conséquent:</p>
<p><strong>Propriété:</strong> La <strong>variance estimée</strong> <span class="math inline">\(S_{n}^{\prime 2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\overline{X}_{n}\right)^{2}\)</span> est un estimateur sans biais et convergent en moyenne quadratique de <span class="math inline">\(Var(X)\)</span>.</p>

<div class="rmdcaution">
La commande <code>var(x)</code> en <code>R</code> donne la variance estimée, et non pas la variance empirique de l’échantillon <code>x</code>.
</div>

<p>On peut montrer également que <span class="math inline">\(S_{n}^{\prime 2}\)</span> et <span class="math inline">\(S_{n}^{2}\)</span> convergent toutes les deux presque sûrement vers <span class="math inline">\(Var(X)\)</span>.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p><span class="math inline">\(S_n^2\)</span> est la variance empirique de l’échantillon <span class="math inline">\(S_n^2= \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}_n^2\)</span><a href="estimation-ponctuelle.html#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tp-illustration-numerique-des-theoremes-limites-avec-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervalle-de-confiance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"plugins": "copy-code-button"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
