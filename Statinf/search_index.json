[["index.html", "Statistique Inférentielle Overview Plan et déroulement du cours", " Statistique Inférentielle Mohamad Ghassany 2020-11-18 Overview Ce document est dédié pour le cours de Statistique Inférentielle destiné aux étudiants en année 3 de lÉcole supérieure dingénieurs Léonard-de-Vinci. Plan et déroulement du cours Séance Type Durée Contenu Cours magistral 1 En ligne 1.5h Séance 1 TD 3h Statistique descriptive, TP Prise en main sur R Séance 2 TD 3h Echantillonage, Théorèmes limites, lois des \\(\\overline{X}_n, S_n^2\\) et \\(P_n\\), TP sur lillustration des théorèmes limites Cours magistral 2 En ligne 1.5h Séance 3 TD 3h Estimation ponctuelle et recherche destimateur, Méthode des moments, Méthode du maximum de vraissemblance Contrôle continu 1 45mins Samedi le 28 novembre 2020 Séance 4 CAO 3h Séance de TP: Suite du TP sur les théorèmes limites, TP sur lestimation ponctuelle Cours magistral 3 En ligne 1.5h Séance 5 TD 3h Estimation par Intervalle de Confiance Séance 6 TD 3h Suite exercices IC et TP Contrôle continu 2 45mins Samedi le 09 janvier 2021 Cours magistral 4 En ligne 1.5h Séance 7 TD 3h Tests dHypothèses: Introduction Tests dhypothèses à partir dun seul échantillon Séance 8 CAO 3h Tests dhypothèses à partir de deux échantillons, Exercices et TP Séance 9 TD 3h Test dajustement de \\(\\chi^2\\), Test dindépendance de \\(\\chi^2\\), Exercices et TP Examen 1.5h Semaine du 08 février 2021 "],["introduction.html", "Introduction", " Introduction La statistique est la science dont lobjet est de recueillir, de traiter et danalyser des données issues de lobservation de phénomènes aléatoires, cest-à-dire dans lesquels le hasard intervient. Lanalyse des données est utilisée pour décrire les phénomènes étudiés, faire des prévisions et prendre des décisions à leur sujet. En cela, la statistique est un outil essentiel pour la compréhension et la gestion des phénomènes complexes. Les données étudiées peuvent être de toute nature, ce qui rend la statistique utile dans tous les champs disciplinaires et explique pourquoi elle est enseignée dans toutes les filières universitaires, de léconomie à la biologie en passant par la psychologie, et bien sûr les sciences de lingénieur. Le point fondamental est que les données sont entâchées dincertitudes et présentent des variations pour plusieurs raisons : le déroulement des phénomènes observés nest pas prévisible à lavance avec certitude (par exemple on ne sait pas prévoir avec certitude les cours de la bourse ou les pannes des voitures) toute mesure est entâchée derreur etc Il y a donc intervention du hasard et des probabilités. Lobjectif essentiel de la statistique est de maîtriser au mieux cette incertitude pour extraire des informations utiles des données, par lintermédiaire de lanalyse des variations dans les observations. Les méthodes statistiques se répartissent en deux classes : La statistique descriptive, statistique exploratoire ou analyse des données, a pour but de résumer linformation contenue dans les données de façon synthétique et efficace. Elle utilise pour cela des représentations de données sous forme de graphiques, de tableaux et dindicateurs numériques (par exemple des moyennes). Elle permet de dégager les caractéristiques essentielles du phénomène étudié et de suggérer des hypothèses pour une étude ultérieure plus sophistiquée. Les probabilités nont ici quun rôle mineur. La statistique inférentielle va au delà de la simple description des données. Elle a pour but de faire des prévisions et de prendre des décisions au vu des observations. En général, il faut pour cela proposer des modèles probabilistes du phénomène aléatoire étudié et savoir gérer les risques derreurs. Les probabilités jouent ici un rôle fondamental. Pour le grand public, les statistiques désignent les résumés de données fournis par la statistique descriptive. Par exemple, on parle des statistiques du chômage ou des statistiques de léconomie américaine. Mais on oublie en général les aspects les plus importants liés aux prévisions et à laide à la décision apportés par la statistique inférentielle. Linformatique et la statistique sont deux éléments du traitement de linformation : linformatique acquiert et traite linformation tandis que la statistique lanalyse. Les deux disciplines sont donc étroitement liées. En particulier, laugmentation considérable de la puissance des ordinateurs et la facilité de transmission des données par internet ont rendu possible lanalyse de très grandes masses de données, ce qui nécessite lutilisation de méthodes de plus en plus sophistiquées, connues sous le nom de data mining, fouille de données ou Big Data. Enfin, linformatique décisionnelle ou business intelligence regroupe les outils daide à la décision devenus essentiels dans la gestion des entreprises. Ces outils nécessitent un recours important aux méthodes statistiques. Plus généralement, tout ingénieur est amené à prendre des décisions au vu de certaines informations, dans des contextes où de nombreuses incertitudes demeurent. Il importe donc quun ingénieur soit formé aux techniques de gestion du risque et de traitement de données expérimentales. Aujourdhui, les statistiques sont partout. Les statistiques descriptives sont présentées dans tous les journaux et magazines. Linférence statistique est devenue indispensable à la santé publique et la recherche médicale, à lingénierie et les études scientifiques, à la commercialisation et léducation, la comptabilité, léconomie, les prévisions météorologiques, les sondages, aux sports, à lassurance, et à toutes les recherches scientifiques. Les statistiques sont en effet enracinées dans notre patrimoine intellectuel. La démarche statistique La statistique et les probabilités sont les deux aspects complémentaires de létude des phénomènes aléatoires. Ils sont cependant de natures bien différentes. Les probabilités peuvent être envisagées comme une branche des mathématiques pures, basée sur la théorie de la mesure, abstraite et complètement déconnectée de la réalité. Les probabilités appliquées proposent des modèles probabilistes du déroulement de phénomènes aléatoires concrets. On peut alors, préalablement à toute expérience, faire des prévisions sur ce qui va se produire. Par exemple, il est usuel de modéliser la durée de bon fonctionnement ou durée de vie dun système, mettons une ampoule électrique, par une variable aléatoire \\(X\\) de loi exponentielle de paramètre \\(\\lambda\\). Ayant adopté ce modèle probabiliste, on peut effectuer tous les calculs que lon veut. Par exemple : La probabilité que lampoule ne soit pas encore tombée en panne à la date \\(t\\) est \\(P(X &gt; t) = e^{\\lambda t}\\) . La durée de vie moyenne est \\(E(X) = 1/\\lambda\\). Si \\(n\\) ampoules identiques sont mises en fonctionnement en même temps, et quelles fonctionnent indépendamment les unes des autres, le nombre \\(N_t\\) dampoules qui tomberont en panne avant un instant \\(t\\) est une variable aléatoire de loi binomiale \\(\\mathcal{B}(n,P(X  t)) = \\mathcal{B}(n,1  e^{-\\lambda t})\\). Donc on sattend à ce que, en moyenne, \\(E(N_t) = n(1  e^{-\\lambda t})\\) ampoules tombent en panne entre 0 et \\(t\\) Dans la pratique, lutilisateur de ces ampoules est très intéressé par ces résultats. Il souhaite évidemment avoir une évaluation de leur durée de vie, de la probabilité quelles fonctionnent correctement pendant plus dun mois, un an, etc Mais si lon veut utiliser les résultats théoriques énoncés plus haut, il faut dune part pouvoir sassurer quon a choisi un bon modèle, cest-à-dire que la durée de vie de ces ampoules est bien une variable aléatoire de loi exponentielle, et, dautre part, pouvoir calculer dune manière ou dune autre la valeur du paramètre \\(\\lambda\\). Cest la statistique qui va permettre de résoudre ces problèmes. Pour cela, il faut faire une expérimentation, recueillir des données et les analyser. On met donc en place ce quon appelle un essai ou une expérience. On fait fonctionner en parallèle et indépendamment les unes des autres \\(n = 10\\) ampoules identiques, dans les mêmes conditions expérimentales, et on relève leurs durées de vie. Admettons que lon obtienne les durées de vie suivantes, exprimées en heures : 91.6 35.7 251 24.3 5.4 67.3 171 9.5 118 57.1 Notons \\(x_1 ,\\ldots,x_n\\) ces observations. Il est bien évident que la durée de vie des ampoules nest pas prévisible avec certitude à lavance. On va donc considérer que \\(x_1 ,\\ldots,x_n\\) sont les réalisations de variables aléatoires \\(X_1 ,\\ldots,X_n\\). Cela signifie quavant lexpérience, la durée de vie de la \\(i^{\\text{ème}}\\) ampoule est inconnue et que lon traduit cette incertitude en modélisant cette durée par une variable aléatoire \\(X_i\\). Mais après lexpérience, la durée de vie a été observée. Il ny a donc plus dincertitude, cette durée est égale au réel \\(x_i\\). On dit que \\(x_i\\) est la réalisation de \\(X_i\\) sur lessai effectué. Puisque les ampoules sont identiques, il est naturel de supposer que les \\(X_i\\) sont de même loi. Cela signifie quon observe plusieurs fois le même phénomène aléatoire. Mais le hasard fait que les réalisations de ces variables aléatoires de même loi sont différentes, doù la variabilité dans les données. Puisque les ampoules ont fonctionné indépendamment les unes des autres, on pourra également supposer que les \\(X_i\\) sont des variables aléatoires indépendantes. On peut alors se poser les questions suivantes : Au vu de ces observations, est-il raisonnable de supposer que la durée de vie dune ampoule est une variable aléatoire de loi exponentielle? Si non, quelle autre loi serait plus appropriée? Cest un problème de choix de modèle ou de test dadéquation. Si le modèle de loi exponentielle a été retenu, comment proposer une valeur (ou un ensemble de valeurs) vraisemblable pour le paramètre \\(\\lambda\\)? Cest un problème destimation paramétrique. Dans ce cas, peut-on garantir que \\(\\lambda\\) est inférieur à une valeur fixée \\(\\lambda_0\\) ? Cela garantira alors que \\(E(X) = 1/\\lambda \\geq 1/\\lambda_0\\), autrement dit que les ampoules seront suffisamment fiables. Cest un problème de test dhypothèses paramétriques. Sur un parc de 100 ampoules, à combien de pannes peut-on sattendre en moins de 50 h? Cest un problème de prévision. Le premier problème central est celui de lestimation : comment proposer, au vu des observations, une approximation des grandeurs inconnues du problème qui soit la plus proche possible de la réalité? La première question peut se traiter en estimant la fonction de répartition ou la densité de la loi de probabilité sous-jacente, la seconde revient à estimer un paramètre de cette loi, la troisième à estimer un nombre moyen de pannes sur une période donnée. Le second problème central est celui des tests dhypothèses : il sagit de se prononcer sur la validité dune hypothèse liée au problème : la loi est-elle exponentielle? \\(\\lambda\\) est-il inférieur à \\(\\lambda_0\\)? un objectif de fiabilité est-il atteint? En répondant oui ou non à ces questions, il est possible que lon se trompe. Donc, à toute réponse statistique, il faudra associer le degré de confiance que lon peut accorder à cette réponse. Cest une caractéristique importante de la statistique par rapport aux mathématiques classiques, pour lesquelles un résultat est soit juste, soit faux. Pour résumer, la démarche probabiliste suppose que la nature du hasard est connue. Cela signifie que lon adopte un modèle probabiliste particulier (ici la loi exponentielle), qui permettra deffectuer des prévisions sur les observations futures. Dans la pratique, la nature du hasard est inconnue. La statistique va, au vu des observations, formuler des hypothèses sur la nature du phénomène aléatoire étudié. Maîtriser au mieux cette incertitude permettra de traiter les données disponibles. Probabilités et statistiques agissent donc en aller-retour dans le traitement mathématique des phénomènes aléatoires. Lexemple des ampoules est une illustration du cas le plus fréquent où les données se présentent sous la forme dune suite de nombres. Cest ce cas que nous traiterons dans ce cours, mais il faut savoir que les données peuvent être beaucoup plus complexes : des fonctions, des images, etc Les principes et méthodes généraux que nous traiterons dans ce cours seront adaptables à tous les types de données. Objectifs et plan du cours Ce cours a pour but de présenter les principes de base dune analyse statistique de données (description, estimation, tests), ainsi que les méthodes statistiques les plus usuelles. Ces méthodes seront toujours illustrées par des problèmes concrets. Le cours privilégie lapplication à la théorie. Les méthodes présentées seront mises en uvre à laide du logiciel (https://www.r-project.org). Le premier chapitre présente les techniques de base en statistique descriptive, représentations graphiques et indicateurs statistiques. Le chapitre suivant introduit léchantillonnage et les théorèmes limites avec une étude des statistiques \\(\\overline{X}_n\\) et \\(S^2\\). Le chapitre 3 est consacré aux problèmes destima tion paramétrique ponctuelle, le chapitre 4 aux intervalles de confiance et le chapitre 5 aux tests dhypothèses. Enfin, des annexes donnent quelques rappels de probabilités utiles en statistique et des tables des lois de probabilité usuelles. "],["statistique-descriptive.html", "Chapitre 1 Statistique descriptive 1.1 Terminologie 1.2 Statistique et Probabilités 1.3 Description dune série de valeurs 1.4 Représentations graphiques 1.5 Indicateurs statistiques 1.6 Statistique descriptive bidimensionnelle", " Chapitre 1 Statistique descriptive La statistique descriptive a pour but de résumer linformation contenue dans les données de façon à en dégager les caratéristiques essentielles sous une forme simple et intelligible. Les deux principaux outils de la statistique descriptive sont les représentations graphiques et les indicateurs statistiques. Il ne faut pas confondre la statistique qui est la science qui vient dêtre définie et une statistique qui est un ensemble de données chiffrées sur un sujet précis. 1.1 Terminologie Les premières statistiques correctement élaborées ont été celles des recensements démographiques. Ainsi le vocabulaire statistique est essentiellement celui de la démographie. Les ensembles étudiés sont appelés population. Les éléments de la population sont appelés individus ou unités statistiques. La population est étudiée selon une ou plusieurs variables (ou caractères). Lensemble des individus constitue léchantillon étudié. Exemple: si léchantillon est un groupe de TD à lESILV, Un individu est un étudiant. La population peut être lensemble des étudiants de lESILV, des élèves ingénieur de France, etc.. Les variables étudiées peuvent être la taille, la filière choisie, la moyenne dannée, la couleur des yeux, etc.. Si léchantillon est constitué de tous les individus de la population, on dit que lon fait un recensement. Il est extrêmement rare que lon se trouve dans cette situation. Quand léchantillon nest quune partie de la population, on parle de sondage. Le principe des sondages est détendre à lensemble de la population les enseignements tirés de létude de léchantillon. Pour que les résultats observés lors dune étude soient généralisables à la population statistique, léchantillon doit être représentatif de cette dernière, cest à dire quil doit refléter fidèlement sa composition et sa complexité. Seul léchantillonnage aléatoire assure la représentativité de léchantillon. Il existe des méthodes pour y parvenir, dont nous ne parlerons pas ici. Un échantillon est qualifié daléatoire lorsque chaque individu de la population a une probabilité connue et non nulle dappartenir à léchantillon. Le cas particulier le plus connu est celui qui affecte à chaque individu la même probabilité dappartenir à léchantillon. Définition 1.1 (Echantillonnage aléatoire simple) Léchantillonnage aléatoire simple est une méthode qui consiste à prélever au hasard et de façon indépendante, \\(n\\) individus ou unités déchantillonnage dune population à \\(N\\) individus. Chaque individu possède ainsi la même probabilité de faire partie dun échantillon de \\(n\\) individus et chacun des échantillons possibles de taille \\(n\\) possède la même probabilité dêtre constitué. 1.2 Statistique et Probabilités Les concepts qui viennent dêtre présentés sont les homologues de concepts du calcul des probabilités et il est possible de disposer en regard les concepts homologues (voir table ci-dessous). Probabilités Statistique Espace fondamental Population Epreuve Tirage (dun individu), expérimentation Evènement élémentaire Individu, observation Variable aléatoire Variable (Caractère) Epreuves répétées Echantillonnage Nbre de répétitions dune épreuve Taille de léchantillon, effectif total Probabilité Fréquence observée Loi de probabilité Distribution observée ou loi empirique Espérance mathématique Moyenne observée Variance Variance observée Le mot variable désigne à la fois la grandeur que lon veut étudier (variable statistique) et lobjet mathématique qui la représente (variable aléatoire). Ainsi la notion de variable se confond avec celle de variable aléatoire. Une variable statistique peut être discrète ou continue, qualitative ou quantitative. Les méthodes de représentation des données diffèrent suivant la nature des variables étudiées. 1.3 Description dune série de valeurs On considère ici une série (un ensemble) de valeurs, numériques, ou non, homogènes en ce sens quelles se réfèrent à une même variable et quelles ne sont pas structurées en sous-ensembles. Chaque valeur est associée à un individu statistique (unité statistique, observation). Cest le cas, par exemple, des notes obtenus par une promotion délèves à un examen. Dans cet exemple, lorsquil y a plusieurs examens, on peut vouloir considérer ensemble les notes dun même élève. Les notes sont alors structurées en sous-ensembles et les méthodes à utiliser diffèrent de celles présentées ici. Pour décrire une telle série, lexamen direct des valeurs nest pas commode dès lors que ces valeurs sont un tant soit peu nombreuses. Pour cela, la statistique propose deux types doutils : des graphiques et des indicateurs statistiques. 1.4 Représentations graphiques Face à un problème particulier, on peut chercher à construire un graphique ad hoc. Mais, la plupart du temps, on utilise des graphiques standard dont la nature diffère selon le type de la variable étudiée. 1.4.1 Variable qualitative Les valeurs que peut prendre une variable qualitative \\(X\\) (ex: couleur) constituent un ensemble de \\(M\\) modalités: {exemple: 1= bleu; 2=blanc;  ; M=rouge}; une telle série présente une apparence numérique mais on ne peut faire de calcul sur ces nombres (dans lexemple, blanc nest pas égale à deux fois bleu). Autres exemples: catégorie socio-professionnelle, genre, région dappartenance, etc. Données On a mesuré une variable qualitative sur \\(n\\) individus. Les données brutes sont constituées par la série des \\(n\\) valeurs \\(\\{x_i; i=1,\\ldots,n\\}\\) avec \\(x_i\\) le numéro de la modalité pour lindividu \\(i\\), \\(x_i \\in 1,\\ldots,M\\). Il est commode dagréger ces données en comptant le nombre dindividus \\(n_m\\) possédant la modalité \\(m\\). \\(n_m\\) est un effectif, ou une fréquence absolue (par opposition à la fréquence relative \\(n_m/n\\)) Table 1.1: Variable qualitative: données brutes (gauche) et agrégées (droite). \\(n_m\\) nombre dindividus possédant la modalité \\(m\\) n°ind. \\(X\\) \\(1\\) \\(x_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(x_i\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(x_n\\) modalité effectif \\(1\\) \\(n_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(m\\) \\(n_m\\) \\(\\vdots\\) \\(\\vdots\\) \\(M\\) \\(n_M\\) Diagramme en bâtons (barplot) Il représente les données agrégées (groupées). Les modalités figurent en abscisse; la longueur dun bâton le long de lordonnée est proportionnelle à leffectif (ou à la fréquence). Il est utile de trier les modalités, généralement par fréquence décroissante. Figure 1.1: Diagramme en bâtons Lordre doit être respecté pour une variable ordinale. Quand il sagit dune variable nominale, il est préférable dordonner les modalités par effectifs croissants ou décroissant pour rendre le graphique plus lisible. Attention à ne pas confondre cette représentation avec un histogramme (cas de variable continue). Diagrammes circulaires (pie chart) Le fameux camembert nest commode que sil y a peu de modalités. Ici encore, il est commode de trier les modalités par effectifs décroissants. Figure 1.2: Diagramme circulaire de la variable couleur des yeux 1.4.2 Variable quantitative Cas de variable quantitative discrète La distribution de \\(X\\) est fournie par le tableau des fréquences qui fait correspondre aux différentes valeurs (modalités) de la variable. Soit lexemple suivant de nombre de personnes par ménage pour les 10 ménages suivants: 2 3 3 3 4 5 5 5 5 6 On peut représenter la série avec un diagramme en bâtons (vertical ou horizontal). Figure 1.3: Le nombre de personnes par ménage Cas de variable quantitative continue Pour la visualisation dune série numérique, loutil de base est lhistogramme, outil qui ressemble à un diagramme en bâtons mais qui doit en être distingué. Il repose sur une subdivision de la plage de variation de \\(X\\) en intervalles; pour chaque intervalle \\([a,b[\\) on compte le nombre dindividus tels que \\(X \\in [a,b[\\), nombre dit effectif de lintervalle. Dans lhistogramme, les intervalles sont représentés sur laxe des abscisses. Au-dessus de chaque intervalle, on dessine un rectangle dont la surface est proportionnelle à son effectif. La base étant la longueur de lintervalle, la hauteur de chaque rectangle sinterprète comme une densité (effectif par unité de longueur). Table 1.2: Variable quantitative: données brutes (gauche) et agrégées (droite). n°ind. \\(X\\) \\(1\\) \\(x_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(x_i\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(x_n\\) intervalle effectif \\(1\\) \\(n_1\\) \\(\\vdots\\) \\(\\vdots\\) \\([a,b[\\) \\(n_{[a,b[}\\) \\(\\vdots\\) \\(\\vdots\\) \\(M\\) \\(n_M\\) Le nombre de classes doit être choisi de façon à ce que leffectif moyen par intervalle soit suffisamment grand afin de pas être gêné par des irrégularités locales et, au contraire, afin de mettre en évidence une tendance générale. Soit lexemple suivant: on sintéresse à la taille de 237 étudiants1. Lidée de représenter en ordonnée la densité (et non leffectif) permet de prendre en compte des intervalles de longueurs inégales, ce qui peut être nécessaire pour les intervalles extrêmes. Cela permet aussi de présenter la distribution de léchantillon. Dans la figure précédente, nous avons utilisé des effectifs (fréquences absolues), on préfère généralement utiliser des fréquences relatives pour pouvoir superposer facilement des distributions de référence. Un autre graphique à base des quartiles est la boîte à dispersion ou boîte à moustaches (boxplot). Un rectangle, délimité par les premier et troisième quartiles, représente \\(50\\%\\) de la population. Dans ce rectangle, une barre centrale représente la médiane. De part et dautre du rectangle, on figure deux segments dont la longueur est environ 1.5 fois lécart interquartile: environ car chaque segment est en fait délimité par une observation réelle incluse dans cet intervalle, celle qui est la plus éloignée de la médiane. Enfin, les observations au-delà de ces limites sont représentées individuellement. Figure 1.4: Boîte à moustaches de Rythme cardiaque de 237 étudiants 1.5 Indicateurs statistiques Les représentations graphiques présentées dans la section précédente ne permettent quune analyse visuelle de la répartition des données. Pour des variables quantitatives, il est intéressant de donner des indicateurs numériques permettant de caractériser au mieux ces données. On donne en général deux indicateurs : un indicateur de tendance centrale (ou de localisation) et un indicateur de dispersion: Lordre de grandeur des observations situées au centre de la distribution: cest la tendance centrale. La largeur de la série, cest-à-dire la plus ou moins grande fluctuation des observations autour de la tendance centrale : cest la dispersion. 1.5.1 Tendance centrale Les mesures de tendance centrale permettent dobtenir une idée juste de lordre de grandeur des valeurs ainsi que de la valeur centrale de la caractéristique que lon désire étudier. Les trois principaux indicateurs de tendance centrale sont: Le Mode. La Moyenne. La Médiane. Le Mode empirique Le mode dune distribution statistique, noté \\(M_o\\), est la modalité de variable la plus représentée dans la distribution. Également appelé valeur dominante de la distribution. Il correspond au sommet de la distribution: le mode est la valeur la plus fréquente La Moyenne empirique La moyenne empirique de léchantillon est la moyenne arithmétique des observations. Cet indicateur, noté \\(\\overline{x}\\), est le plus utilisé. Le calcul de la moyenne dépends de la représentation des données: Si les données statistiques sont exploitées en série (données individuelles): \\[ \\overline{x}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\] Exemple: Les notes dun étudiant dans 7 matières sont: 18 16 15 14 12 17 11 La note moyenne est donc \\[ \\overline{x}=\\frac{18+16+15+14+12+17+11}{7}=14.71 \\] Si les données statistiques sont groupées par modalités, \\(n_i\\) étant leffectif correspondant à la modalité \\(x_i\\) : \\[ \\overline{x}=\\frac{1}{n} \\sum_{i=1}^{p} n_{i} x_{i} \\] Exemple: Soit les notes obtenues par un élève au baccalauréat: \\(x_i\\) \\(n_i\\) 4 2 8 3 16 2 13 3 5 2 Total 12 La moyenne empirique pondérée de cet élève au baccalauréat est: \\[ \\overline{x}=\\frac{(4 \\times 2)+(8 \\times 3)+(16 \\times 2)+(13 \\times 3)+(5 \\times 2)}{12}=\\frac{113}{12}=9.42 \\] La Médiane empirique On appelle médiane dune distribution statistique, notée \\(M_e\\) , la valeur de la variable qui partage en deux groupes deffectif identique les observations classées par ordre croissant. Il y a 50% des observations qui sont inférieures ou égales à la médiane et 50% des observations qui sont supérieures ou égales à la médiane. Lavantage principal de la médiane, par rapport à la moyenne arithmétique, est quelle nest pas indûment influencée par quelques données extrêmes. Si lon considère les \\(x_i\\) rangés par ordre croissant la médiane est définie par: \\[\\begin{align} M_e = x_{(n+1)/2} \\quad &amp;\\text{si}\\quad n \\quad\\text{impair}\\\\ M_e = \\frac{x_{n/2}+x_{(n/2)+1}}{2} \\quad &amp;\\text{si}\\quad n \\quad\\text{pair} \\end{align}\\] 1.5.2 Dispersion (variabilité) Après celle relative à la tendance centrale, la deuxième question que lon se pose à propos dune série numérique est celle de sa dispersion (souvent sous-entendu autour de la tendance centrale). Exemples: Un patient apprend de son médecin que sa pression intra-oculaire est de 19. La pression moyenne pour ceux de son âge et de son sexe est de 17. Que peut-il conclure? Ce nest pas nécessairement inquiétant: les données dune population sont presque toutes distinctes de la moyenne. Mais sécarte-t-il trop de la moyenne? De combien les autres membres de la population sécartent de la moyenne? La température moyenne à Montréal est de 6.9°C. Ceci nempêche pas la température de baisser à -35°C en hiver et de monter à +35°C en été. On va définir: Létendue. La variance et lécart-type. Etendue (amplitude) empirique Cest la différence entre les valeurs maximum et minimum, soit, les valeurs ayant été classées par ordre croissant : \\(x_n-x_1\\). Historiquement, cest le critère le plus ancien (employé par les Grecs pour leurs mesure astronomiques). Il est utilisé lorsque lon ne sintéresse pas en détail à la dispersion : par exemple, on dira que le temps de trajet en TGV entre Rennes et Paris est compris entre 2h05 (train direct) et 2h25 (quelques arrêts). Variance et Ecart-type empiriques La variance et sa racine lécart-type sont les indicateurs de dispersion utilisés de manière standard. La variance (noté usuellement \\(s^2\\) pour une série de valeurs observées, \\(\\sigma^2\\) pour une distribution théorique et \\(V(X)\\) pour une variable aléatoire), est la moyenne des carrés des écarts à la moyenne. Soit, pour une série numérique: \\[ s^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2} \\] Par rapport à la variance, lintérêt de lécart-type est quil sexprime dans les mêmes unités que les valeurs étudiées. Cependant, la variabilité doit toujours se comparer à la valeur moyenne. Des données présentent une forte variabilité si lécart-type est grand par rapport à la moyenne. Aussi on définit le coefficient de variation empirique de léchantillon par \\[ cv_{n}=\\frac{s_{n}}{\\overline{x}_{n}} \\] Lintérêt de cet indicateur est quil est sans dimension. Une pratique empirique courante est de considérer que léchantillon possède une variabilité significative si \\(cv_{n} &gt; 0.15\\). Si \\(cv_{n} \\leq 0.15\\), les données présentent peu de variabilité et on considère que la moyenne empirique à elle seule est un bon résumé de tout léchantillon. En , la commande var(x) donne \\(s*^2 = \\frac{n}{n-1}s^2\\) au lieu de \\(s^2\\). Cest aussi ce que lon a sur les calculatrices dotées de fonctionnalités statistiques. On en verra lexplication au chapitre Estimation. De même \\(s*=\\sqrt{s*^2}\\) est donné en par sd(x) (standard deviation). \\(s_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}-\\overline{x}_{n}^{2}\\) évoque \\(\\operatorname{Var}(X)=E\\left[(X-E(X))^{2}\\right]=E\\left(X^{2}\\right)-[E(X)]^{2}\\). En finance, la variabilité dune série de données est appelée volatilité. Létude de la volatilité est fondamentale dans les analyses de risque financier. 1.6 Statistique descriptive bidimensionnelle Dans cette section, on sintéresse a létude simultanée de deux variables \\(X\\) et \\(Y\\), étudiées sur le même échantillon, toujours noté \\(\\Omega\\). Lobjectif essentiel des méthodes présentées est de mettre en évidence une éventuelle variation simultanée des deux variables, que nous appellerons alors liaison. Dans certains cas, cette liaison peut être considérée a priori comme causale, une variable \\(X\\) expliquant lautre \\(Y\\); dans dautres, ce nest pas le cas, et les deux variables jouent des rôles symétriques. Dans la pratique, il conviendra de bien différencier les deux situations et une liaison nentraîne pas nécessairement une causalité. Les données sont présentées de la façon suivante: on dispose de deux séries \\(X\\) et \\(Y\\) représentant lobservation des variables \\(X\\) et \\(Y\\) sur les mêmes \\(n\\) individus: on a une série bidimensionnelle \\((X,Y)\\) de taille \\(n\\): individu \\(X\\) \\(Y\\) \\(1\\) \\(x_1\\) \\(y_1\\) \\(2\\) \\(x_2\\) \\(y_2\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\ldots\\) \\(i\\) \\(x_i\\) \\(y_i\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\ldots\\) \\(n\\) \\(x_n\\) \\(y_n\\) où \\(x_i\\) est la valeur de \\(X\\) et \\(y_i\\) celle de \\(Y\\) pour lindividu n° \\(i\\) de la série. Deux variables quantitatives Pour étudier la liaison entre deux variables quantitatives (discrètes), on commence par faire un graphique du type nuage de points (scatterplot). La forme générale de ce graphique indique sll existe ou non une liaison entre les deux variables. Il sagit dun graphique très commode pour représenter les observations simultanées de deux variables quantitatives. Figure 1.5: Exemple de nuage de points représentant la consommation de carburant en fonction du poids de voitures. Lexemple extrait du dataset mtcars dans R Le choix des échelles a retenir pour réaliser un nuage de points peut savérer délicat. Dune façon générale, on distinguera le cas de variables homogènes (représentant la même grandeur et exprimées dans la même unité) de celui des variables hétérogenes. Dans le premier cas, on choisira la même échelle sur les deux axes (qui seront donc orthonormés); dans le second cas, il est recommandé soit de représenter les variables centrées et réduites sur des axes orthonormés, soit de choisir des échelles telles que ce soit sensiblement ces variables là que lon représente (cest en genéral cette seconde solution quutilisent, de façon automatique, les logiciels statistiques). Rappel: variables centrées et réduites Si \\(X\\) est une variable quantitative de moyenne \\(\\overline{x}\\) et décart-type \\(\\sigma_{X},\\) on appelle variable centrée associée à \\(X\\) la variable \\(X-\\overline{x}\\) (elle est de moyenne nulle et décart-type \\(\\sigma_{X}\\)), et variable centrée et réduite (ou tout simplement variable réduite) associe à \\(X\\) la variable \\(\\frac{X-\\overline{x}}{\\sigma}\\) (elle est de moyene nulle et décart-type égal à un). Une variable centrée et réduite sexprime sans unité. Coefficient de corrélation linéaire On appelle coefficient de corrélation linéaire de \\(X\\) et de \\(Y\\) la valeur définie par \\[\\rho = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] Où \\(Cov(X,Y)\\) est la covariance de \\(X\\) et de \\(Y\\) la valeur si elle existe: \\[Cov(X,Y) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\\] On peut montrer que \\(-1 \\leq \\rho(X,Y) \\leq 1\\). Interprétation de \\(\\rho\\) Le coefficient de corrélation est une mesure du degré de linéarité entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proches de \\(1\\) ou \\(-1\\) indiquent une linéarité quasiment rigoureuse entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proche de 0 indiquent une absence de toute relation linéaire. Lorsque \\(\\rho(X,Y)\\) est positif, \\(Y\\) a tendance à augmenter si \\(X\\) en fait autant. Lorsque \\(\\rho(X,Y) &lt; 0\\), \\(Y\\) a tendance à diminuer si \\(X\\) augmente. Si \\(\\rho(X,Y) =0\\), on dit que ces deux statistiques sont non corrélées. Figure 1.6: Illustration de leffet de la variation of the effect of varying the strength and direction of a correlation La corrélation mesure lassociation, pas la causalité. Dans certains cas, une liaison peut être considérée a priori comme causale, une variable expliquant lautre. Dans dautres, ce nest pas le cas et les deux variables jouent alors des rôles symétriques. Sur ce lien on trouve des exemples de variables associées linéairement mais non liées de manière causale. Dautre part, une corrélation nulle ne signifie pas que deux variables sont indépendantes. Voici quelques exemples: Figure 1.7: Exemples où la corrélation est presque nulle mais les variables ne sont pas indépendantes. Une variable quantitative et une qualitative On dispose dune variable qualitative \\(X\\) à \\(p\\) modalités \\(m_{1}, \\ldots, m_{p}\\) et une variable quantitative \\(Y\\). On a alors \\(p\\) sous-populations déterminées par les \\(p\\) modalités de \\(X\\). Létude de la liaison entre \\(X\\) et \\(Y\\) consiste en létude des différences entre ces sous-populations: il y aura absence de lien si on ne distingue pas de différence notoire dans les caractéristiques de ces différentes sous-populations. Une façon commode de représenter les données dans le cas de létude simultanée dune variable quantitative et dune variable qualitative consiste à réaliser des boîtes à moustaches parallèles. Les boîtes à moustaches permettent de comparer facilement des groupes dindividus, par exemple ici les garçons et les filles parmi 237 étudiants2: Figure 1.8: Boîtes à moustaches par sexe Deux variables qualitatives On considère dans ce paragraphe deux variables qualitatives observées simultanément sur \\(n\\) individus. On suppose que la première, notée \\(X,\\) possède \\(r\\) modalités notées \\(x_{1}, \\ldots, x_{\\ell}, \\ldots, x_{r},\\) et que la seconde, notée \\(Y,\\) possède \\(c\\) modalités notées \\(y_{1}, \\ldots, y_{h}, \\ldots, y_{c}\\). Ces données sont présentées dans un tableau à double entrée, appelé tables de contingence, dans lequel on dispose les modalités de \\(X\\) en lignes et celles de \\(Y\\) en colonnes. Ce tableau est donc de dimension \\(r \\times c\\) et a pour élément générique le nombre \\(n_{\\ell h}\\) dobservations conjointes des modalités \\(x_{\\ell}\\) de \\(X\\) et \\(y_{h}\\) de \\(Y ;\\) les quantités \\(n_{\\ell h}\\) sont appelées les effectifs conjoints. Une table de contingence se présente donc sous la forme suivante: \\(y_1\\) \\(\\ldots\\) \\(y_h\\) \\(\\ldots\\) \\(y_c\\) sommes \\(x_1\\) \\(n_{11}\\) \\(n_{1h}\\) \\(n_{1c}\\) \\(n_{1.}\\) \\(\\vdots\\) \\(x_{\\ell}\\) \\(n_{\\ell 1}\\) \\(n_{\\ell h}\\) \\(n_{\\ell c}\\) \\(n_{\\ell .}\\) \\(\\vdots\\) \\(x_r\\) \\(n_{r 1}\\) \\(n_{r h}\\) \\(n_{rc}\\) \\(n_{r.}\\) sommes \\(n_{.1}\\) \\(n_{.h}\\) \\(n_{.c}\\) \\(n\\) Les quantités \\(n_{\\ell .}(\\ell=1, \\ldots, r)\\) et \\(n_{.h}(h=1, \\ldots, c)\\) sont appelées les effectifs marginaux; ils sont définis par \\(n_{\\ell .}=\\sum_{h=1}^{c} n_{\\ell h}\\) et \\(n_{. h}=\\sum_{\\ell=1}^{r} n_{\\ell h}\\) et ils vérifient \\(\\sum_{\\ell=1}^{r} n_{\\ell.}=\\sum_{h=1}^{c} n_{.h}=n\\). De façon analogue, on peut définir les notions de fréquences conjointes et de fréquences marginales. On peut représenter graphiquement deux variables qualitatives avec des diagrammes en barre parallèles (mosaïcplots). Indices de liaison Lorsque tous les profil-lignes sont égaux, ce qui est équivalent à ce que tous les profil-colonnes soient égaux et que \\[ \\forall(\\ell, h) \\in\\{1, \\ldots, r\\} \\times\\{1, \\ldots, c\\} : n_{\\ell h}=\\frac{n_{\\ell .} n_{.h}}{n} \\] on dit quil nexiste aucune forme de liaison entre les deux variables considérées \\(X\\) et \\(Y .\\) Par suite, la mesure de la laison va se faire en évaluant lécart entre la situation observée et létat de non liaison défini ci-dessus. Khi-deux Il est courant en statistique de comparer une table de contingence observée, deffectif conjoint générique \\(n_{\\ell h},\\) à une table de contingence donnée a priori (et appelée standard), deffectif conjoint générique \\(s_{\\ell h},\\) en calculant la quantié \\[ \\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-s_{\\ell h}\\right)^{2}}{s_{\\ell h}} \\] De façon naturelle, pour mesurer la liaison sur une table de contingence, on utilise donc lindice appelé khi-deux (chi-square) et défini comme suit: \\[ \\chi^{2}=\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-\\frac{n_{\\ell+} n_{+h}}{n}\\right)^{2}}{\\frac{n_{\\ell+} n_{+h}}{n}}=n\\left[\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{n_{\\ell h}^{2}}{n_{\\ell+} n_{+h}}-1\\right] \\] Le coefficient \\(\\chi^{2}\\) est toujours positif ou nul et il est dautant plus grand que la liaison entre les deux variables considérés est forte. Un document complet et intéressant sur la visualization des données est sur ce lien Disponibles dans le jeu de données survey de la librairie MASS de  On pourra dire en regardant ces deux figures quil ny a pas de différence de rythme cardiaque entre les garçons et les filles, mais il y probablement une différence de taille. Pour confirmer cette comparaison on effectuera un test dhypothèse de comparaison de moyennes entre deux échantillons. "],["exercices.html", "Exercices", " Exercices Exercice 1.1 (Nature des variables et graphique correspondant) Lors dune enquête, on interroge 1000 individus sur leur âge, leur sexe, leur couleur préférée, leur nombre de frères et soeurs et leur département de naissance. Quelle est la nature de chacune de ces variables? Par quel outil graphique visualiseriez-vous la distribution de chacune des variables élémentaires? Exercice 1.2 (Petite série deffectifs) On réalise un sondage auprès de 1000 personnes pour évaluer le nombre dindividus par ménage. On obtient la série statistique suivante: Nb de personnes par ménage 1 2 3 4 5 6 Total effectif 107 137 197 302 180 77 1000 Représenter cette série statistique à laide dun graphique. Calculer la moyenne, la variance, lécart-type et la médiane de cette distribution. Exercice 1.3 (Robustesse de la médiane aux données extrêmes) On a repertorié le temps mis par 20 étudiants pour décrocher un premier emploi (en mois): 0 0 0 0 0 1 1 1 2 2 2 3 3 3 4 4 6 6 8 20 Calculer la moyenne puis la médiane des observations. Létudiant qui a mis 20 mois à trouver du travail a réalisé un tour du monde avant de chercher un emploi. On considère donc quil ne fait pas partie de la population qui nous intéresse et on décide de supprimer cette valeur de léchantillon. Même question que la question 1 sans la dernière valeur (20). Comparer la stabilité des deux indicateurs de position vis-à-vis de lélimination de valeurs extrêmes. Exercice 1.4 (Corrélation) Les données suivantes représentent la taille (en cm) et le salaire annuel brut (en k) de 12 avocats ayant fait la même formation et obtenu presque les même notes. Taille Salaire 162.6 61 165.1 64 167.6 58 170.2 73 175.3 47 177.8 66 Taille Salaire 182.9 75 182.9 58 188.0 92 188.0 72 190.5 60 193.0 84 Représenter les données sur un nuage de points. Calculer le coefficient de corrélation et interpréter. "],["tp-statistique-descriptive-avec-r.html", "TP Statistique descriptive avec R Quest-ce que cest que ? 1ère partie: Données quantitatives discrètes 2ème partie : Analyse descriptive", " TP Statistique descriptive avec R Quest-ce que cest que ? Cest un langage de programmation et un logiciel gratuit et libre. Il est surtout utilisé pour le développement de programmes statistiques et des analyses de données. Il gagne en popularité depuis quelques années avec lémergence de la data science et du fait quil est gratuit et ouvert (open-source). est née dun projet de recherche mené par deux chercheurs, Ross Ihaka et Robert Gentleman à luniversité dAuckland (Nouvelle-Zélande) en 1993. En 1997 est mis en place le Comprehension R Archive Network (CRAN) qui centralise les contributions au projet. Depuis le projet connaît une croissance soutenue, grâce à des contributions de la part de milliers de personnes à travers le monde. RStudio Cest une IDE (Integrated Development Environment) ou Environnement Intégré de Développement. Il sert dinterface entre et lutilisateur, offre à celui diverses commodités dutilisation Une introduction au RStudio est présentée dans lannexe A. Vous trouverez une bonne introduction à sur ce lien . 1ère partie: Données quantitatives discrètes Le nombre darbres plantés sur les parcelles dun lotissement a été compté. Les données obtenues sont les suivantes: \\[1,2,4,1,6,3,2,1,2,0,1,2,2,1,3,0,3,2,1,2,2,3,2,3.\\] 1. Quelle est la nature de variable étudiée? 2. Rentrer ces données sous la forme dun vecteur nommé arbres et affichez ce vecteur. 3. Trier les valeurs de ce vecteur par ordre croissant. 4. Donner la taille de léchantillon (cest-à-dire le nombre de composantes de ce vecteur) en la notant n et affichez sa valeur. Effectifs et fréquence 5. Montrer la séquence des modalités et la séquence des effectifs correspondants. 6. Montrer le tableau de fréquences et de pourcentages. 7. Calculer et afficher les effectifs cumulés et les fréquences cumulées. Mesures de tendance centrale 8. Calculer le nombre moyen darbres par parcelle. 9. Calculer le nombre maximum et le nombre minimum darbres sur une parcelle. 10. Calculer le nombre médian darbres par parcelle. 11. Utiliser la fonction summary() pour obtenir un tableau récapitulatif des indicateurs. Indicateurs de dispersion 12. Calculer la variance du nombre darbres plantés sur les parcelles. 13. Calculer maintenant lécart-type et vérifier que lécart-type est la racine carrée de la variance. 14. Calculer la variance vous-même. La variance obtenue est elle la même que la précédente? Le logiciel utilise \\(n-1\\) pour le dénomiateur dans la définition de la variance, cest-à-dire \\(\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2\\) (décart-type noté \\(\\sigma_{n-1}\\) ou \\(s\\). Cette quantité est souvent préférée dans les applications numériques pour des questions destimation). Représentations graphiques 15. La fontion plot() affiche par défaut un nuage de points avec en abscisse le numéro de lobservation (ici de 1 à 24) et en ordonnée le nombre darbres. Tester cette fonction. Modifier le titre de la figure, les noms des axes, la couleur et la forme des points affichés. 16. Afficher la courbe des fréquence cumulées. (Indication: Utiliser la fonction ecdf()). 17. Tracer un diagramme en bâtons par la fonction barplot() à partir du tableau des effectifs ou des fréquences. 2ème partie : Analyse descriptive Données utilisées Une enquête a été réalisée sur 237 étudiants. Les données sont les suivantes: Sex: The sex of the student. (Factor with levels Male and Female.) Wr.Hnd: span (distance from tip of thumb to tip of little finger of spread hand) of writing hand, in centimetres. NW.Hnd: span of non-writing hand. W.Hnd: writing hand of student. (Factor, with levels Left and Right.) Fold: Fold your arms! Which is on top (Factor, with levels R on L, L on R, Neither.) Pulse: pulse rate of student (beats per minute). Clap: Clap your hands! Which hand is on top? (Factor, with levels Right, Left, Neither.) Exer: how often the student exercises. (Factor, with levels Freq (frequently), Some, None.) Smoke: how much the student smokes. (Factor, levels Heavy, Regul (regularly), Occas (occasionally), Never.) Height: height of the student in centimetres. M.I: whether the student expressed height in imperial (feet/inches) or metric (centimetres/metres) units. (Factor, levels Metric, Imperial.) Age: age of the student in years. Définition du répertoire de travail Vous avez la possibilité de définir un Répertoire de travail dans lequel vous allez stocker votre script R, vos données etc Ceci est réalisé par la fonction setwd(\"..Chemin/de/votre/repertoire\"). Cette fonction considère comme seul paramètre le chemin daccès au répertoire que vous avez choisi. A tout moment, vous pouvez vérifier le répertoire de travail courant en executant linstruction suivante: getwd() 1. Définisser votre répertoire de travail. Chargement des données Il existe une multitude de fonctions qui permettent de charger un fichier de données. Télécharger le fichier de données en cliquant ici et enregistrer le dans votre répertoire. Ensuite utiliser la fonction read.csv() pour charger les données dans . Cette fonction prend comme principaux paramètres dentrée le fichier à charger (file=\"data.txt\"), le séparateur de colonnes dans le fichier initial (sep=) et la présence (ou non) des noms de colonnes dans le fichier (header=). Ouvrez toujours le fichier de données dans un éditeur de texte pour connaitre le séparateur de colonnes et voir si les noms de colonnes sont présents. 2. Charger le fichier des données dans lenquête dans un tableau nommé data. Linstruction de chargement du fichier est la suivante: donnees = read.csv(&quot;enquete.csv&quot;, header = T , sep=&quot;,&quot;) Le fichier de données est chargé dans lenvironnement et est affecté à lobjet donnees. Cest cet objet, de type dataframe qui va faire lobjet de manipulations par la suite. 3. Afficher le nombre de dobservations (lignes) et le nombre de variables (colonnes). 4. Utiliser la fonction head() pour afficher les premières lignes (6 par défaut) de données chargées. 5. Laccès à une colonne dun dataframe se fait par la notation $: nom_du_dataframe$nom_variable. Afficher les valeurs de la variable Age de vos données. Analyse descriptive univariée Indicateurs statistiques pour variables quantitatives 6. Calculer et afficher la moyenne et lécart-type dage des élèves qui ont participé à lenquête. 7. Appliquer la fonction summary() sur la variable Age. Quest ce que cette fonction calcule et affiche? Représentations graphiques pour variables quantitatives 8. Tracer lhistogramme de la variable Age. Ecrire un titre correspondante à votre figure, modifier les noms des axes et les couleurs des bâtons. 9. Afficher la boîte à moustache correspondante à la variable Age. Commenter ce quon observe sur cette figure. Indicateurs statistiques et représentations graphiques pour variables qualitatives 10. Choisir une variable qualitative parmi les variables de cette enquête. Justifier votre choix. Calculer et afficher les effectifs et les fréquences de cette variable. Pour voir les variables dans votre dataframe, vous pouvez utiliser la fonction names() pour afficher les noms des variables, ou str() (structure) pour voir toutes les colonnes, leur types, les quelques premières valeurs, etc.. Ou simplement dans Rstudio on peut voir la structure du dataframe dans la fenêtre Environment. 11. Afficher un diagramme circulaire (en utilisant la fonction pie()) pour la variable qualitative choisie. Analyse descriptive bivariée Indicateurs pour le croisement de deux variables qualitatives Le tableau de contingence est un moyen particulier de représenter simultanément deux caractères observés sur une même population, sils sont discrets ou bien continus et regroupés en classes. 12. Un tableau de contingence des effectifs joints croisant deux variables qualitatives est réalisé par la fonction table(). Effectuer et afficher le croisement de deux variables (Sex) et (Smoke). 13. Utiliser la fonction addmargins() pour ajouter au tableau les effectifs marginaux. Représentations graphiques pour le croisement de deux variables qualitatives On peut représenter le croisement de deux variables qualitatives avec un diagramme en bâtons. Dans le cas de deux variables qualitatives, la fonction barplot() prend comme premier paramètre le tableau de contingence. 14. Afficher un diagramme en bâtons représentant la distribution du tabagisme (variable Smoke) en fontion du sexe des étudiants. La figure souhaitée est la suivante: On remarque quil y a plus dhommes que des femmes qui fument régulièrement. (selon lenquête réalisée) Il est plus convenable dans ce cas de représenter le croisement de ces deux variables qualitatives à laide dun mosaicplot. Indicateurs pour le croisement dune variable qualitative et dune variable quantitative Disons quon souhaite calculer la moyenne de fréquence cardiaque chez les hommes ayant répondu au questionnaire de lenquête. On a besoin de filtrer le dataframe de la façon suivante: # On utilise la fontion subset pour créer un sous ensemble de nos données # Remarquer qu&#39;on utilise == pour comparer pulse_hommes = subset(donnees, donnees$Sex==&quot;Male&quot;) 15. Vérifier que le sous ensemble créé pulse_homme ne contient que des hommes. 16. Ensuite calculer les indicateurs statistiques de la fréquence cardiaque chez les hommes (vous pouvez utiliser la fonction summary()). 17. Faire la même chose mais pour les femmes. Représentation graphique pour le croisement dune variable qualitative et dune variable quantitative On peut réaliser une boîte à moustache des valeurs de la variable quantitative en fonction des modalités de la variables qualitative, pour cela on peut utliser la fonction boxplot(). Plus précisémeent, on utilise le paramètre formula qui permet de spécifier que nous voulons une boîte à moustache de la variable quantitative en fonction (caractère ~ ) de la variable qualitative. 18. Afficher sur une même figure la fréquence cardiaque en fontion du sexe. Interpréter la figure. La figure souhaitée est la suivante: 19. Afficher sur une même figure la taille (variable Height) en fontion du sexe. Interpréter la figure. Représentation graphique pour le croisement de deux variables quantitatives Un nuage de points entre les deux variables quantitatives est réalisé par la fonction plot(). Le premier paramètre correspond à la variable en abscisse et le deuxième à la variable en ordonnées. 20. Afficher la fréquence cardiaque en fonction de lage. Modifier les paramètres de la figure (titre, noms des axes, couleurs des points, tailles, formes, etc..)  "],["échantillonnage-et-théorèmes-limites.html", "Chapitre 2 Échantillonnage et Théorèmes limites 2.1 Échantillonnage 2.2 La statistique \\(\\overline{X}_n\\) 2.3 Théorèmes limites 2.4 Loi dun pourcentage 2.5 Etude de la statistique \\(S^2\\) 2.6 Introduction à lestimation", " Chapitre 2 Échantillonnage et Théorèmes limites 2.1 Échantillonnage Létude dune caractéristique dune pièce fabriquée en grand nombre (telle que la luminosité dune ampoule, sa durée de vie ou encore le diamètre dune pièce mécanique) relève, elle aussi, de la statistique descriptive. Il nest toutefois pas possible de mesurer cette caractéristique sur toutes les pièces produites. Il est alors nécessaire de se limiter à létude des éléments dun échantillon. Cet échantillon devra répondre à des critères particuliers pour pouvoir représenter la population toute entière dans létude statistique. La démarche statistique présente plusieurs étapes : Prélèvement dun échantillon représentatif de la population ou échantillon aléatoire par des techniques appropriées. Cela relève de la théorie de léchantillonnage. Étude des caractéristiques de cet échantillon, issu dune population dont on connaît la loi de probabilité. On sintéresse principalement à ceux issus dune population gaussienne. Définition 2.1 (Echantillon) Un échantillon aléatoire est un \\(n\\)-uplet \\((X_1,\\ldots,X_n)\\) de \\(n\\) variables aléatoires indépendantes suivant la même loi quune variable \\(X\\) appelée variable aléatoire parente. Une réalisation de léchantillon sera notée \\((x_1,\\ldots,x_n)\\). Définition 2.2 (Une statistique) Soit \\(X\\) une variable aléatoire. Considérons un \\(n\\)-échantillon \\((X_1,\\ldots,X_n)\\) de \\(X\\). Une Statistique \\(T\\) est une variable aléatoire fonction mesurable de \\((X_1,\\ldots,X_n)\\). \\[ T(X)=T(X_1,\\ldots,X_n) \\] A un échantillon, on peut associer plusieurs statistiques. 2.2 La statistique \\(\\overline{X}_n\\) Définition 2.3 (Moyenne empirique) La statistique \\(\\overline{X}_n\\) ou moyenne empirique dun échantillon est une statistique définie par \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\] Espérance et variance de \\(\\overline{X}_n\\): Soit \\(m\\) lespérance et \\(\\sigma^2\\) la variance de la variable parente \\(X\\) (e.g. lespérance et la variance de la population). Lespérance et la variance de la statistique \\(\\overline{X}_n\\) sont: \\[E(\\overline{X}_n) = m\\] \\[V(\\overline{X}_n) = \\frac{\\sigma^2}{n}\\] Démonstration: \\(E(\\overline{X}_n) = \\frac{1}{n} \\sum_{i=1}^n E(X_i) = \\frac{1}{n} nm = m\\) \\(V(\\overline{X}_n) =V(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n^2} V(\\sum_{i=1}^n X_i) = \\frac{1}{n^2} \\sum_{i=1}^n V(X_i) = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\quad \\quad\\) (Les \\(X_i\\) étant supposées indépendantes) 2.3 Théorèmes limites Cette section introduit trois résultats importants de la théorie asymptotique des probabilités: la loi faible des grands nombres, la loi forte des grands nombres et le théorème central limite, dans sa version pour variables aléatoires indépendantes et identiquement distribuées (\\(X_i\\) suivent la même loi pour \\(i=1,\\ldots,n\\)). Ce sont des résultats qui traitent les propriétés de la distribution de la statistique \\(\\overline{X}_n\\). Des variables aléatoires indépendantes et identiquement distribuées (i.i.d) sont des variables aléatoires indépendantes qui suivent la même loi de probabilité, donc ont la même espérance et la même variance. Les deux lois des grands nombres énoncent les conditions sous lesquelles la moyenne dune suite de variables aléatoires converge vers leur espérance commune et expriment lidée que lorsque le nombre dobservations augmente, la différence entre la valeur attendue (\\(m = E(X)\\)) et la valeur observée (\\(\\overline{X}_n\\)) tend vers zéro. De son côté, le théorème central limite établit que la distribution standardisée dune moyenne tend asymptotiquement vers une loi normale, et cela même si la distribution des variables sous-jacentes est non normale. Ce résultat est central en probabilités et statistique et peut être facilement illustré (cf. figure 2.1). Indépendamment de la distribution sous-jacente des observations (ici une loi uniforme), lorsque \\(n\\) croît, la distribution de \\(\\overline{X}_n\\) tend vers une loi normale: on observe dans lillustration la forme de plus en plus symétrique de la distribution ainsi que la concentration autour de lespérance (ici \\(m = 0.5\\)) et la réduction de la variance. Figure 2.1: Illustration du théorème central limite: histogramme de la moyenne de 200 échantillons issus dune loi uniforme sur lintervalle (0,1) en fonction de la taille \\(n\\) de léchantillon. Les retombées pratiques de ces résultats sont importantes. En effet, la moyenne de variables aléatoires est une quantité qui intervient dans plusieurs procédures statistiques. Aussi, le résultat du théorème central limite permet lapproximation des probabilités liées à des sommes de variables aléatoires (e.g. méthode de Monte-Carlo). De plus, lorsque lon considère des modèles statistiques, le terme derreur représente la somme de beaucoup derreurs (erreurs de mesure, variables non considérées, etc.). En prenant comme justification le théorème central limite, ce terme derreur est souvent supposé se comporter comme une loi normale. 2.3.1 Loi faible des grands nombres Théorème 2.1 (Loi faible des grands nombres) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées. On suppose que \\(E(|X_i|) &lt; \\infty\\) et que tous les \\(X_i\\) admettent la même espérance \\(E(X_i)=m\\). Alors: \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\rightarrow m \\quad \\text{au sens de la convergence en probabilités}\\] càd \\(\\forall \\varepsilon &gt; 0, \\lim_{n \\to \\infty} P(|\\overline{X}_n - m|&gt; \\varepsilon) = 0\\). \\(\\overline{X}_n \\text{ converge en probabilité vers } m \\text{ quand } n \\rightarrow +\\infty\\). 2.3.2 Loi forte des grands nombres Théorème 2.2 (Loi forte des grands nombres) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées. On suppose que \\(E(|X_i|) &lt; \\infty\\) et que tous les \\(X_i\\) admettent la même espérance \\(E(X_i)=m\\). Alors: \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\rightarrow m \\quad \\text{presque sûrement}\\] càd \\(P\\big( \\lim_{n \\to \\infty} \\overline{X}_n = m\\big) = 1\\). \\(\\overline{X}_n \\text{ converge presque sûrement vers } m\\) Illustration de la loi des grands nombres Prenons lexemple dun lancer de dé équilibré. On lance un dé et on note \\(X\\) le résultat obtenu. La loi de \\(X\\) est la suivante: \\(x_i\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(P(X = x_i)\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) Et donc lespérance de \\(X\\) est \\(E(X)=\\sum_i p_i x_i = 3.5\\). Pour illustrer le théorème on va procéder à un échantillonnage. On répète le lancement du dé \\(n\\) fois. A chaque \\(n\\) on va calculer la moyenne empirique des résultats obtenus, quon va noter \\(\\overline{X}_n\\). Selon la loi de grands nombre cette moyenne va converger vers lespérance théorique: \\[\\overline{X}_n \\rightarrow E(X)=m=3.5 \\quad \\text{quand} \\quad n \\rightarrow \\infty\\] Traçons lévolution de la moyenne empirique en fonction de la taille \\(n\\) de léchantillon, dans deux différents échantillonnages aléatoires: Figure 2.2: Convergence de \\(\\overline{X}_n\\) vers \\(m=3.5\\) pour deux différents échantillonnages 2.3.3 Théorème central limite Théorème 2.3 (Théorème central limite) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées, despérance \\(m\\) et variance \\(\\sigma^2\\) finie. Alors \\[\\begin{equation} \\frac{\\overline{X}_n - m}{ \\sigma/\\sqrt{n}} \\xrightarrow{n \\to \\infty} \\mathcal{N}(0,1) \\quad \\text{en distribution} \\tag{2.1} \\end{equation}\\] On voit bien que, afin que la convergence se fasse, une standardisation est nécessaire: en effet, on peut voir le rapport dans (2.1) comme \\[\\frac{\\overline{X}_n - m}{ \\sigma/\\sqrt{n}} = \\frac{\\overline{X}_n - E(\\overline{X}_n)}{ \\sqrt{var(\\overline{X}_n)}}\\] Notes Historiques La loi faible des grands nombres a été établie la première fois par J. Bernoulli pour le cas particulier dune variable aléatoire binaire ne prenant que les valeurs 0 ou 1. Le résultat a été publié en 1713. La loi forte des grands nombres est due au mathématicien E. Borel (1871- 1956), doù parfois son autre appellation: théorème de Borel. Le théorème central limite a été formulé pour la première fois par A. de Moivre en 1733 pour approximer le nombre de piles dans le jet dune pièce de monnaie équilibrée. Ce travail a été un peu oublié jusquà ce que P.S. Laplace ne létende à lapproximation dune loi binomiale par la loi normale dans son ouvrage Théorie analytique des probabilités en 1812. Cest dans les premières années du \\(XX^e\\) siècle que A. Lyapounov la redéfini en termes généraux et prouvé avec rigueur. Application 1: Pour une taille déchantillon \\(n\\) suffisamment grande, on peut considérer que \\(\\overline{X}_n\\) a pour loi: \\[ \\overline{X}_n \\thicksim \\mathcal{N}\\big(m, \\frac{\\sigma^2}{n}\\big)\\] Dans la notation de la loi normale ci dessus \\(\\frac{\\sigma^2}{n}\\) est la variance. \\(\\frac{\\sigma}{\\sqrt{n}}\\) est lécart-type. Application 2: la loi dun pourcentage, étudiée dans la section suivante. 2.4 Loi dun pourcentage Soit \\(X\\) la variable aléatoire représentant le nombre de succès au cours dune suite de \\(n\\) répétitions indépendantes dune même épreuve dont la probabilité de succès est \\(p\\). La loi de \\(X\\) est la loi binomiale de paramètres \\(n\\) et \\(p\\) notée \\(\\mathcal{B}(n, p)\\). \\(X\\) est la somme de \\(n\\) variables indépendantes de Bernoulli de paramètre \\(p\\). Notons \\(P_n\\) la fréquence empirique du nombre de succès parmi les \\(n\\) épreuves: \\[P_n= \\frac{X}{n}\\] \\(P_n = \\overline{X}_n\\) car \\(X\\) est la somme de \\(n\\) variables indépendantes de Bernoulli de paramètre \\(p\\). \\(P_n\\) a pour espérance et pour variance: \\[E(P_n) = p \\quad \\quad \\text{et} \\quad \\quad V(P_n) = \\frac{p(1-p)}{n}\\] En appliquant le théorème central limite à \\(X\\) somme des variables de Bernoulli: \\[ \\text{Pour } n \\text{ suffisamment grand, on peut considérer que } P_n \\text{ suit la loi normale :}\\] \\[P_n \\thicksim \\mathcal{N}\\bigg(p, {\\frac{p(1-p)}{n}}\\bigg)\\] Ce résultat est une autre formulation du théorème de De Moivre-Laplace (lien). 2.5 Etude de la statistique \\(S^2\\) Définition 2.4 (Variance empirique) La statistique \\(S_n^2\\) ou variance empirique déchantillon est définie par: \\[S_n^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_n\\right)^{2}\\] Propriétés: \\(S_n^{2}=\\frac{1}{n} \\big(\\sum_{i=1}^{n}X_{i}^2\\big)-\\big(\\overline{X}_n\\big)^{2}\\). \\(S_n^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\big(X_{i}-m\\big)^2-\\big(\\overline{X}_n-m\\big)^{2}\\). \\(S_n^{2} \\text{ converge presque sûrement vers } \\sigma^2\\). Espérance de \\(S_n^{2}\\): Lespérance de \\(S_n^{2}\\) est: \\[E(S_n^{2}) = \\frac{n-1}{n}\\sigma^2\\] démonstration: \\[\\begin{align} E(S_n^{2}) &amp; =\\frac{1}{n} \\sum_{i=1}^{n}E\\big(X_{i}-m\\big)^2-E\\big(\\overline{X}_n-m\\big)^{2} \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n}V(X_i)-V(\\overline{X}_n) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n}\\sigma^2- \\frac{\\sigma^2}{n} = \\sigma^2- \\frac{\\sigma^2}{n} = \\frac{n-1}{n} \\sigma^2 \\end{align}\\] On peut remarquer que si on pose: \\({S_n^{*}}^2 = \\frac{n}{n-1}S_n^2\\) alors \\(E({S_n^{*}}^2) = \\sigma^2\\). 2.6 Introduction à lestimation On appelle estimation, la procédure dutilisation des informations obtenues à partir dun échantillon qui permet de déduire des résultats concernant lensemble de la population. Dans ce cours, les estimations sont calculées à partir dun échantillonnage aléatoire simple et avec remise, càd tous les individus de la population ont une probabilité égale de faire partie de léchantillon et quun individu peut être choisi plus dune fois. La statistique inconnue dune population, à estimer à partir dun échantillon, est appelée un paramètre. Souvent le paramètre à estimer est une moyenne, un total, une proportion, un écart-type ou une variance. Le paramètre de la population est estimé à partir dune estimation elle même calculée à partir des données dun échantillon. Le tableau ci dessous illustre les différents symboles souvent utilisés. Paramètres population Estimations calculées sur un échantillon de taille \\(n\\) Moyenne \\(m\\) \\(\\overline{X}_n=\\hat{m}\\) Ecart-type \\(\\sigma\\) \\(S_n=\\hat{\\sigma}\\) Variance \\(\\sigma^2\\) \\(S_n^2=\\hat{\\sigma}^2\\) Proportion \\(p\\) \\(P_n=\\hat{p}\\) On dit que \\(\\overline{X}_n\\) est un estimateur de \\(m\\). La valeur obtenue est une estimation de \\(m\\), quon appelle \\(\\hat{m}\\). Les estimateurs sont des variables aléatoires et les estimations sont les valeurs observées des estimateurs (i.e des variables aléatoires). Dans ce chapitre nous avons introduit des estimateurs de la moyenne, la variance et la proportion. Nous avons aussi étudié les lois de ces estimateurs. Dans le chapitre suivant, nous allons étudier la théorie de lestimation ponctuelle et la recherche dun estimateur. "],["exercices-1.html", "Exercices", " Exercices Exercice 2.1 (Loi de \\(\\overline{X}_n\\)) Une population est composée de 3 salariés A, B et C âgés respectivement de 23, 37 et 45 ans. On choisit au hasard un salarié. Définir lexpérience aléatoire \\(\\varepsilon\\), la population \\(\\Omega\\), la probabilité \\(P\\) et la variable aléatoire \\(X\\) étudiée. Calculer \\(E(X)=m\\) et \\(V(X)=\\sigma^2\\). Que représentent \\(E(X)\\) et \\(V(X)\\)? On choisit maintenant au hasard un échantillon de 2 salariés. Définir la nouvelle expérience aléatoire \\(\\varepsilon_n\\), lensemble des échantillons \\(E_n\\) et les variables aléatoires \\(X_i, i=1,\\ldots,n\\). Définir la variable aléatoire \\(\\overline{X}_n\\) et déterminer sa loi. Calculer \\(E(\\overline{X}_n)\\) et \\(V(\\overline{X}_n)\\). Retrouver les formules du cours. Exercice 2.2 (Loi de \\(P_n\\)) Une population est composée de 3 individus A, B et C dont les résultats de vote pour un certain candidat sont respectivement les suivants: Non, Non et Oui. On choisit au hasard un individu. Définir lexpérience aléatoire \\(\\varepsilon\\), la population \\(\\Omega\\), la probabilité \\(P\\) et la variable aléatoire \\(X\\) étudiée. Calculer \\(E(X)\\) et \\(V(X)\\). Que représente \\(E(X)\\)? On choisit maintenant au hasard un échantillon de 2 individus. Définir la nouvelle expérience aléatoire \\(\\varepsilon_n\\), lensemble des échantillons \\(E_n\\) et les variables aléatoires \\(X_i, i=1,\\ldots,n\\). Définir la variable aléatoire \\(P_n\\) et déterminer sa loi. Calculer \\(E(P_n)\\) et \\(V(P_n)\\). Retrouver les formules du cours. Exercice 2.3 Est ce quil sagit dune variable aléatoire? Moyenne de la population. Taille de la population. Taille de léchantillon. Moyenne de léchantillon. Variance de la moyenne de léchantillon. Plus grande valeur de léchantillon. Variance de la population. Variance estimée de la moyenne de léchantillon. Exercice 2.4 (Théorème Central Limite) Devant laugmentation des problèmes de poids dans la population européenne, une nouvelle étude est mandatée pour mesurer la relation entre celui-ci et la quantité de calories ingérées par habitant. Des études antérieures montrent quun Européen consomme en moyenne \\(2700\\) calories par jour avec un écart-type de \\(800\\). On dispose dans cette étude dun échantillon de \\(500\\) Européens. Définir les variables aléatoires étudiées dans lénoncé. On les nomme \\(X_i\\) et \\(\\overline{X}_n\\). Utiliser le TCL pour donner la distribution de la v.a. \\(\\overline{X}_n\\). Calculer la probabilité que la moyenne des calories consommées par jour par les Européens, quon a nommé \\(\\overline{X}_n\\), dans léchantillon soit supérieure à \\(2750\\). Exercice 2.5 Afin destimer leur espérance respective, on échantillonne 2 populations. On utilise un échantillon de taille \\(n_{1}\\) pour la population 1, qui présente un écart-type égal à \\(\\sigma_{1}\\). Pour la population 2, dont lécart-type vaut \\(\\sigma_{2}=2\\sigma_{1}\\), on prend un échantillon de taille \\(n_{2}=2 n_{1}\\). Pour lequel des 2 échantillons est-ce que lestimation de la moyenne de la population est la plus précise? Exercice 2.6 Dans une certaine commune française, on veut estimer la proportion de familles vivant en dessous du seuil de pauvreté. Si cette proportion est environ \\(0.15\\), quelle est la taille de léchantillon nécessaire pour que lécart-type de lestimateur soit égal à \\(0.02\\)? Extra Exercice 2.7 (Théorème Central Limite) Rémy et ses 9 amis voudraient jouer au bowling. Ils décident de rassembler leur argent de poche et espèrent obtenir la somme totale nécessaire. On peut supposer que largent de poche de chacun est une variable aléatoire \\(X_i\\) qui suit la loi exponentielle de paramètre \\(\\lambda=0.06\\). Sa densité est donc \\[ f(x)=0.06 e^{-0.06 x} \\times 1_{\\mathbb{R}^+}(x) \\] De plus, on admet que les \\(X_i\\) sont indépendantes. Démontrer que la loi exponentielle \\(\\mathcal{E}(\\lambda)\\) est un cas exceptionnel de la loi Gamma en donnant les paramètres de celle-ci. (un rappel de la définition de la loi Gamma est donnée ci dessous). Soit \\(S_{10}=\\sum_{i=1}^{10} X_{i}\\) Quelle est la loi de \\(S_{10}\\)? Sachant quune partie de bowling coûte \\(15 \\), quelle est la probabilité que Rémy et ses amis puissent jouer une partie? (Indication: Appliquer le TCL pour \\(S_{10}= \\sum_{i=1}^n X_i = n \\times \\overline{X}_n\\)) Comment faut-il choisir \\(z&gt;0\\) pour que la probabilité que la somme totale dargent du groupe soit supérieure à \\(z\\) soit égale à \\(5 \\%\\)? Loi Gamma \\(\\Gamma(a,\\lambda)\\) On dit que la variable aléatoire \\(X\\) suit une loi Gamma de paramètres \\(a&gt;0\\) et \\(\\lambda&gt;0\\), \\(X \\thicksim \\Gamma(a,\\lambda)\\) si \\(X\\) a la densité: \\[\\forall \\, x \\in \\mathbb{R}, \\quad f_{a,\\lambda} (x)= \\frac{\\lambda^a}{\\Gamma(a)} x^{a-1} e^{-\\lambda x} \\times {1}_{\\mathbb{R}_{+}}(x)\\] La fonction Gamma est définie sur \\(\\mathbb{R}_{+}^*\\) par: \\(\\Gamma(x) = \\int_0^{+\\infty} t^{x-1}e^{-t} dt\\). Propriétés de la fonction Gamma: \\(\\Gamma(x+1)=x\\Gamma(x) \\quad \\forall \\, x &gt;0\\) \\(\\Gamma(n+1) = n! \\quad \\forall n \\in \\mathbb{N}\\) \\(\\Gamma(1) = 1\\) et \\(\\Gamma(\\frac{1}{2})=\\sqrt{\\pi}\\) Propriétés de la loi Gamma: \\(E(X)= \\frac{a}{\\lambda}\\) et \\(V(X)= \\frac{a}{\\lambda^2}\\). Si \\((X_n)_{n \\in \\mathbb{N}}\\) est une suite de variables aléatoires indépendantes de lois \\(\\Gamma(a_n,\\lambda)\\) alors la variable aléatoire somme \\(\\sum_{n=1}^N X_n\\) suit également une loi gamma \\(\\Gamma(\\sum_{n=1}^N a_n, \\lambda)\\). "],["tp-illustration-numérique-des-théorèmes-limites-avec-r.html", "TP illustration numérique des théorèmes limites avec R Illustrations de théorèmes limites Bonus", " TP illustration numérique des théorèmes limites avec R Réalisations de variables aléatoires Pour générer (simuler) des réalisations de variables aléatoires on utilise la fonction qui commence par la lettre r (pour random) succédée par le nom de la loi que lon souhaite simuler. Par exemple rnorm pour simuler des réalisations de la variable alétoire de loi normale. Tapez ?rnorm pour voir la liste des paramètres (arguments) de cette fonction. rnorm(10) # génére 10 réalisations alétoires de la loi normale centrée réduite. #ans&gt; [1] 1.7111 0.1184 0.2113 0.8255 0.4184 1.8033 0.3251 0.3784 0.0252 0.0345 rnorm(10, mean = 1, sd = 3) # génére 10 réalisations alétoires de la loi normale d&#39;espérance 1 et écart-type 3 #ans&gt; [1] 4.7308 5.9451 -3.1275 0.5337 -3.8268 3.9036 1.9544 0.0549 -0.6323 #ans&gt; [10] 4.6497 1. Simuler un vecteur de 1000 réalisations indépendantes de loi uniforme sur lintervalle \\([0,1]\\). Simuler ensuite 1000 réalisations de la loi exponentielle de paramètre 2. Extra: Afin de vérifier les simulations on peut afficher lhistogramme des réalisations et superposer avec la densité de la loi correspondante. Fonction de densité Pour calculer la densité dune certaine loi, il suffit dutiliser comme fonction le nom de la loi dans avec le préfixe d pour une densité. Taper ?dnorm pour comprendre le fonctionnement de cette commande. 2. Que vaut la densité de la loi normale centrée réduite en 0? Essayer la même chose pour la loi binomiale de paramètres \\(n=10\\) et \\(p=0.5\\). Cette commande permet de tracer facilement des fonctions de densité. 3. Que se passe-t-il si lon demande la fonction de densité de la loi binomiale en 0.3? Pourquoi? 4. Tracer la fonction de densité de la loi normale centrée réduite. Commencer par créer un vecteur dabscisses à laide de la fonction seq(). Tracer ensuite la fonction de densité en ces points. Il existe plusieurs paramètres réglables pour avoir des courbes de différents design. 5. Tracer un histogramme dun vecteur de 10000 réalisations indépendantes dune loi normale centrée réduite. Superposer lhistogramme avec la densité réelle de cette loi. Fonction de répartition Pour calculer la fonction de répartition dune certaine loi on utilise le nom de la loi dans avec le préfixe p. 6. Que vaut la fonction de répartition de la loi normale centrée réduite en 0? 7. Tracer la fonction de répartition de la loi normale centrée réduite. On peut également tracer des fonctions de répartitions empiriques (calculées sur un échantillon). 8. Créer un vecteur de 100 réalisations indépendantes de la loi de votre choix. Utiliser la fonction ecdf() pour construire la fonction de répartition empirique de votre vecteur. Puis superposer avec la fonction de répartition théorique. Illustrations de théorèmes limites Loi forte de grands nombres On considère une variable \\(X\\) à valeurs dans \\(\\{0,1,3\\}\\), distribuée comme suit: \\(P(X=0)=0.5, P(X=1)=0.25, P(X=3)=0.25\\). 9. Proposer une façon de simuler \\(X\\). (Suggestion: utiliser la fonction sample()). 10. On considère \\(X_1,X_2,\\ldots\\) une suite infinie de variables indépendantes de même loi que \\(X\\). Soit \\(\\overline{X}_n\\) la moyenne empirique des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,n\\}\\). 10.1 Simuler la suite des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,10000\\}\\). 10.2 Produire un graphique représentant lévolution de \\(\\overline{X}_n\\) pour \\(n\\) variant de 1 à 10000. (Indication: penser à utiliser la fonction cumsum()). 10.3 Que constate-on? Pouvait-on sy attendre? Théorème central limite On désire maintenant approfondir comment \\(\\overline{X}_{500}\\) varie autour de sa valeur moyenne. 11. Proposer une transformation affine de \\(\\overline{X}_{500}\\), de la forme \\(Y=a \\times (\\overline{X}_{500} + b)\\), qui suive approximativement la loi \\(\\mathcal{N}(0,1)\\). 11.1 Simuler avec \\(10000\\) réalisations indépendantes de la variable \\(Y\\). Nous les noterons \\(Y_1, \\ldots, Y_{10000}\\). 11.2 Confirmer lapproximation gaussienne en réalisant un histogramme des valeurs prises par les \\(Y_j\\) pour \\(j \\in \\{1,\\ldots,10000\\}\\), sans oublier de tracer la densité gaussienne correctement renormalisée. Commenter lécart entre lhistogramme et la densité gaussienne. Bonus Illustration du théorème de Moivre Laplace Pour \\((n=10, p=0.5),\\) puis \\((n=50, p=0.9),\\) puis \\((n=100, p=0.1)\\) Simuler un échantillon \\(X\\) de la loi binomiale de paramètres \\(n\\) et \\(p .\\) Calculer léchantillon centré-réduit \\(Xcr=\\frac{x-n p}{\\sqrt{n p(1-p)}}\\). Représenter un histogramme des valeurs de \\(X c r .\\) Superposer sur le même graphique la densité de la loi normale \\(\\mathcal{N}(0,1) .\\) Représenter la fonction de répartition empirique de \\(X c r .\\) Superposer sur le même graphique la fonction de répartition de la loi normale \\(N(0,1)\\). "],["estimation-ponctuelle.html", "Chapitre 3 Estimation ponctuelle 3.1 Introduction 3.2 Méthodes destimation 3.3 La méthode des moments 3.4 La méthode du maximum de vraisemblance 3.5 Qualité dun estimateur 3.6 Propriétés des estimateurs des moments (EMM) 3.7 Propriétés des estimateurs de maximum de vraisemblance (EMV)", " Chapitre 3 Estimation ponctuelle 3.1 Introduction Dans ce chapitre, on suppose que les données \\(x_1,\\ldots,x_n\\) sont \\(n\\) réalisations indépendantes dune même variable aléatoire sous-jacente \\(X\\) (variable parente). Il est équivalent de supposer que \\(x_1,\\ldots,x_n\\) sont les réalisations de variables aléatoires \\(X_1,\\ldots,X_n\\) indépendantes et de même loi (i.i.d). Nous adopterons ici la seconde formulation, qui est plus pratique à manipuler. Les techniques de statistique descriptive, comme lhistogramme ou le graphe de probabilités, permettent de faire des hypothèses sur la nature de la loi de probabilité des \\(X_i\\). Des techniques statistiques plus sophistiquées, les tests dadéquation, permettent de valider ou pas ces hypothèses. On supposera ici que ces techniques ont permis dadopter une famille de lois de probabilité bien précise (par exemple, loi normale, loi de Poisson, etc.) pour la loi des \\(X_i\\), mais que la valeur du ou des paramètres de cette loi est inconnue. On notera \\(\\theta\\) le paramètre inconnu. Le problème traité dans ce chapitre est celui de lestimation du paramètre \\(\\theta\\). Comme on la déjà dit, il sagit de donner, au vu des observations \\(x_1,\\ldots,x_n\\), une approximation ou une évaluation de \\(\\theta\\) que lon espère la plus proche possible de la vraie valeur inconnue. On pourra proposer une unique valeur vraisemblable pour \\(\\theta\\) (estimation ponctuelle, dans ce chapitre) ou un ensemble de valeurs vraisemblables (estimation ensembliste ou région (intervalle) de confiance, dans le chapitre suivant). On notera \\(F(x;\\theta)\\) la fonction de répartition des \\(X_i\\). Pour les variables aléatoires discrètes on notera \\(P(X = x;\\theta)\\) les probabilités élémentaires, et pour les variables aléatoires continues on notera \\(f(x;\\theta)\\) la densité. Par exemple, quand \\(X\\) est de loi exponentielle \\(\\mathcal{E}(\\lambda)\\), on aura \\(F(x;\\lambda) = 1  e^{\\lambda x}\\) et \\(f(x;\\lambda) = \\lambda e^{\\lambda x}\\). Lestimation du paramètre \\(\\theta\\) sagit de donner, au vu des observations \\(x_1,\\ldots,x_n\\), une approximation ou une évaluation de \\(\\theta\\) que lon espère la plus proche possible de la vraie valeur inconnue. 3.2 Méthodes destimation Il existe de nombreuses méthodes pour estimer un paramètre \\(\\theta\\). Dans cette section, nous ne nous intéressons quaux deux méthodes destimation les plus usuelles, la méthode des moments et la méthode du maximum de vraisemblance. Mais il faut dabord définir précisément ce que sont une estimation et surtout un estimateur. Pour estimer \\(\\theta\\) on ne dispose que des données \\(x_1,\\ldots,x_n\\), donc une estimation de \\(\\theta\\) sera une fonction de ces observations. Définition 3.1 (Définition dune statistique) Une statistique \\(t\\) est une fonction des observations \\(x_1,\\ldots,x_n\\) : \\[\\begin{align} t: \\, &amp; \\mathbb{R}^n \\rightarrow \\mathbb{R}^m \\\\ &amp; (x_1,\\ldots,x_n) \\rightarrow t(x_1,\\ldots,x_n) \\end{align}\\] Par exemple, \\(\\overline{x}_n = \\frac{1}{n} \\sum_{i=1}^n x_i, \\,\\, x_1^2 \\,\\, \\text{ou} \\,\\, (x_1,x_3+x_4,2 \\ln x_6)\\) sont des statistiques. Puisque les observations \\(x_1,\\ldots,x_n\\) sont des réalisations des variables aléatoires \\(X_1,\\ldots,X_n\\), la quantité calculable à partir des observations \\(t(x_1,\\ldots,x_n)\\) est une réalisation de la variable aléatoire \\(t(X_1,\\ldots,X_n)\\). Et on retrouve par exemple le fait que \\(\\overline{x}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\) est une réalisation de \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Pour simplifier les écritures, on note souvent \\(t_n = t(x_1,\\ldots,x_n)\\) et \\(T_n = t(X_1,\\ldots,X_n)\\). Par abus, on donne le même nom de statistique aux deux quantités, mais dans une perspective destimation, on va nommer différemment \\(t_n\\) et \\(T_n\\). Définition 3.2 (Définition dun estimateur) Un estimateur dune grandeur \\(\\theta\\) est une statistique \\(T_n\\) à valeurs dans lensemble des valeurs possibles de \\(\\theta\\). Une estimation de \\(\\theta\\) est une réalisation \\(t_n\\) de lestimateur \\(T_n\\). Un estimateur est donc une variable aléatoire, alors quune estimation est une valeur déterministe. Dans lexemple des ampoules dans lintroduction (ici), lestimateur de \\(\\lambda\\) est \\(1/\\overline{X}_n\\) et lestimation de \\(\\lambda\\) est \\(0.012\\).3 Un estimateur est une variable aléatoire, alors quune estimation est une valeur déterministe. 3.3 La méthode des moments 3.3.1 Lestimateur des moments (EMM) Cest la méthode la plus naturelle. Lidée de base est destimer une espérance mathématique par une moyenne empirique, une variance par une variance empirique, etc Si le paramètre à estimer est lespérance de la loi des \\(X_i\\), alors on peut lestimer par la moyenne empirique de léchantillon. Autrement dit, si \\(\\theta = E(X)\\), alors lestimateur de \\(\\theta\\) par la méthode des moments (EMM) est \\(\\hat{\\theta}_n=\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Plus généralement, pour \\(\\theta \\in \\mathbb{R}\\), si \\(E(X) = \\phi(\\theta)\\), où \\(\\phi\\) est une fonction inversible, alors lestimateur de \\(\\theta\\) par la méthode des moments est \\(\\hat{\\theta}_n = \\phi^{-1} (\\overline{X}_n)\\). De la même manière, on estime la variance de la loi des \\(X_i\\) par la variance empirique de léchantillon \\(S_n^2= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2\\). 3.3.2 Exemples 3.3.2.1 Exemple 1: loi de Bernoulli Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi de Bernoulli \\(\\mathcal{B}(p)\\), \\(E(X) = p\\). Donc lestimateur de \\(p\\) par la méthode des moments est \\(\\hat{p}_n = \\overline{X}_n\\). Cet estimateur nest autre que la proportion de 1 dans léchantillon. On retrouve donc le principe destimation dune probabilité par une proportion (voir 2.4 et 2.6). 3.3.2.2 Exemple 2: loi exponentielle Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi exponentielle \\(\\mathcal{E}(\\lambda)\\), \\(E(X) = 1/\\lambda\\). Donc lestimateur de \\(\\lambda\\) par la méthode des moments est \\(\\hat{\\lambda}_n = 1/\\overline{X}_n\\). 3.3.2.3 Exemple 3: loi normale Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi normale \\(\\mathcal{N}(m,\\sigma^2 )\\), \\(E(X) = m\\) et \\(V(X) = \\sigma^2\\), donc les estimateurs de \\(m\\) et \\(\\sigma^2\\) par la méthode des moments sont \\(\\hat{m} = \\overline{X}_n\\) et \\(\\hat{\\sigma}^2=S_n^2\\).4 3.3.2.4 Exemple 4: loi gamma Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi gamma \\(\\Gamma(a,\\lambda)\\), \\(E(X) = a/\\lambda\\) et \\(V(X) = a/\\lambda^2\\). On en déduit facilement que : \\[ \\lambda = \\frac{E(X)}{V(X)} \\quad \\text{et} \\quad a = \\frac{[E(X)]^2}{V(X)}\\] Donc les EMM de \\(a\\) et \\(\\lambda\\) sont: \\[ \\hat{\\lambda} = \\frac{\\overline{X}_n}{S_n^2} \\quad \\text{et} \\quad \\hat{a} = \\frac{\\overline{X}_n^2}{S_n^2}\\] Lidée de base de lestimateur par la méthode des moments est destimer une espérance mathématique par une moyenne empirique, une variance par une variance empirique, etc 3.4 La méthode du maximum de vraisemblance 3.4.1 La fonction de vraisemblance Définition 3.3 Quand les observations sont toutes discrètes ou toutes continues, on appelle fonction de vraisemblance (ou plus simplement vraisemblance) pour léchantillon \\(x_1,\\ldots,x_n\\), la fonction du paramètre \\(\\theta\\) : \\[\\begin{equation*} \\mathcal{L}(\\theta; x_1,\\ldots,x_n) = \\left\\lbrace \\begin{array}{ll} P(X_1=x_1,\\ldots,X_n=x_n; \\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont discrètes}\\\\ f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n;\\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont continues} \\end{array} \\right. \\end{equation*}\\] Dans tous les exemples que nous traiterons ici, les \\(X_i\\) sont indépendantes et de même loi. Dans ce cas, la fonction de vraisemblance sécrit: \\[\\begin{equation*} \\mathcal{L}(\\theta; x_1,\\ldots,x_n) = \\left\\lbrace \\begin{array}{ll} \\displaystyle \\prod_{i=1}^n P(X_i=x_i; \\theta) = \\prod_{i=1}^n P(X=x_i; \\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont discrètes}\\\\ \\displaystyle \\prod_{i=1}^n f_{X_i}(x_i;\\theta) = \\prod_{i=1}^n f(x_i;\\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont continues} \\end{array} \\right. \\end{equation*}\\] Remarque: La probabilité et la densité utilisées dans cette définition sont des fonctions des observations \\(x_1,\\ldots,x_n\\), dépendant du paramètre \\(\\theta\\). A linverse, la fonction de vraisemblance est considérée comme une fonction de \\(\\theta\\) dépendant des observations \\(x_1,\\ldots,x_n\\), ce qui permet, par exemple, de dériver cette fonction par rapport à \\(\\theta\\). 3.4.1.1 Exemple introductif Dans cet exemple, \\(n = 1\\). On considère que lon sait que \\(X_1\\) est de loi binomiale \\(\\mathcal{B}(15,p)\\), avec \\(p\\) inconnu. On observe \\(x_1 = 5\\) et on cherche à estimer \\(p\\). La fonction de vraisemblance est : \\[\\mathcal{L}(p;5) = P(X_1 = 5;p) = C_{15}^5 p^5 (1-p)^{15-5}\\] Cest la probabilité davoir observé un 5 quand la valeur du paramètre est \\(p\\). Calculons-là pour quelques valeurs de \\(p\\). \\(p\\) \\(0.1\\) \\(0.2\\) \\(0.3\\) \\(0.4\\) \\(0.5\\) \\(0.6\\) \\(0.7\\) \\(0.8\\) \\(0.9\\) \\(\\mathcal{L}(p;5)\\) \\(0.01\\) \\(0.10\\) \\(0.21\\) \\(0.19\\) \\(0.09\\) \\(0.02\\) \\(0.003\\) \\(10^{-4}\\) \\(210^{-7}\\) On tire de cette table que quand \\(p = 0.8\\), cest-à-dire quand \\(X_1\\) est de loi \\(\\mathcal{B}(15,0.8)\\), il ny a quune chance sur \\(10000\\) dobserver \\(x_1 = 5\\). En revanche, il y a \\(21\\%\\) de chances dobserver un \\(5\\) quand \\(p = 0.3\\). Il est donc beaucoup plus vraisemblable que \\(p\\) soit égal à \\(0.3\\) plutôt quà \\(0.8\\). En suivant ce raisonnement, on aboutit à dire que la valeur la plus vraisemblable de \\(p\\) est celle pour laquelle la probabilité dobserver un \\(5\\) est maximale. Cest donc la valeur de \\(p\\) qui maximise la fonction de vraisemblance. Pour la calculer, on peut annuler la dérivée de la vraisemblance (en fonction de \\(p\\)). Mais on remarque que la vraisemblance est un produit. Comme il est plus commode de maximiser (ou de dériver) une somme quun produit, on utilise le fait que la valeur qui rend maximale une fonction rend aussi maximal son logarithme. On va donc plutôt maximiser le logarithme de la fonction de vraisemblance, quon appelle la log-vraisemblance. Pour notre exemple, la log-vraisemblance vaut: \\[\\ln \\mathcal{L}(p;x_1)=\\ln C_{15}^{x_1} + x_1 \\ln p + (15-x_1) \\ln (1-p)\\] Sa dérivée est: \\[ \\frac{\\partial }{\\partial p } \\ln \\mathcal{L}(p;x_1)= \\frac{x_1}{p} - \\frac{15-x_1}{1-p} = \\frac{x_1 - 15 p}{p(1-p)} \\] qui sannule pour \\(p = \\frac{x_1}{15} = \\frac{5}{15} = \\frac{1}{3}\\). Donc la valeur la plus vraisemblable de \\(p\\) est \\(\\frac{1}{3}\\). La vraisemblance maximale est \\(\\mathcal{L}(\\frac{1}{3};5) = 21.4\\%\\). La valeur qui rend maximale une fonction rend aussi maximal son logarithme. 3.4.2 Lestimateur de maximum de vraisemblance (EMV) En suivant le raisonnement précédent, pour \\(n\\) quelconque, il est logique de dire que la valeur la plus vraisemblable de \\(\\theta\\) est la valeur pour laquelle la probabilité dobserver \\(x_1 ,\\ldots,x_n\\) est la plus forte possible. Cela revient à faire comme si cétait léventualité la plus probable qui sétait produite au cours de lexpérience. Définition 3.4 Lestimation de maximum de vraisemblance de \\(\\theta\\) est la valeur \\(\\hat{\\theta}_n\\) de \\(\\theta\\) qui rend maximale la fonction de vraisemblance \\(\\mathcal{L}(\\theta;x_1 ,\\ldots,x_n)\\). Lestimateur de maximum de vraisemblance (EMV) de \\(\\theta\\) est la variable aléatoire correspondante. Comme dans lexemple, dans la plupart des cas, la fonction de vraisemblance sexprime comme un produit. Donc \\(\\hat{\\theta}_n\\) sera en général calculé en maximisant la log-vraisemblance: \\[ \\hat{\\theta}_{n}=\\arg \\max _{\\theta} \\,\\, \\ln \\mathcal{L}\\left(\\theta ; x_{1}, \\ldots, x_{n}\\right) \\] Quand \\(\\theta = (\\theta_1 ,\\ldots,\\theta_d ) \\in \\mathbb{R}^d\\) et que toutes les dérivées partielles ci-dessous existent, \\(\\hat{\\theta}_{n}\\) est solution du système déquations appelées équations de vraisemblance: \\[ \\forall j \\in\\{1, \\ldots, d\\}, \\quad \\frac{\\partial}{\\partial \\theta_{j}} \\ln \\mathcal{L}\\left(\\theta ; x_{1}, \\ldots, x_{n}\\right)=0 \\] A priori, une solution de ce système déquations pourrait être un minimum de la vraisemblance. Mais on peut montrer que la nature dune fonction de vraisemblance fait que cest bien un maximum que lon obtient. Il est fréquent que le système des équations de vraisemblance nait pas de solution explicite. Dans ce cas, on le résoud par des méthodes numériques, comme la méthode de Newton-Raphson (lien 1 , lien 2 ). En , la maximisation numérique peut se faire à laide de la commande optim(). 3.4.2.1 Exemples 3.4.2.1.1 Exemple 1: loi de Bernoulli Soit les \\(X_i\\), sont de loi \\(\\mathcal{B}(p)\\), on a: \\[P\\left(X_{i}=x_{i} ; p\\right)=\\left\\{\\begin{array}{cc}{p} &amp; {\\text { si } x_{i}=1} \\\\ {1-p} &amp; {\\text { si } x_{i}=0}\\end{array}\\quad \\right\\} = p^{x_{i}}(1-p)^{1-x_{i}}\\] Donc la fonction de vraisemblance est: \\[\\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} P\\left(X_{i}=x_{i} ; p\\right)=\\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}=p^{\\sum_{i=1}^{n} x_{i}}(1-p)^{\\sum_{i=1}^{n}\\left(1-x_{i}\\right)}\\] Doù \\(\\ln \\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\left(\\sum_{i=1}^{n} x_{i}\\right) \\ln p+\\left(n-\\sum_{i=1}^{n} x_{i}\\right) \\ln (1-p)\\). Alors \\[\\frac{\\partial}{\\partial p} \\ln \\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\frac{\\sum_{i=1}^{n} x_{i}}{p}-\\frac{n-\\sum_{i=1}^{n} x_{i}}{1-p}=\\frac{\\sum_{i=1}^{n} x_{i}-n p}{p(1-p)}\\] qui sannule pour \\(p=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}=\\overline{x}_{n}\\). Par conséquent, lEMV de \\(p\\) est \\(\\hat{p}_n= \\overline{X}_n\\). LEMV de \\(p\\) est \\(\\hat{p}_n= \\overline{X}_n\\). Le même que lEMM de \\(p\\). 3.4.2.1.2 Exemple 2: loi exponentielle Si les \\(X_i\\) sont de loi \\(\\mathcal{E}(\\lambda)\\), la fonction de vraisemblance est: \\[\\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} f_{X_{i}}\\left(x_{i} ; \\lambda\\right)=\\prod_{i=1}^{n} \\lambda e^{-\\lambda x_{i}}=\\lambda^{n} e^{-\\lambda \\sum_{i=1}^{n} x_{i}}\\] Doù \\(\\ln \\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=n \\ln \\lambda-\\lambda \\sum_{i=1}^{n} x_{i}\\). Alors \\(\\frac{\\partial}{\\partial \\lambda} \\ln \\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\frac{n}{\\lambda}-\\sum_{i=1}^{n} x_{i}\\), qui sannule pour \\(\\lambda=\\frac{n}{\\sum_{i=1}^{n} x_{i}}=\\frac{1}{\\overline{x}_{n}}\\). Par conséquent, lEMV de \\(\\lambda\\) est \\(\\hat{\\lambda}_n = \\frac{1}{\\overline{X}_n}\\). 3.4.2.1.3 Exemple 3: loi normale Si les \\(X_i\\) sont de loi \\(\\mathcal{N}(m,\\sigma^2)\\), la fonction de vraisemblance est: \\[\\begin{aligned} \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right) &amp;=\\prod_{i=1}^{n} f_{X_{i}}\\left(x_{i} ; m, \\sigma^{2}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(x_{i}-m\\right)^{2}}{2 \\sigma^{2}}} \\\\ &amp;=\\frac{1}{(\\sigma \\sqrt{2 \\pi})^{n}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}} \\end{aligned}\\] Doù \\(\\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{n}{2} \\ln \\sigma^{2}-\\frac{n}{2} \\ln 2 \\pi-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\). On doit annuler les dérivées partielles de ce logarithme par rapport à \\(m\\) et \\(\\sigma^2\\). On a: \\(\\frac{\\partial}{\\partial m} \\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}-2\\left(x_{i}-m\\right)=\\frac{1}{\\sigma^{2}}\\left(\\sum_{i=1}^{n} x_{i}-n m\\right)\\), qui sannule pour \\(m=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}=\\overline{x}_{n}\\). \\(\\frac{\\partial}{\\partial \\sigma^{2}} \\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\), qui sannule pour \\(\\sigma^2=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\). \\(\\hat{m}_n\\) et \\(\\hat{\\sigma}_n^2\\) sont les valeurs de \\(m\\) et \\(\\sigma^2\\) qui vérifient les deux conditions en même temps. On a donc \\(\\hat{m}_{n}=\\overline{X}_{n}\\) et \\(\\hat{\\sigma}_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=S_{n}^{2}\\). Remarque: Dans ces les trois exemples, la méthode des moments et la méthode du maximum de vraisemblance donnent les mêmes résultats. Ce nest le cas que pour quelques lois de probabilité parmi les plus élémentaires. En fait, dans la plupart des cas, les deux méthodes fournissent des estimateurs différents. Cest le cas de la loi gamma. Cela amène à se poser la question de la qualité et de loptimalité dun estimateur, ce qui fait lobjet de la section suivante. 3.5 Qualité dun estimateur En toute généralité, \\(\\theta\\) peut être un paramètre à plusieurs dimensions, mais on supposera dans toute cette section et dans la suivante que \\(\\theta\\) est un réel. Cela signifie par exemple que, quand \\(X\\) est de loi normale \\(\\mathcal{N}(m,\\sigma^2)\\), on sintéressera séparément à la qualité des estimateurs de \\(m\\) et de \\(\\sigma^2\\). Les estimateurs \\(T_n\\) considérés ici seront donc des variables aléatoires réelles. Pour \\(\\theta \\in \\mathbb{R}^d, d \\geq 2\\), toutes les notions de ces sections sont généralisables, mais la complexité des résultats augmente notablement. Par exemple, la notion de variance est remplacée par celle de matrice de covariance. 3.5.1 Estimateur sans biais et de variance minimale (ESBVM) Un estimateur \\(T_n\\) de \\(\\theta\\) sera un bon estimateur sil est suffisamment proche, en un certain sens, de \\(\\theta\\). Il faut donc définir une mesure de lécart entre \\(\\theta\\) et \\(T_n\\). On appelle cette mesure le risque de lestimateur. On a intérêt à ce que le risque dun estimateur soit le plus petit possible. Par exemple, les risques \\(T_n - \\theta,\\,\\, |T_n - \\theta|\\,\\, \\text{et} \\,\\, (T_n - \\theta)^2\\) expriment bien un écart entre \\(T_n\\) et \\(\\theta\\). Mais comme il est plus facile dutiliser des quantités déterministes que des quantités aléatoires, on sintéresse en priorité aux espérances des quantités précédentes. En particulier: Définition 3.5 (biais) On appelle le biais de \\(T_n\\) la quantité \\(E(T_n)-\\theta\\). Définition 3.6 (risque quadratique) On appelle le risque quadratique ou erreur quadratique moyenne: \\[ EQM(T_n)=E[(T_n-\\theta)^2]\\] Dans le cas du biais, le risque peut être nul: Définition 3.7 (estimateur sans biais) Un estimateur \\(T_n\\) de \\(\\theta\\) est sans biais si et seulement si \\(E(T_n) = \\theta\\). Il est biaisé si et seulement si \\(E(T_n) \\neq \\theta\\). Le biais mesure une erreur systématique destimation de \\(\\theta\\) par \\(T_n\\). Par exemple, si \\(E(T_n)-\\theta &lt; 0\\), cela signifie que \\(T_n\\) aura tendance à sous-estimer \\(\\theta\\). Dautre part, lerreur quadratique moyenne (nommée aussi risque quadratique) sécrit: \\[\\begin{aligned} E Q M\\left(T_{n}\\right) &amp; = E\\left[\\left(T_{n}-\\theta\\right)^{2}\\right]=E\\left[\\left(T_{n}-E\\left(T_{n}\\right)+E\\left(T_{n}\\right)-\\theta\\right)^{2}\\right] \\\\ &amp; = E\\left[\\left(T_{n}-E\\left(T_{n}\\right)\\right)^{2}\\right]+2 E\\left[T_{n}-E\\left(T_{n}\\right)\\right] E\\left[E\\left(T_{n}\\right)-\\theta\\right]+E\\left[\\left(E\\left(T_{n}\\right)-\\theta\\right)^{2}\\right] \\\\ &amp; = \\operatorname{Var}\\left(T_{n}\\right)+\\left[E\\left(T_{n}\\right)-\\theta\\right]^{2} \\\\ &amp; = \\text { Variance de l&#39;estimateur }+\\text { carré de son biais } \\end{aligned}\\] Si \\(T_n\\) est un estimateur sans biais, \\(EQM(T_n ) = Var(T_n )\\). On a donc intérêt à ce quun estimateur soit sans biais et de faible variance. Par ailleurs, on en déduit immédiatement que de deux estimateurs sans biais, le meilleur est celui qui a la plus petite variance. On a intérêt à ce quun estimateur soit sans biais et de faible variance. La variance dun estimateur mesure sa variabilité. Si lestimateur est sans biais, cette variabilité est autour de \\(\\theta\\). Si on veut estimer correctement \\(\\theta\\), il ne faut pas que cette variabilité soit trop forte. En pratique, si on observe plusieurs jeux de données similaires, on obtient une estimation de \\(\\theta\\) pour chacun dentre eux. Alors si lestimateur est de faible variance, ces estimations seront toutes proches les unes des autres, et sil est sans biais leur moyenne sera très proche de \\(\\theta\\). Il est logique de sattendre à ce que, plus la taille des données augmente, plus on a dinformation sur le phénomène aléatoire observé, meilleure sera lestimation. En théorie, avec une observation infinie, on devrait pouvoir estimer \\(\\theta\\) sans aucune erreur. On peut traduire cette affirmation par le fait que le risque de lestimateur \\(T_n\\) doit tendre vers \\(0\\) quand la taille \\(n\\) de léchantillon tend vers linfini. Cela revient à dire que lestimateur \\(T_n\\) doit converger, en un certain sens, vers \\(\\theta\\). Il sagit en fait détudier la convergence de la suite de variables aléatoires \\(\\{T_n\\}_{n \\geq 1}\\) vers la constante \\(\\theta\\). On sait quil existe plusieurs types de convergence de suites de variables aléatoires. On peut étudier la convergence presque sûre ou la convergence en probabilité, mais on sintéresse en général à la convergence en moyenne quadratique (ou convergence dans \\(L^2\\)). Définition 3.8 Lestimateur \\(T_n\\) converge en moyenne quadratique vers \\(\\theta\\) si et seulement si son erreur quadratique moyenne tend vers \\(0\\) quand \\(n\\) tend vers linfini: \\[T_{n} \\stackrel{M Q}{\\longrightarrow} \\theta \\Leftrightarrow \\lim _{n \\rightarrow \\infty} E\\left[\\left(T_{n}-\\theta\\right)^{2}\\right]=0\\] Si \\(T_n\\) est sans biais, il sera convergent en moyenne quadratique si et seulement si sa variance tend vers \\(0\\) quand \\(n\\) tend vers linfini. Finalement, on considèrera que le meilleur estimateur possible de \\(\\theta\\) est un estimateur sans biais et de variance minimale (ESBVM). Un tel estimateur nexiste pas forcément. 3.6 Propriétés des estimateurs des moments (EMM) Propriétés de \\(\\overline{X}_n\\) Si \\(\\theta = E(X)\\), alors lEMM de \\(\\theta\\) est \\(\\hat{\\theta}_n = \\overline{X}_n\\). La justification de cette méthode est la loi des grands nombres, qui dit que \\(\\overline{X}_n\\) converge presque sûrement vers \\(E(X)\\). Donc, si \\(\\theta = E(X)\\), \\(\\overline{X}_n\\) est un estimateur de \\(\\theta\\) convergent presque sûrement. Autrement dit, si on a beaucoup dobservations, on peut estimer une espérance par une moyenne empirique. On peut en fait montrer facilement que \\(\\overline{X}_n\\) est un bon estimateur de \\(\\theta = E(X)\\), sans utiliser la loi des grands nombres: \\[ E\\left(\\overline{X}_{n}\\right)=E\\big[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\big]=\\frac{1}{n} \\sum_{i=1}^{n} E\\left(X_{i}\\right)=\\frac{1}{n} n \\theta=\\theta \\] Donc \\(\\overline{X}_{n}\\) est un estimateur sans biais de \\(\\theta = E(X)\\). La variance de \\(\\overline{X}_{n}\\) est: \\[ \\operatorname{Var}\\left(\\overline{X}_{n}\\right)=\\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}\\left(X_{i}\\right)=\\frac{\\operatorname{Var}(X)}{n} \\] car les \\(X_i\\) sont indépendantes, donc la variance de leur somme est égale à la somme de leurs variances, qui sont toutes égales à \\(Var(X)\\). \\(Var(\\overline{X}_{n})\\) tend vers \\(0\\) quand \\(n\\) tend vers linfini. Par conséquent: Propriété: La moyenne empirique \\(\\overline{X}_{n}\\) est un estimateur sans biais et convergent en moyenne quadratique de \\(E(X)\\). Propriétés de la variance empirique \\(S_{n}^{2}\\) On considère maintenant lestimation de la variance de la loi des \\(X_i\\) par la variance empirique de léchantillon \\(S_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}-\\overline{X}_{n}^{2}\\). Déterminons le biais de cet estimateur. \\[ \\begin{aligned} E\\left(S_{n}^{2}\\right) &amp;=E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}-\\overline{X}_{n}^{2}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} E\\left(X_{i}^{2}\\right)-E\\left(\\overline{X}_{n}^{2}\\right)=E\\left(X^{2}\\right)-E\\left(\\overline{X}_{n}^{2}\\right) \\\\ &amp;=\\operatorname{Var}(X)+E(X)^{2}-\\operatorname{Var}\\left(\\overline{X}_{n}\\right)-E\\left(\\overline{X}_{n}\\right)^{2} \\\\ &amp;=\\operatorname{Var}(X)+E(X)^{2}-\\frac{\\operatorname{Var}(X)}{n}-E(X)^{2}=\\left(1-\\frac{1}{n}\\right) \\operatorname{Var}(X) \\\\ &amp;=\\frac{n-1}{n} \\operatorname{Var}(X) \\neq \\operatorname{Var}(X) \\end{aligned} \\] Donc contrairement à ce quon pourrait croire, la variance empirique \\(S_n^2\\) nest pas un estimateur sans biais de \\(Var(X)\\). Cet estimateur nest quasymptotiquement sans biais. En revanche, on voit que \\(E\\left(\\frac{n}{n-1} S_{n}^{2}\\right)=\\frac{n}{n-1} E\\left(S_{n}^{2}\\right)=\\operatorname{Var}(X)\\). On pose donc \\({S_n^{*}}^2=\\frac{n}{n-1} S_{n}^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\big(X_{i}-\\overline{X}_{n}\\big)^{2}\\). \\({S_n^{*}}^2\\) est appelée variance estimée de léchantillon. Le résultat précédent montre que cest un estimateur sans biais de \\(\\operatorname{Var}(X)\\). Par ailleurs, on montre que \\[ \\operatorname{Var}\\left({S_n^{*}}^2\\right)=\\frac{1}{n(n-1)}\\left[(n-1) E\\left[(X-E(X))^{4}\\right]-(n-3) \\operatorname{Var}(X)^{2}\\right] \\] qui tend vers \\(0\\) quand \\(n\\) tend vers linfini. Par conséquent: Propriété: La variance estimée \\({S_n^{*}}^2=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}\\) est un estimateur sans biais et convergent en moyenne quadratique de \\(Var(X)\\). La commande var(x) en donne la variance estimée, et non pas la variance empirique de léchantillon x. On peut montrer également que \\({S_n^{*}}^2\\) et \\(S_{n}^{2}\\) convergent toutes les deux presque sûrement vers \\(Var(X)\\). Remarque 1: On na pas de résultat général sur la qualité de \\(S_{n}\\) comme estimateur de lécart-type de la loi, \\(\\sigma(X)=\\sqrt{\\operatorname{Var}(X)}\\). A priori, ni \\(S_{n}\\) ni \\(S_{n}^{*}\\) ne sont des estimateurs sans biais de \\(\\sigma(X)\\). Remarque 2: Le simple exemple de la variance montre quun estimateur des moments nest pas forcément sans biais. On peut montrer quun EMM est asymptotiquement sans biais et convergent presque sûrement. 3.7 Propriétés des estimateurs de maximum de vraisemblance (EMV) Un estimateur de maximum de vraisemblance nest pas forcément unique (la vraisemblance peut avoir plusieurs maxima), ni sans biais, ni de variance minimale, ni efficace. Mais il possède dexcellentes propriétés asymptotiques (non évoqués dans ce cours). Le fait que lEMV soit asymptotiquement sans biais et efficace fait que, si on a beaucoup de données, on est pratiquement certains que la méthode du maximum de vraisemblance est la meilleure méthode destimation possible. Cest pourquoi cette méthode est considérée comme globalement la meilleure et est utilisée de préference à toute autre méthode, y compris celle des moments. car \\(E(X)=1/\\lambda\\) si \\(X \\thicksim \\mathcal{E}(\\lambda)\\) \\(S_n^2\\) est la variance empirique de léchantillon \\(S_n^2= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2\\) "],["exercices-2.html", "Exercices", " Exercices Exercice 3.1 (La variance corrigée) Soit \\(X\\) une variable aléatoire ayant une espérance \\(m\\) et une variance \\(\\sigma^2\\), sa variance empirique est \\(s_{n}^2=\\frac{1}{n} \\sum X_{i}^{2}-\\overline{X}_{n}^{2}\\) avec \\(\\overline{X}_{n}\\) la moyenne empirique de \\(X\\) et \\(\\frac{1}{n} \\sum X_{i}^{2}\\) la moyenne empirique de \\(X^{2}\\). Calculer \\(E\\left(\\overline{X}_{n}\\right)\\) et \\(V\\left(\\overline{X}_{n}\\right)\\) et en déduire \\(E(\\overline{X}_{n}^{2})\\). Montrer enfin que \\(E\\left(s_{n}^2\\right)=\\frac{n-1}{n} V(X)\\) et en déduire un estimateur sans biais de la variance (on nomme cet estimateur \\(s_{n}^{*^2}\\)). Exercice 3.2 (EMM) On considère léchantillon statistique \\[1,0,2,1,1,0,1,0,0\\] Cacluler sa moyenne et sa variance empirique. En supposant que les données de cet échantillon sont des réalisations dune variable de loi inconnue, donner une estimation non biaisée de lespérance et de la variance de cette loi. On choisit de modéliser les valeurs de cet échantillon par une loi binomiale \\(\\mathcal{B}(2, p)\\). Utiliser la moyenne empirique pour proposer une estimation ponctuelle pour \\(p\\). Avec le même modèle, utiliser la variance pour proposer une autre estimation de \\(p\\). On choisit de modéliser les valeurs de cet échantillon par une loi de Poisson \\(\\mathcal{P}(\\lambda)\\), qui a pour espérance \\(\\lambda\\). Quelle estimation ponctuelle proposez-vous pour \\(\\lambda\\)? Exercice 3.3 (EMV loi normale) Considérons un échantillon aléatoire \\((X_1,\\ldots,X_n)\\) (les \\(X_i\\) sont iid) et issu dune variable aléatoire parente \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\). Calculer les estimateurs de maximum de vraisemblance (EMV) de \\(\\mu\\) et \\(\\sigma^2\\). Ces estimateurs sont-ils sans biais? Exercice 3.4 (EMV loi géométrique) Les oiseaux dun certain type prennent leur envol après avoir effectué quelques sauts sur le sol. On suppose que ce nombre \\(X\\) de sauts peut être modélisé par une distribution géométrique sur \\(\\mathbb{N}^*\\): \\[P(X=x)= p(1-p)^{x-1} \\quad x \\geq 1\\] Pour \\(n=130\\) oiseaux de ce type, on a relevé les données suivantes: Nombre de sauts \\(x\\) 1 2 3 4 5 6 7 8 9 10 11 12 Effectifs 48 31 20 9 6 5 4 2 1 1 2 1 Quel est lestimateur du maximum de vraisemblance de \\(p\\)? Calculer la valeur de cet estimateur avec les données de léchantillon. Exercice 3.5 (Comparaison destimateurs) Soit \\(X\\) une VAR de loi uniforme sur un intervalle \\([0, a]\\) où \\(a\\) est un paramètre inconnu, et on dispose de \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) un \\(n\\) -echantillon de \\(X\\). On note \\(\\overline{X}_{n}\\) la moyenne empirique de \\(X\\). Soit \\(T_{n}=2 \\overline{X}_{n}\\), lestimateur par la méthode de moments de \\(a\\). Montrer que \\(T_{n}\\) est un estimateur sans biais de \\(a\\) et calculer son risque quadratique. Soit \\(T_{n}^{\\prime}=\\max \\left(X_{1}, \\ldots, X_{n}\\right)\\). Montrer que \\(T_n^{\\prime}\\) est lestimateur par maximum de vraisemblance de \\(a\\). Donner la fonction de répartition de \\(T_{n}^{\\prime}\\). En déduire une densité de \\(T_{n}^{\\prime}\\), puis son biais et son risque quadratique. Soit \\(T_{n}^{\\prime \\prime}=\\frac{n+1}{n} T_{n}^{\\prime}\\). Déterminer son biais et son risque quadratique. Pour de grandes valeurs de \\(n\\), quel est le meilleur estimateur de \\(a\\)? Extra Exercice 3.6 On considère une variable aléatoire \\(X\\) de densité \\(f_{\\theta}\\) avec \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) et \\[ f_{\\theta}(x)=\\left\\{\\begin{array}{cl}{\\frac{1}{2}-\\theta} &amp; {\\text { si } x \\in[-1,0]} \\\\ {\\frac{1}{2}+\\theta} &amp; {\\text { si } x \\in[0,1]} \\\\ {0} &amp; {\\text { sinon }}\\end{array}\\right. \\] Représenter \\(f_{\\theta}\\) et justifier que \\(f_{\\theta}\\) est bien une densité de probabilité pour \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\). Calculer lespérance et la variance de \\(X\\). On considère maintenant \\(X_{1}, \\ldots X_{n}\\) des variables aléatoires indépendantes et de même loi. On suppose que la loi commune est de densité \\(f_{\\theta}\\) avec \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) inconnu. On va chercher à estimer \\(\\theta\\). Proposer un esitmateur \\(\\hat{\\theta}\\) de \\(\\theta\\) basé sur la méthode des moments. On prend \\(\\hat{\\theta}_{n}=\\frac{1}{n} \\sum X_{i}\\). Calculer son biais et son risque quadratique. On va maintenant sintéresser à lestimateur du maximum de vraisemblance de \\(\\theta\\). On note \\(N_{1}=\\sum_{i=1}^{n} \\mathbf{1}_{\\left\\{X_{i} \\geq 0\\right\\}}\\) et \\(N_{2}=\\sum_{i=1}^{n} \\mathbf{1}_{\\left\\{X_{i}&lt;0\\right\\}}\\). Soient \\(x_{1}, \\ldots, x_{n} \\in [-1,1]\\) et \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) écrire la vraisemblance du modèle au point \\(\\left(x_{1}, \\ldots, x_{n} ; \\theta\\right)\\) en fonction de \\(N_{1}, N_{2}\\) et \\(\\theta\\). Montrer que lestimateur du maximum devraisemblance existe et vaut: \\[ \\hat{\\theta}_{M V}=\\frac{N_{1}-N_{2}}{2\\left(N_{1}+N_{2}\\right)}=\\frac{N_{1}}{n}-\\frac{1}{2} \\] On pose \\(1_{\\left\\{X_{i} \\geq 0\\right\\}}\\), pour \\(1 \\leq i \\leq n\\). Calculer lespérance et la variance des variables aléatoires \\(Y_{i}\\). En déduire lespérance et le risque quadratique de \\(\\hat{\\theta}_{M V}\\). Quel estimateur vaut-il mieux utiliser entre \\(\\hat{\\theta}\\) et \\(\\hat{\\theta}_{M V}\\)? Justifier. "],["tp-estimation.html", "TP estimation 3.8 Comparaison destimateurs 3.9 Le maximum de vraisemblance 3.10 Loi de Weibull", " TP estimation Il faut finir les TPs de la 1ère et 2ème séances avant de commencer ce TP. 3.8 Comparaison destimateurs Soit \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) un échantillon de la loi uniforme sur \\([0, \\theta]\\) où \\(\\theta\\) est un paramètre inconnu. On considère les estimateurs convergents (on dit aussi consistants) suivants du paramètre \\(\\theta\\). \\[ \\begin{aligned} T_{1}=&amp; \\frac{2}{n} \\times\\left(X_{1}+\\ldots+X_{n}\\right) \\\\ T_{2}=&amp; \\sqrt{\\frac{3}{n} \\times\\left(X_{1}^{2}+\\ldots+X_{n}^{2}\\right)} \\\\ T_{3}=&amp;\\left(\\frac{4}{n} \\times\\left(X_{1}^{3}+\\ldots+X_{n}^{3}\\right)\\right)^{\\frac{1}{3}} \\\\ T_{4}=&amp;\\left(\\frac{3}{2 n} \\times(\\sqrt{X_{1}}+\\ldots+\\sqrt{X_{n}})\\right)^{2} \\\\ T_{5}=&amp;\\left(\\frac{1}{2 n} \\times\\left(\\frac{1}{\\sqrt{X_{1}}}+\\ldots+\\frac{1}{\\sqrt{X_{n}}}\\right)\\right)^{-2} \\\\ T_{6}=&amp; \\exp (1) \\times\\left(X_{1} \\times \\ldots \\times X_{n}\\right)^{\\frac{1}{n}} \\\\ T_{7}=&amp; \\max \\left\\{X_{1}, \\ldots, X_{n}\\right\\} \\\\ T_{8}=&amp; \\frac{n+1}{n} \\max \\left\\{X_{1}, \\ldots, X_{n}\\right\\} \\end{aligned} \\] \\(T1\\) est lemm de \\(\\theta\\). \\(T7\\) est lemv de \\(\\theta\\) (biaisé). \\(T8\\) est lemv corrigé de \\(\\theta\\). 1. Choisir une valeur de \\(\\theta\\) et simuler 1000 échantillons de taille 100 de la loi uniforme sur \\([0, \\theta]\\). Calculer pour chacun de ces échantillons la valeur prise par les 8 estimateurs. On pourra ensuite créer une matrice ayant 1000 lignes et 8 colonnes dont la jème colonne contient les 1000 réalisations de lestimateur \\(T_{j}\\): T &lt;- cbind(T1,T2,T3,T4,T5,T6,T7,T8) 2. Calculer la moyenne empirique et la variance empirique des 8 échantillons de taille 1000 ainsi obtenus. En déduire une estimation du biais et de lerreur quadratique de chacun des 8 estimateurs. On rappelle que pour un estimateur \\(T\\), le biais est \\(E[T]-\\theta\\) et lerreur quadratique est \\(E\\left[(T-\\theta)^{2}\\right]\\). 3. Quels estimateus sont les moins biaisés? Et les estimateurs de risque quadratique minimale? 4. Représenter sur un même graphique les boites à moustaches (boxplots) des 8 estimateurs. Superposer sur le même graphique la vraie valeur du paramètre, en rouge. Commenter la pertinence de chacun des estimateurs: lequel préfereriez-vous utiliser? On pourra utiliser boxplot(data.frame(T)) abline(h=theta,col=&quot;red&quot;) 3.9 Le maximum de vraisemblance Pour cette partie, imaginons quon a une série de valeurs, ça peut par exemple être lâge de 1000 étudiants pris au hasard dans une ville. Nous avons tracé lhistogramme de léchantillon et obtenu le suivant: On peut voir ici que la distribution des valeurs suit approximativement une loi normale avec une moyenne aux alentours de 22 et un écart-type difficile à évaluer au premier coup doeil. En effet, pour obtenir les données de cet échantillon, jai généré un échantillon suivant la loi normale de moyenne 22 et décart-type mystère que vous devez découvrir en utilisant la méthode du maximum de vraisemblance. Les données ont été générée avec la commande suivante: data = rnorm(1000, mean = 22, sd = mystere) # où j&#39;ai caché ici la valeur mystère de l&#39;écart-type utilisé. Les données de léchantillon obtenu se trouvent dans le fichier mystere.txt. Pour lire les données de léchantillon du fichier mystere.txt, vous pouvez les copier et les saisir dans un vecteur en utilisant la fonction c() ou bien utiliser la fonction scan(). Par exemple: donnees &lt;- scan(\"mystere.txt\", sep=\",\") Sur lintervalle \\([1,4]\\), chercher la valeur de \\(\\sigma\\) qui maximise la vraisemblance. On la notera \\(\\hat{\\sigma}\\). On procédera de façon empirique en représentant le graphe de la Log-vraisemblance sur lintervalle \\([1,4]\\). On se contentera dune valeur approchée de \\(\\hat{\\sigma}\\) à \\(10^{1}\\) près. Quelle estimation de \\(\\sigma\\) par maximum de vraisemblance proposez-vous? Re-tracer ensuite lhistogramme montré ci dessus en superposant la densité de la loi normale de moyenne 22 et décart-type estimé. 3.10 Loi de Weibull En fiabilité, la durée de vie \\(X\\) dun composant électronique est souvent modélisée par une loi de Weibull de paramètre \\(\\theta&gt;0\\). Une variable aleatoire \\(X\\) distribuée selon une loi de Weibull de paramètre \\(\\theta\\) a pour densité: \\[\\begin{equation} f(x)=\\theta x^{\\theta-1} \\exp (-x^{\\theta}) \\, \\times \\, 1_{\\mathbb{R}^+} \\tag{3.1} \\end{equation}\\] A noter que quand \\(\\theta=1\\) la loi de Weibull coincide avec la loi Exponentielle de paramètre égal à \\(1\\). Linterprétation du paramètre \\(\\theta\\) est la suivante. Quand \\(\\theta&gt;1\\) cela signifie que la propension à tomber en panne à linstant \\(t\\) augmente avec \\(t\\), quand \\(\\theta&lt;1\\), cest linverse. Enfin, quand \\(\\theta=1\\) la propension à tomber en panne à linstant \\(t\\) ne dépent pas de \\(t\\) (la loi exponentielle est sans mémoire). On entend ici par panne toute défaillance du composant électronique rendant celui-ci hors dusage. 1. Représenter le graphe de la densité dune loi de Weibull quand \\(\\theta = 1/2, 1, 2,\\text{et } 3\\). On se limitera à lintervalle \\([0,5]\\) pour \\(x\\). Pour cette question, vous devez créer la fonction dweibull qui calcule la densité comme définie dans léquation (3.1). Pour estimer \\(\\theta\\) on dispose de la durée de vie de \\(n=1000\\) composants. Les données (en milliers dheures) se trouvent dans le fichier weibull.txt. 2. Chercher la valeur de \\(\\theta\\) qui maximise la vraisemblance. (\\(\\theta \\in [0,5]\\)). On la notera \\(\\hat{\\theta}\\). 3. Afficher lhistogramme de léchantillon et superposer la densité de Weibull de paramètre \\(\\hat{\\theta}\\). "],["estimation-par-intervalle-de-confiance.html", "Chapitre 4 Estimation par Intervalle de confiance 4.1 Intervalles de confiance pour les paramètres de la loi normale 4.2 Intervalle de confiance pour une proportion 4.3 Récapitulatif pour la construction dintervalles de confiance", " Chapitre 4 Estimation par Intervalle de confiance Jusquà présent, on a estimé un paramètre \\(\\theta\\) par une unique valeur \\(\\hat{\\theta}_{n}\\) (estimation ponctuelle). Si lestimateur \\(\\hat{\\theta}_{n}\\) possède de bonnes propriétés (sans biais, variance minimale), on peut sattendre à ce que \\(\\hat{\\theta}_{n}\\) soit proche de la vraie valeur de \\(\\theta\\). Cependant, il est très peu probable que \\(\\hat{\\theta}_{n}\\) soit exactement égal à \\(\\theta\\). En particulier, si la loi de \\(\\hat{\\theta}_{n}\\) est continue, on est certains que \\(P\\left(\\hat{\\theta}_{n}=\\theta\\right)=0\\) Par conséquent, plutôt que destimer \\(\\theta\\) par la seule valeur \\(\\hat{\\theta}_{n}\\), il semble raisonnable de proposer un ensemble de valeurs vraisemblables pour \\(\\theta,\\) quil est logique de prendre proches de \\(\\hat{\\theta}_{n}\\). Cet ensemble de valeurs est appelé région de confiance. Dire que toutes les valeurs de cet ensemble sont vraisemblables pour \\(\\theta\\), cest dire quil y a une forte probabilité que \\(\\theta\\) appartienne à cet ensemble. On supposera dans ce chapitre que \\(\\theta \\in \\mathbb{R}\\), donc la région de confiance sera un intervalle. Définition 4.1 Un intervalle de confiance de seuil (ou niveau de signification) \\(\\alpha \\in [0,1]\\) pour un paramètre \\(\\theta\\), est un intervalle aléatoire \\(I\\) tel que \\(P(\\theta \\in I)=1-\\alpha\\). \\(\\alpha\\) est la probabilité que le paramètre \\(\\theta\\) nappartienne pas à lintervalle \\(I\\), cest à dire la probabilité que lon se trompe en affirmant que \\(\\theta \\in I\\). Cest donc une probabilité derreur, qui doit être assez petite. Les valeurs usuelles de \\(\\alpha\\) sont \\(10 \\%, 5 \\%, 1 \\%,\\) etc. Remarque fondamentale: Les intervalles de confiance suscitent souvent des erreurs dinterprétation et des abus de langage. La raison essentielle de ce problème est expliquée ci dessous. Dans lécriture \\(P(\\theta \\in I)\\), \\(\\theta\\) est une grandeur inconnue mais non aléatoire. Ce sont les bornes de lintervalle \\(I\\) qui sont aléatoires. Posons \\(I=[Z_1,Z_2]\\). \\(Z_1\\) et \\(Z_2\\) sont des variables aléatoires. Soient \\(z_1\\) et \\(z_2\\) les réalisations de \\(Z_1\\) et \\(Z_2\\) pour une expérience donnée. Il est correct de dire une phrase du type : \\(\\theta\\) a \\(95 \\%\\) de chances dêtre compris entre \\(Z_1\\) et \\(Z_2\\), mais il est incorrect de dire: \\(\\theta\\) a \\(95 \\%\\) de chances dêtre compris entre \\(z_1\\) et \\(z_2\\). En fait, si on recommence 100 fois lexpérience, on aura 100 réalisations du couple \\(\\left(Z_{1}, Z_{2}\\right)\\), et donc 100 intervalles de confiance différents. En moyenne, \\(\\theta\\) sera dans 95 de ces intervalles. Par conséquent, il vaut mieux dire : on a une confiance de \\(95 \\%\\) dans le fait que \\(\\theta\\) soit compris entre \\(z_1\\) et \\(z_2\\). Le problème à régler est de trouver un procédé pour déterminer un intervalle de confiance pour un paramètre \\(\\theta\\). Il semble logique de proposer un intervalle de confiance centré sur un estimateur performant \\(\\hat{\\theta}_{n},\\) cest-à-dire de la forme \\(I=\\left[\\hat{\\theta}_{n}-\\varepsilon, \\hat{\\theta}_{n}+\\varepsilon\\right]\\) . Il reste alors à déterminer \\(\\varepsilon\\) de sorte que : \\[ P(\\theta \\in I)=P\\left(\\hat{\\theta}_{n}-\\varepsilon \\leq \\theta \\leq \\hat{\\theta}_{n}+\\varepsilon\\right)=P\\left(\\left|\\hat{\\theta}_{n}-\\theta\\right| \\leq \\varepsilon\\right)=1-\\alpha \\] Si cet intervalle de confiance est petit, lensemble des valeurs vraisemblables pour \\(\\theta\\) est resserré autour de \\(\\hat{\\theta}_{n}\\). Si lntervalle de confiance est grand, des valeurs vraisemblables pour \\(\\theta\\) peuvent être éloignées de \\(\\hat{\\theta}_{n}\\). Donc un intervalle de confiance construit à partir dun estimateur permet de mesurer la précision de cet estimateur. Soit \\(a\\) et \\(b\\) les bornes dun intervalle de confiance \\(I C_{1-\\alpha}(\\theta)\\) de niveau de confiance \\(1-\\alpha\\) pour le paramètre \\(\\theta\\). On a: \\[ p(a \\leq \\theta \\leq b)=1-\\alpha \\text { et donc } p(\\theta&lt;a)+p(\\theta&gt;b)=\\alpha \\] En posant \\(\\alpha=\\alpha_{1}+\\alpha_{2}\\), il existe une infinité de choix possibles pour \\(\\alpha_{1}\\) et \\(\\alpha_{2}\\), et donc de choix pour \\(a\\) et \\(b\\). Nous ne considérons que le cas dun intervalle bilatéral à risques symétriques, pour lesquels le risque est partagé en deux parts égales \\(\\alpha_{1}=\\alpha_{2}=\\frac{\\alpha}{2}\\). Néanmoins il arrive en pratique que lon sintéresse à des risques unilatéraux, mais nous en parlerons plus en détail dans les chapitres suivants sur les tests statistiques. Pour trouver un intervalle de confiance, il existe plusieurs méthodes. La plus efficace consiste à chercher une fonction pivotale, cest à dire une variable aléatoire fonction à la fois du paramètre \\(\\theta\\) et des observations \\(X_{1}, \\ldots, X_{n}\\), dont la loi de probabilité ne dépende pas de \\(\\theta\\). Dans la suite de ce chapitre, nous allons illustrer cette méthodologie par des exemples, en déterminant des intervalles de confiance pour: La moyenne et la variance dans un échantillon de loi normale. Une proportion, cest-à-dire le paramètre dun échantillon de loi de Bernoulli. Remarques: Lintervalle de confiance est fonction de lestimation \\(\\hat{\\theta}_{n}\\) de \\(\\theta\\). Lintervalle de confiance est également fonction de \\(\\alpha\\). Plus \\(\\alpha\\) est petit, plus le niveau de confiance est grand, et donc plus lintervalle sélargit. Lorsque la taille de léchantillon grandit, lestimateur \\(\\hat{\\theta}_{n}\\) étant convergeant la variance \\(V(\\hat{\\theta}_{n})\\) diminue, et lintervalle se rétrécit. Il est recommandé avant de poursuivre la lecture de se rappeler des lois déduites de la loi normale: loi de \\(\\chi^{2}\\), loi de Student \\(St(n)\\) et loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\). Suite à la définition de ces lois, nous introduisons le théorème de Fisher: Théorème 4.1 (Théorème de Fisher) Si \\(X_{1}, \\ldots, X_{n}\\) sont indépendantes et de même loi \\(\\mathcal{N}\\left(m, \\sigma^{2}\\right)\\), alors, en posant \\(\\overline{X}_{n}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\) et \\(S_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}\\), on a: \\(\\sum_{i=1}^{n} X_{i}\\) est de loi \\(\\mathcal{N}\\left(n m, n \\sigma^{2}\\right)\\). \\(\\overline{X}_{n}\\) est de loi \\(\\mathcal{N} \\left(m, \\frac{\\sigma^{2}}{n}\\right)\\). \\(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(X_{i}-m\\right)^{2}\\) est de loi \\(\\chi_{n}^{2}\\). \\(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=\\frac{n S_{n}^{2}}{\\sigma^{2}}\\) est de loi \\(\\chi_{n-1}^{2}\\). \\(\\overline{X}_{n}\\) et \\(S_{n}^{2}\\) sont indépendantes. \\(\\sqrt{n-1} \\frac{\\overline{X}_{n}-m}{S_{n}}\\) est de loi de Student \\(S t(n-1)\\). 4.1 Intervalles de confiance pour les paramètres de la loi normale 4.1.1 Intervalle de confiance pour lespérance dune loi normale avec variance connue Soit \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) avec \\(\\sigma\\) connu. Le meilleur estimateur de \\(\\mu\\) est \\(\\overline{X}\\). Comme \\(X\\) est de loi normale on a \\[ Z=\\frac{\\overline{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim \\mathcal{N}(0,1) \\] En prenant des risques symétriques, on peut lire dans les tables les quantiles \\(z_\\frac{\\alpha}{2}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\) de la loi normale centrée réduite dordres respectifs \\(\\frac{\\alpha}{2}\\) et \\(1-\\frac{\\alpha}{2}\\), tels que : \\[ P\\left(z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{1-\\frac{\\alpha}{2}}\\right)=1-\\alpha \\] ou encore \\[ P\\left(Z \\leq z_{\\frac{\\alpha}{2}}\\right)=p\\left(Z \\geq z_{1-\\frac{\\alpha}{2}}\\right)=\\frac{\\alpha}{2} \\] Les quantiles La notion du quantile est définie de la façon suivante: Définition 4.2 Pour une variable aléatoire continue \\(X\\), le nombre \\(q_{\\alpha}\\) tel que \\(P(X \\leq q_{\\alpha}) = \\alpha\\), est le quantile dordre \\(\\alpha\\) de la loi de \\(X\\). Ces quantiles sont notés de différentes façons: \\(z_{\\alpha}\\) pour la loi normale, \\(t_{n,\\alpha}\\) pour la loi de Student à \\(n\\) degrés de liberté, \\(z_{n,\\alpha}\\) pour la loi de \\(\\chi_{n}^{2}\\), etc. La figure 4.1 suivante illustre la définition de ces quantiles: Figure 4.1: Quantiles \\(z_{\\frac{\\alpha}{2}}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\) de la loi normale centrée réduite Comme la loi normale est symétrique, on a la propriété suivante: \\[z_{1-\\frac{\\alpha}{2}} = - z_{\\frac{\\alpha}{2}}\\] Ces quantiles sont donnés par les tables statistiques. Par exemple, pour \\(\\alpha=0.05=5\\%\\), on obtient \\(z_{\\frac{\\alpha}{2}}=-1.96\\). Pour \\(\\alpha=0.05=5\\%\\), on obtient selon la table de la loi normale centrée réduite \\(z_{\\frac{\\alpha}{2}}=-1.96\\) et \\(z_{1-\\frac{\\alpha}{2}} = 1.96\\). Revenons à la détermination dIC pour lespérance dune loi normale avec variance connue, on a \\[P(z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{1-\\frac{\\alpha}{2}}) = 1- \\alpha\\] qui peut sécrire \\[P(z_{\\frac{\\alpha}{2}} \\leq Z \\leq -z_{\\frac{\\alpha}{2}}) = 1- \\alpha\\] doù on tire \\[P(\\overline{X} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}) = 1- \\alpha\\] Doù lintervalle de confiance: \\[I C_{1-\\alpha}(\\mu)= [\\overline{X} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}},\\overline{X} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}]\\] Pour une réalisation numérique \\(x_1,\\ldots,x_n\\) dun échantillon \\(X_1,\\ldots,X_n\\) de taille \\(n\\), on obtient lintervalle de confiance sur \\(\\hat{\\mu}\\) au niveau de confiance \\(1-\\alpha\\): \\[I C_{1-\\alpha}(\\mu)= [\\overline{x} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}},\\overline{x} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}]\\] qui donne pour \\(\\alpha=0.05\\): \\[[\\overline{x} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\\overline{x} +1.96 \\frac{\\sigma}{\\sqrt{n}}]\\] La table de la loi normale centrée réduite et une application interactive permettant de calculer les quantiles de loi normale se trouvent dans lannexe B. Une deuxième table qui donne directement le quantile \\(z_{1-\\frac{\\alpha}{2}}\\) par rapport à \\(\\alpha\\) est présentée dans lannexe C. Le problème est que cet intervalle nest utilisable que si on connaît la valeur de \\(\\sigma\\). Or, dans la pratique, on ne connaît jamais les vraies valeurs des paramètres. Une idée naturelle est alors de remplacer \\(\\sigma\\) par un estimateur. 4.1.2 Intervalle de confiance pour lespérance dune loi normale avec variance inconnue Si la variance \\(\\sigma^2\\) est inconnue, on utilisera à sa place son meilleur estimateur \\(S_n^{*2}\\). Selon le théorème de Fisher, on sait que \\(\\frac{n }{\\sigma^{2}}S_{n}^{2}\\) est de loi \\(\\chi_{n-1}^{2}\\), et que \\(\\frac{n-1}{\\sigma^{2}} S_{n}^{*2}\\) est de loi \\(\\chi_{n-1}^{2}\\) aussi. La statistique que lon utilisera est donc \\[T_{n-1} = \\frac{\\overline{X}-\\mu}{S_n^*/\\sqrt{n}}\\] En remarquant quelle sécrit \\[T_{n-1} = \\frac{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{\\frac{n-1}{\\sigma^2}S^{*2}}{n-1}}}\\] on trouve quelle suit une loi de Student à \\(n-1\\) degrés de liberté, comme rapport dune loi normale centrée réduite sur la racine dun \\(\\chi^2\\) divisé par son degré de liberté. Comme précédemment, on obtient lintervalle de confiance: \\[I C_{1-\\alpha}(\\mu)= [\\overline{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{S^*}{\\sqrt{n}},\\overline{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{S^*}{\\sqrt{n}}]\\] où \\(t_{n-1,\\frac{\\alpha}{2}}\\) est le quantile dordre \\(\\frac{\\alpha}{2}\\) de la loi de Student à \\(n-1\\) degrés de liberté. Une table de la loi de Student se trouve dans lannexe D. Remarque: La variable aléatoire \\(\\frac{\\overline{X}-\\mu}{S^*/\\sqrt{n}}\\) est une fonction des observations \\(X_1,\\ldots,X_n\\) et du paramètre \\(\\mu\\) pour lequel on recherche un intervalle de confiance, dont la loi de probabilité ne dépend pas des paramètres du modèle \\(\\mu\\) et \\(\\sigma^2\\). Cest ce quon a appelé une fonction pivotale et cest ce que nous utiliserons pour construire des intervalles de confiance. Si la loi de \\(X\\) nest pas une loi normale: Dans ce cas, lorsque la taille de léchantillon \\(n\\) est supérieure ou égale à 30, le théorème central limite nous permet dutiliser le fait que \\(\\overline{X}_n\\) suit une loi normale, et donc les résultats précédents sont applicables. 4.1.3 Intervalle de confiance pour la variance dune loi normale Conformément à ce qui précède, on recherche une fonction pivotale, cest à dire une fonction des observations \\(X_{1}, \\ldots, X_{n}\\) et de \\(\\sigma^{2}\\), dont la loi de probabilité ne dépend ni de \\(\\mu\\) ni de \\(\\sigma^{2}\\). Une telle fonction est donnée par le théorème de Fisher: \\(\\frac{n S_{n}^{2}}{\\sigma^{2}}\\) est de loi \\(\\chi_{n-1}^{2}\\). On a donc, quels que soient les réels \\(a\\) et \\(b\\), \\(0&lt;a&lt;b\\) : \\[\\begin{aligned} P\\left(a \\leq \\frac{n S_{n}^{2}}{\\sigma^{2}} \\leq b\\right) &amp;=P\\left(\\frac{n S_{n}^{2}}{b} \\leq \\sigma^{2} \\leq \\frac{n S_{n}^{2}}{a}\\right) \\quad \\text { d&#39;une part } \\\\ &amp;=F_{\\chi_{n-1}^{2}}(b)-F_{\\chi_{n-1}^{2}}(a) \\quad \\text { d&#39;autre part. } \\end{aligned}\\] Il y a une infinité de façons possibles de choisir \\(a\\) et \\(b\\) de sorte que cette probabilité soit égale à \\(1-\\alpha\\). La façon la plus usuelle de procéder est déquilibrer les risques, cest-à-dire de prendre \\(a\\) et \\(b\\) tels que \\(F_{\\chi_{n-1}^{2}}(b)=1-\\frac{\\alpha}{2}\\) et \\(F_{\\chi_{n-1}^{2}}(a)=\\frac{\\alpha}{2}\\). La table de la loi du \\(\\chi^{2}\\) donne la valeur \\(z_{n, \\alpha}\\) telle que, quand \\(Z\\) est une variable aléatoire de loi \\(\\chi_{n}^{2}\\), alors \\(P\\left(Z&gt;z_{n, \\alpha}\\right)=1-F_{\\chi_{n}^{2}}\\left(z_{n, \\alpha}\\right)=\\alpha\\). Alors, pour \\(b=z_{n-1, 1-\\alpha / 2}\\) et \\(a=z_{n-1,\\alpha / 2}\\), on a bien \\[P\\left(\\frac{n S_{n}^{2}}{b} \\leq \\sigma^{2} \\leq \\frac{n S_{n}^{2}}{a}\\right)=1-\\alpha\\] Doù le résultat : Un intervalle de confiance de seuil \\(\\alpha\\) pour le paramètre \\(\\sigma^2\\) de la loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\) est: \\[\\left[\\frac{n S_{n}^{2}}{z_{n-1, 1-\\alpha / 2}}, \\frac{n S_{n}^{2}}{z_{n-1,\\alpha / 2}}\\right]=\\left[\\frac{(n-1) S_{n}^{* 2}}{z_{n-1, 1-\\alpha / 2}}, \\frac{(n-1) S_{n}^{* 2}}{z_{n-1,\\alpha / 2}}\\right]\\] Remarque 1: \\(P\\left(a \\leq \\sigma^{2} \\leq b\\right)=P(\\sqrt{a} \\leq \\sigma \\leq \\sqrt{b}),\\) donc un intervalle de confiance de seuil \\(\\alpha\\) pour lécart-type \\(\\sigma\\) est: \\[ \\left[\\sqrt{\\frac{n}{z_{n-1, 1-\\alpha / 2}}} S_{n}, \\sqrt{\\frac{n}{z_{n-1,\\alpha / 2}}} S_{n}\\right] \\] Remarque 2: Lintervalle de confiance est de la forme \\(\\left[\\varepsilon_{1} S_{n}^{2}, \\varepsilon_{2} S_{n}^{2}\\right]\\) avec \\(\\varepsilon_{1}&lt;1\\) et \\(\\varepsilon_{2}&gt;1\\) et non pas de la forme \\(\\left[S_{n}^{2}-\\varepsilon, S_{n}^{2}+\\varepsilon\\right] .\\) En fait, si on cherche un intervalle de confiance pour \\(\\sigma^{2}\\) de la forme \\(\\left[S_{n}^{2}-\\varepsilon, S_{n}^{2}+\\varepsilon\\right],\\) la démarche ne va pas aboutir, et on ne peut pas le savoir à lavance. Cest lintérêt des fonctions pivotales, qui imposent delles-mêmes la forme de lintervalle de confiance. Une table de la loi de \\(\\chi^2\\) se trouve dans lannexe E. 4.2 Intervalle de confiance pour une proportion Le probleme connu sous le nom dintervalle de confiance pour une proportion est en fait le problème de la détermination dun intervalle de confiance pour le paramètre \\(p\\) de la loi de Bernoulli, au vu dun échantillon \\(X_{1}, \\ldots, X_{n}\\) de cette loi. On a montré dans le chapitre précédent que le meilleur estimateur de \\(p\\) est \\(\\hat{P}_{n}=\\overline{X}_{n}\\). Supposant que la proportion \\(p\\) dindividus présentant un certain caractère \\(C\\) (par exemple, les individus ayant voté pour un certain candidat) au sein dune population est inconnue. Le meilleur estimateur de \\(p\\) est \\(\\hat{P}_{n}=\\overline{X}_{n} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) où \\(X_i\\) est une v.a. de Bernoulli de paramètre \\(p\\), définie par: \\[X_{i}=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { si l&#39;individu } i \\text{ possède la caractère } C} \\\\ {0} &amp; {\\text { sinon }}\\end{array}\\right.\\] Comme \\(X\\) suit une loi de Bernoulli \\(\\mathcal{B}(p)\\), \\(n \\hat{p} = \\sum_{i=1}^n X_i\\) suit une loi Binomiale \\(\\mathcal{B}(n,p)\\). Si \\(n\\) est faible, on utilisera les tables de la loi binomiale. Si \\(n\\) est suffisamment grand5, on peut considérer (daprès le TCL6) que \\(\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(np, np(1-p))\\), doù \\(\\hat{p}_n\\) suit une loi normale \\(\\mathcal{N}(p, \\frac{p(1-p)}{n})\\), et donc \\(Z = \\frac{\\hat{p}_n - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) suit une loi \\(\\mathcal{N}(0,1)\\). Le résultat précédent nest applicable que si \\(n\\) est suffisamment grand. On obtient alors, en fonction des quantiles \\(P(z_{\\alpha/2} \\leq Z \\leq - z_{\\alpha/2})=1-\\alpha\\), lintervalle de confiance pour \\(p\\): \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\Big]\\] Cet intervalle recouvre \\(p\\) avec la probabilité \\(1-\\alpha\\), mais il est toutefois inopérant puisque ses bornes dépendent de \\(p\\). En pratique, il existe trois façons dobtenir lintervalle de confiance. Nous retiendrons celle qui remplace \\(p\\) par son estimateur \\(\\hat{P}_n\\). Ainsi, on obtient lintervalle de confiance sur la proportion \\(p\\) en fonction de la valeur \\(\\hat{p}_n\\) de \\(\\hat{P}_n\\) sur notre échantillon: \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\Big]\\] 4.3 Récapitulatif pour la construction dintervalles de confiance Pour construire un IC, il faut dabord déterminer le paramètre concerné par létude et vérifier sil y a dautres paramètres à estimer. Le tableau suivant résume les cas les plus communs et qui sont présentés dans ce chapitre. Paramètre à borner dans un IC Symbole Autres paramètres? Section Moyenne dune loi normale \\(\\mu\\) Variance \\(\\sigma^2\\) connue 4.1.1 Moyenne dune loi quelconque, échantillon grand \\(\\mu\\) \\(\\sigma^2\\) connue et \\(n\\) suffisamment grand pour appliquer TCL 4.1.1 Moyenne dune loi normale \\(\\mu\\) \\(\\sigma^2\\) inconnue 4.1.2 Variance (ou écart-type) dune loi normale \\(\\sigma^2\\) Moyenne inconnue, mais estimée 4.1.3 Proportion \\(p\\) Rien 4.2 de sorte que \\(np&gt;5\\) et \\(n(1-p)&gt;5\\). Théorème centrale limite "],["exercices-3.html", "Exercices", " Exercices Exercice 4.1 Pour une population de loi normale de variance \\(\\sigma^2\\) connue, répondre aux questions suivantes: Quel est le niveau de confiance pour lintervalle \\(\\overline{x} - 2.14 \\sigma/\\sqrt{n} \\leq \\mu \\leq \\overline{x} + 2.14 \\sigma/\\sqrt{n}\\) ? Quel est le niveau de confiance pour lintervalle \\(\\overline{x} - 1.85 \\sigma/\\sqrt{n} \\leq \\mu \\leq \\overline{x} + 1.85 \\sigma/\\sqrt{n}\\) ? Quel est le niveau de confiance pour lintervalle \\(\\mu \\leq \\overline{x} + 1.96 \\sigma/\\sqrt{n}\\) ? Exercice 4.2 Nous souhaitons estimer un intervalle de confiance du gain dun circuit électronique7. On admet que le gain est distribué selon une loi normale despérance inconnue \\(m\\) et décart type \\(\\sigma = 20\\). Décrire lexpérience aléatoire, lunivers et la variable aléatoire \\(X\\). Soit \\(n\\) la taille de léchantillon et \\(\\alpha\\) lerreur. Donner lexpression de lintervalle de confiance de \\(m\\) en fonction de \\(n\\), \\(\\sigma\\) et \\(\\alpha\\). Comment doit varier la longueur de lintervalle de confiance en fonction de la taille déchantillon et le niveau de confiance? Confirmer votre réponse en donnant lintervalle de confiance de \\(m\\) selon les cas suivants: IC à 95% où \\(n=10\\) et \\(\\hat{m} = 1000\\). IC à 95% où \\(n=25\\) et \\(\\hat{m} = 1000\\). IC à 99% où \\(n=10\\) et \\(\\hat{m} = 1000\\). IC à 99% où \\(n=25\\) et \\(\\hat{m} = 1000\\). Quelle doit être la taille de léchantillon \\(n\\) pour que la longueur de lintervalle de confiance à \\(95\\%\\) soit \\(4\\)? Exercice 4.3 Le poids de paquets de poudre de lessive, à lissue de lempaquetage, est supposé suivre une loi normale \\(\\mathcal{N}(\\mu, \\sigma^2 )\\) dont lécart-type \\(\\sigma\\) est supposé connu et égal à \\(5 g\\). \\(\\sigma\\) représente la variabilité du poids due à limprécision de la machine. Le poids marqué sur les paquets est de \\(710 g\\). Toutes les heures, \\(10\\) paquets sont prélevés au hasard et pesés. On obtient pour une heure donnée, pour un échantillon de \\(10\\) paquets un poids moyen de \\(707g\\). Donner un estimateur puis une estimation du poids moyen des paquets de lessive. Donner un intervalle de confiance à \\(95\\%\\), puis à \\(99\\%\\) pour le poids moyen des paquets de lessive Déterminer \\(\\alpha\\) (à lunité près) pour quau seuil de risque \\(\\alpha \\%\\) un intervalle de confiance du poids moyen des paquets de lessive soit \\([705g;709g]\\). Exercice 4.4 Dans la population française, le pourcentage dindividus dont le sang est de rhésus négatif est de \\(15\\%\\). Dans un échantillon représentatif de \\(200\\) Basques français on observe que \\(44\\) personnes sont de rhésus négatif. Donner un intervalle de confiance à \\(99\\%\\) de la proportion de Basques français ayant un rhésus négatif. Exercice 4.5 Le Bureau de météorologie du gouvernement australien a fourni les précipitations annuelles moyennes (en millimètres) en Australie entre 1983 et 2002 comme suit8 499.2 555.2 398.8 391.9 453.4 459.8 483.7 417.6 469.2 452.4 499.3 340.6 522.8 469.9 527.2 565.5 584.1 727.3 558.6 338.6 Supposant que les précipitations annuelles moyennes suivent une loi normale de paramètres inconnus9. Construire un intervalle de confiance \\(95\\%\\) pour la moyenne annuelle de précipitations. Exercice 4.6 Le pourcentage de titane dans un alliage utilisé en aéronautique est mesuré sur \\(51\\) pièces sélectionnées de manière aléatoire. Ce pourcentage suit une distribution normale de paramètres inconnus. Lécart type (corrigé) de léchantillon est \\(s = 0.37\\). Construire un intervalle de confiance bilatéral à \\(95\\%\\) pour \\(\\sigma\\). Exercice 4.7 Les ampoules électriques dun fabricant A ont une durée de vie normalement distribuée et de moyenne \\(\\mu_1\\) avec un écart-type \\(\\sigma_1 = 200h\\) et celles dun fabricant B ont une durée de vie normalement distribuée et de moyenne \\(\\mu_2\\) avec un écart-type \\(\\sigma_2 = 100h\\). Un échantillon de \\(150\\) ampoules de A a donné une durée de vie moyenne de \\(1400h\\). Un échantillon de \\(100\\) ampoules B a donné une durée de vie moyenne de \\(1200h\\). Déterminer un intervalle de confiance à \\(95\\%\\) puis à \\(99 \\%\\) de la différence des durées de vie moyenne des variétés A et B. En électronique, le gain désigne la capacité dun circuit électronique à augmenter la puissance ou lamplitude dun signal. (source: Wikipedia). Lien On peut vérifier cette assomption, càd la normalité de données avec la figure connue sous le nom Droite de Henry ou QQ-plot. On pourra la tracer dans R avec la fonction qqnorm() "],["tp-intervalle-de-confiance.html", "TP Intervalle de confiance 4.4 IC pour la moyenne et la variance de la loi normale 4.5 IC pour une proportion et effet de la confiance 4.6 IC pour le paramètre \\(\\lambda\\) dune loi exponentielle", " TP Intervalle de confiance 4.4 IC pour la moyenne et la variance de la loi normale On se propose détudier le poids des poulpes femelles. On va construire des intervalles de confiances pour la moyenne et la variance de cette variable, à laide du fichier de données poulpe.csv . Récupérer le fichier poulpe.csv et charger le dans votre session . Sélectionner les poulpes femelles. Calculer les estimateurs de la moyenne et de la variance. Représenter les données sur un histogramme. Déterminer un intervalle de confiance à \\(95\\%\\) pour la moyenne, en calculant les bornes de lintervalle avec la fonction qt(). Retrouver cet intervalle à laide de la fonction t.test(). Déterminer un intervalle de confiance à \\(95\\%\\) pour la variance. Créer une fonction ICvar() ayant comme arguement un vecteur dobservations, et un risque \\(\\alpha\\), et qui sort lintervalle de confiance pour la variance de léchantillon. 4.5 IC pour une proportion et effet de la confiance Soit \\(X_1,\\ldots,X_n\\) un échantillon de la loi de Bernoulli de paramètre \\(p\\). Lemm et lemv de \\(p\\) est \\(\\hat{p}_n = \\overline{X}_n=\\frac{1}{n} \\sum_{i=1}^n X_i\\). Lintervalle de confiance approximativement de niveau \\(1-\\alpha\\) pour \\(p\\) si \\(np &gt; 5\\) et \\(n(1-p)&gt;5\\) est \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\Big]\\] où \\(z_{\\alpha/2}\\) désigne le quantile dordre \\(\\alpha/2\\) dune loi normale centrée réduite. Choisir une valeur de \\(p\\), strictement comprise entre \\(0\\) et \\(1\\). Tirer \\(100\\) échantillons de taille \\(100\\) de la loi de Bernoulli de paramètre \\(p\\). Pour \\(\\alpha=0.1\\), puis \\(0.05\\), puis \\(0.01\\), calculer les valeurs prises par les \\(100\\) intervalles de confiance bilatéraux de niveau \\(1-\\alpha\\) pour \\(p\\). Calculer en pourcentage le nombre dintervalles qui contiennent la valeur de \\(p\\). Interpréter. Représenter graphiquement les intervalles par des segments horizontaux superposés, et la vraie valeur du paramètre \\(p\\) par un trait rouge vertical. (Indication: utiliser les fonctions matplot() et abline(). On pourra aussi colorier différemment les IC qui contiennent ou pas \\(p\\)). Refaire la simulation précédante en tirant \\(100\\) échantillons de taille \\(20\\) au lieu de \\(100\\) et en choisissant \\(p\\) entre \\(0\\) et \\(0.2\\). Pour \\(\\alpha=0.05\\), calculer le nombre dintervalles qui ne contiennent pas la valeur de \\(p\\). Interpréter. 4.6 IC pour le paramètre \\(\\lambda\\) dune loi exponentielle Soit \\(X_1,\\ldots,X_n\\) un échantillon de la loi exponentielle de paramètre \\(\\lambda\\). Lemv de \\(\\lambda\\) est \\(\\hat{\\lambda} = n/(X_1+\\ldots+X_n)\\). La variable aléatoire \\(Y=2\\lambda X\\) suit une loi exponentielle de paramètre \\(1/2\\), qui est aussi une loi \\(\\chi^2(2)\\) (preuve ). Soit donc \\(Y_i = 2\\lambda X_i\\) un échantillon iid de loi \\(\\chi^2(2)\\). Soit \\(T=2\\lambda \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i\\). Donc \\(T\\) suit la loi \\(\\chi^2(2n)\\). Construire un IC pour \\(\\lambda\\) (sur papier). Un modèle théorique suggère que le durée des appels téléphoniques suit une distribution exponentielle de paramètre \\(\\lambda\\). Un échantillon aléatoire de \\(n = 10\\) durées dappel suivantes (en minutes): 2.84 2.37 7.52 2.76 3.83 1.32 8.43 2.25 1.63 0.27 Calculer lIC de \\(\\lambda\\) à niveau de confiance \\(95\\%\\) selon cet échantillon. Simulation Choisir maintenant une valeur de \\(\\lambda\\). Tirer \\(100\\) échantillons de taille \\(10\\) de la loi exponentielle de paramètre \\(\\lambda\\). Calculer les \\(100\\) valeurs prises par lestimateur \\(T\\) sur ces échantillons. Pour \\(\\alpha=0.1\\), puis \\(0.05\\), puis \\(0.01\\), calculer les valeurs prises par les \\(100\\) intervalles de confiance bilatéraux de niveau \\(1-\\alpha\\) pour \\(\\lambda\\). Calculer en pourcentage le nombre dintervalles qui contiennent la valeur de \\(\\lambda\\). Représenter graphiquement les intervalles par des segments horizontaux superposés, et la vraie valeur du paramètre \\(\\lambda\\) par un trait rouge vertical. "],["tests-dhypothèses.html", "Chapitre 5 Tests dhypothèses 5.1 Introduction: le problème de décision 5.2 Les tests dhypothèses à partir dun seul échantillon", " Chapitre 5 Tests dhypothèses 5.1 Introduction: le problème de décision Dans tous les domaines, de lexpérimentation scientifique à la vie quotidienne, on est amené à prendre des décisions sur une activité risquée au vu de résultats dexpériences ou dobservation de phénomènes dans un contexte incertain. Par exemple: Informatique: au vu des résultats des tests dun nouveau système informatique, on doit décider si ce système est suffisamment fiable et performant pour être mis en vente. Essais thérapeutiques: décider si un nouveau traitement médical est meilleur quun ancien au vu du résultat de son expérimentation sur des malades. Finance: au vu du marché, décider si on doit ou pas se lancer dans une opération financière donnée. Dans chaque cas, le problème de décision consiste à trancher, au vu dobservations, entre une hypothèse appelée hypothèse nulle, notée \\(H_0\\), et une autre hypothèse dite hypothèse alternative, notée \\(H_1\\). En général, on suppose quune et une seule de ces deux hypothèses est vraie. Un test dhypothèses est une procédure qui permet de choisir entre ces deux hypothèses. Dans un problème de décision, deux types derreurs sont possibles: Erreur de première espèce: décider que \\(H_1\\) est vraie alors que \\(H_0\\) est vraie. Erreur de seconde espèce: décider que \\(H_0\\) est vraie alors que \\(H_1\\) est vraie. Les conséquences de ces deux erreurs peuvent être dimportances diverses. En général, une des erreurs est plus grave que lautre: Informatique: si on conclut à tort que le système nest pas assez fiable et performant, on engagera des dépenses inutiles pour le tester et lanalyser et on risque de se faire souffler le marché par la concurrence; si on décide à tort quil est suffisamment fiable et performant, on va mettre en vente un produit qui ne satisfera pas la clientèle, ce qui peut coûter cher en image de marque comme en coût de maintenance. Essais thérapeutiques: on peut adopter un nouveau traitement moins efficace, voire pire que lancien, ou se priver dun nouveau traitement plus efficace que lancien. Finance: si on décide à tort que lon peut lancer lopération, on risque de perdre beaucoup dargent; si on décide à tort de ne pas lancer lopération, on peut se priver dun bénéfice important. A toute décision correspond une probabilité de décider juste et une probabilité de se tromper: La probabilité de lerreur de première espèce, qui est la probabilité de rejeter à tort \\(H_0\\), est notée \\(\\alpha\\) et est appelée seuil ou niveau de signification du test. Cest la même terminologie que pour les intervalles de confiance, ce qui nest pas un hasard10. Dans certains contextes, cette probabilité est appelée risque fournisseur. La probabilité de lerreur de deuxième espèce est notée \\(\\beta\\) et est parfois appelée risque client. Cest la probabilité daccepter à tort \\(H_0\\). La probabilité de décider \\(H_1\\) ou de rejeter \\(H_0\\) à raison est \\(1-\\beta\\). Elle est appelée puissance du test. \\(1-\\alpha\\) est appelée niveau de confiance du test. Les deux tableaux suivants résument simplement le rôle de ces probabilités. Décision \\(\\backslash\\) Vérité \\(H_0\\) \\(H_1\\) \\(H_0\\) conclusion correcte erreur de deuxième espèce \\(H_1\\) erreur de première espèce conclusion correcte Décision \\(\\backslash\\) Vérité \\(H_0\\) \\(H_1\\) \\(H_0\\) niveau de confiance \\(1-\\alpha\\) risque \\(\\beta\\) \\(H_1\\) risque \\(\\alpha\\) Puissance de test \\(1-\\beta\\) Lidéal serait évidemment de trouver une procédure qui minimise les deux risques derreur en même temps. Malheureusement, on montre quils varient en sens inverse, cest- à-dire que toute procédure diminuant \\(\\alpha\\) va en général augmenter \\(\\beta\\) et réciproquement. Dans la pratique, on va donc considérer que lune des deux erreurs est plus importante que lautre, et tâcher déviter que cette erreur se produise. Il est alors possible que lautre erreur survienne. Par exemple, dans le cas du procès, on fait en général tout pour éviter de condamner un innocent, quitte à prendre le risque dacquitter un coupable. On va choisir \\(H_0\\) et \\(H_1\\) de sorte que lerreur que lon cherche à éviter soit lerreur de première espèce, \\(\\alpha\\). Mathématiquement cela revient à se fixer la valeur du seuil du test \\(\\alpha\\). Plus la conséquence de lerreur est grave, plus \\(\\alpha\\) sera choisi petit. Les valeurs usuelles de \\(\\alpha\\) sont \\(10\\%\\), \\(5\\%\\), \\(1\\%\\), ou beaucoup moins. Le principe de précaution consiste à limiter au maximum la probabilité de se tromper, donc à prendre \\(\\alpha\\) très petit. On choisit \\(H_0\\) et \\(H_1\\) de sorte que lerreur que lon cherche à éviter soit lerreur de première espèce, \\(\\alpha\\). Le choix dun test sera le résultat dun compromis entre risque de premier espèce et puissance du test. On appelle règle de décision une règle qui permette de choisir entre \\(H_0\\) et \\(H_1\\) au vu des observations \\(x_1 ,\\ldots,x_n\\), sous la contrainte que la probabilité de rejeter à tort \\(H_0\\) est égale à \\(\\alpha\\) fixé. Une idée naturelle est de conclure que \\(H_0\\) est fausse sil est très peu probable dobserver \\(x_1 ,\\ldots,x_n\\) quand \\(H_0\\) est vraie. Par exemple, admettons que lon doive décider si une pièce est truquée ou pas au vu de 100 lancers de cette pièce. Si on observe 90 piles, il est logique de conclure que la pièce est truquée et on pense avoir une faible probabilité de se tromper en concluant cela. Mais si on observe 65 piles, que conclure? Une fois que lon a fixé raisonnablement \\(\\alpha\\), il faut choisir une variable de décision, qui doit apporter le maximum dinformation sur le problème posé, et dont la loi sera différente selon que \\(H_0\\) ou \\(H_1\\) est vraie. La loi sous \\(H_0\\) doit être connue. On définit alors la région critique qui est lensemble des valeurs de la variable de décision qui conduisent à rejeter \\(H_0\\) au profit de \\(H_1\\). Sa forme est déterminée par la nature de \\(H_1\\), et sa détermination exacte est donnée par \\(P(RC|H_0) = \\alpha\\). La région dacceptation est son complémentaire \\(\\overline{RC}\\). Remarque: il vaut mieux dire ne pas rejeter \\(H_0\\) que accepter \\(H_0\\). En effet, si on rejette \\(H_0\\), cest que les observations sont telles quil est très improbable que \\(H_0\\) soit vraie. Si on ne rejette pas \\(H_0\\), cest quon ne dispose pas de critères suffisants pour pouvoir dire que \\(H_0\\) est fausse. Mais cela ne veut pas dire que \\(H_0\\) est vraie. Un test permet de dire quune hypothèse est très probablement fausse ou seulement peut-être vraie. Par conséquent, dans un problème de test, il faut choisir les hypothèses \\(H_0\\) et \\(H_1\\) de façon à ce que ce qui soit vraiment intéressant, cest de rejeter \\(H_0\\). 5.1.1 Les tests unilatéraux et bilatéraux Une hypothèse alternative bilatérale est appropriée lorsquon souhaite vérifier si le paramètre \\(\\theta\\) dune distribution diffère dune valeur arbitraire \\(\\theta_0\\). Par exemple, sil est important de détecter les valeurs de la moyenne réelle \\(\\mu\\) plus grandes ou plus petites que \\(\\mu_0\\), on doit utiliser lalternative bilatérale (seule \\(H_1\\) est composite): \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu\\neq\\mu_{0} \\] Par contre, supposons quon désire rejeter \\(H_0\\) seulement lorsque la vraie valeur de la moyenne excède \\(\\mu_0\\). Ainsi, les hypothèses sont \\[ H_{0}: \\mu \\leq \\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] La région critique est alors située dans laile supérieure de la distribution de la statistique. On rejette \\(H_0\\) si \\(\\overline{x}\\) est trop grand. Ceci est un test unilatéral à droite (\\(H_0\\) et \\(H_1\\) sont composites). Une troisième possibilité consiste à faire un test unilatéral à gauche en posant les hypothèses \\[ H_{0}: \\mu \\geq \\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] On rejette alors \\(H_0\\) lorsque la moyenne échantillonnale est trop petite. Pour les règles de décision des tests unilatéraux, sous \\(H_0\\) on suppose que \\(\\mu=\\mu_0\\). \\(H_0\\) et \\(H_1\\) sont complémentaires: une des deux hypothèses est forcément vraie. 5.1.2 Relation entre tests dhypothèses bilatéraux et intervalles de confiance Il existe une relation étroite entre le test dune hypothèse sur un paramètre \\(\\theta\\) et lintervalle de confiance pour \\(\\theta\\). Si \\([L,U]\\) est un intervalle de confiance de niveau \\(1-\\alpha\\) pour le paramètre \\(\\theta\\), alors le test de seuil \\(\\alpha\\) de lhypothèse \\[ H_{0}: \\theta=\\theta_{0} \\text { contre } H_{1}: \\theta\\neq \\theta_{0} \\] mènera au rejet de \\(H_0\\) si et seulement si \\(\\theta_0\\) nappartient par à lintervalle \\([L,U]\\). Exemple 5.1 Considérons une situation où un intervalle de confiance de niveau \\(1-\\alpha=0.95\\) pour la moyenne dune population a été calculé à partir dun échantillon, et où on a obtenu \\([41,43]\\). Comme la valeur \\(40\\) nappartient pas à cet intervalle, alors on sait quen conduisant un test dhypothèse sur le même échantillon, lhypothèse \\(H_0 : \\mu = 40\\) sera rejetée au seuil \\(\\alpha=0.05\\), au profit de son alternative bilatérale. 5.1.3 La démarche générale dun test dhypothèse Récapitulons lensemble de la démarche à suivre pour effectuer un test dhypothèses: Choisir \\(H_0\\) et \\(H_1\\) de sorte que ce qui importe, cest le rejet de \\(H_0\\). Se fixer \\(\\alpha\\) selon la gravité des conséquences de lerreur de première espèce. Déterminer la région critique: établir la règle de rejet de \\(H_0\\) à partir de la distribution de la statistique du test (lorsque \\(H_0\\) est vraie). Calculer la valeur observée de la statistique du test à partir de léchantillon. Comparer la valeur observée à la valeur critique: Regarder si les observations se trouvent ou pas dans la région critique.11 Conclure au rejet ou au non-rejet de \\(H_0\\). 5.1.4 Types de test dhypothèses On distingue différentes catégories de tests: Les tests paramétriques ont pour objet de tester une certaine hypothèse relative à un ou plusieurs paramètres dune variable aléatoire de loi spécifiée (généralement supposée normale). Lorsque le test est toujours valide pour des variables non gaussiennes, on dit que le test est robuste (à la loi). Les tests non paramétriques qui portent généralement sur la fonction de répartition de la variable aléatoire, sa densité, etc. Les tests libres (distributions free) qui ne supposent rien sur la loi de probabilité de la variable aléatoire étudiée (et qui sont donc robuste). Ces tests sont souvent non paramétriques, mais pas toujours. On sintéressera dabord dans ce chapitre à des tests dhypothèses paramétriques quand lobservation est un échantillon dune loi de probabilité. Puis on étudiera dans les chapitres suivants des tests de comparaison de deux échantillons, et on terminera en présentant le plus célèbre des tests dhypothèses, le test du \\(\\chi^2\\). 5.2 Les tests dhypothèses à partir dun seul échantillon Dans cette section, nous abordons les détails de la procédure des tests dhypothèses dans diverses situations: les tests sur la moyenne dune distribution normale avec variance connue et inconnue, les tests sur la variance dune distribution normale, et les tests sur une proportion (pour une grande taille déchantillon). 5.2.1 Les tests sur la moyenne dune distribution normale de variance connue 5.2.1.1 La procédure pour le test bilatéral Supposons que la variable aléatoire \\(X\\) représente un certain processus ou une certain population dintérêt. On suppose que \\(X\\) suit une loi normale ou que, dans le cas contraire, le théorème central limite sapplique. De plus on suppose que la moyenne \\(\\mu\\) de \\(X\\) est inconnue, mais que sa variance \\(\\sigma^2\\) est connue. On veut tester les hypothèses \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu\\neq\\mu_{0} \\] où \\(\\mu_0\\) est une constante spécifiée. Un échantillon aléatoire simple \\(X_1,\\ldots,X_n\\) de taille \\(n\\) est disponible. Chaque observation dans cet échantillon a une moyenne théorique \\(\\mu\\) inconnue et une variance théorique \\(\\sigma^2\\) connue. Si lhypothèse nulle \\(H_0: \\mu = \\mu_0\\) est vraie, alors \\(\\overline{X}_n \\thicksim \\mathcal{N}(\\mu_0,\\sigma^2/n)\\). La procédure de test utilise la statistique: \\[ Z_0 = \\frac{ \\overline{X}_n - \\mu_0 }{\\sigma/\\sqrt{n}} \\] laquelle, sous lhypothèse nulle, suit la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). Donc, si \\(H_0: \\mu = \\mu_0\\) est vraie, la probabilité est \\(1-\\alpha\\) quune valeur de la statistique \\(Z_0\\) soit entre \\(z_{\\frac{\\alpha}{2}}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\), où \\(z_{1-\\frac{\\alpha}{2}}\\) est le quantile de la loi normale centrée réduite tel que \\(P(Z_0 \\geq z_{1-\\frac{\\alpha}{2}})=\\alpha/2\\). Les figures 5.1 et 5.2 illustrent respectivement la distribution de \\(\\overline{X}_n\\) et \\(Z_0\\) lorsque \\(H_0\\) est vraie. Figure 5.1: La distribution de \\(\\overline{X}_n\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie Figure 5.2: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie Visiblement, un échantillon donnant une valeur de la statistique qui appartient aux extrémités de la distribution de \\(Z_0\\) est peu probable si \\(H_0: \\mu = \\mu_0\\) est vraie. Le cas échéant, on aurait raison de remettre en doute la véracité de \\(H_0\\). Critère de rejet de \\(H_0: \\mu = \\mu_0\\) (test bilatéral, loi normale, variance connue) On se donne comme règle de rejeter \\(H_0: \\mu = \\mu_0\\) au profit de \\(H_1: \\mu \\neq \\mu_0\\) si \\[\\begin{equation} z_0 &lt; z_{\\alpha/2} \\text{ ou si } z_0 &gt; z_{1-\\alpha/2} \\tag{5.1} \\end{equation}\\] et de na pas rejeter \\(H_0\\) si \\[\\begin{equation} z_{\\alpha/2} \\leq z_0 \\leq z_{1-\\alpha/2} \\tag{5.2} \\end{equation}\\] Léquation (5.1) définit la région critique ou de rejet de \\(H_0\\). La probabilité de lerreur de première espèce de cette procédure du test est \\(\\alpha\\). On peut déduire de léquation (5.2) que les deux valeurs critique de la figure 5.1 sont les suivantes: \\[ v_1 = \\mu_0 + z_{\\alpha/2} \\, \\sigma/\\sqrt{n} \\quad \\text{ et } \\quad v_2 = \\mu_0 + z_{1-\\alpha/2} \\, \\sigma/\\sqrt{n} \\] Exemple 5.2 On étudie la vitesse de combustion du carburant dune fusée. Le cahier des charges exige que la vitesse moyenne de combustion soit de \\(40\\, cm/{s}\\). Supposons que lécart-type de la vitesse de combustion est denviron \\(2 \\, cm/s\\) et que cette variable suit une distribution normale. Hypothèses et seuil: Lexpérimentateur décide de spécifier une probabilité derreur de première espèce \\(\\alpha=0.05\\) et base le test sur un échantillon aléatoire de taille \\(n=25\\). Il veut tester les hypothèses \\({H}_{0}: \\mu=40\\, cm/s\\) contre \\({H}_{1}: \\mu \\neq 40\\, cm/s\\). Règle de rejet de \\({H}_{0}\\): Puisque le test est bilatéral, on rejettera \\(H_{0}\\) si \\(\\overline{x}\\) est trop grand ou trop petit par rapport à \\(40\\, cm/s\\). On devra comparer la valeur observée \\(z_{0}\\) de la statistique du test \\(Z_{0}\\) avec les valeurs critiques \\(\\pm z_{0.025}=\\pm 1,96\\). Si \\(-1.96 \\leq z_{0} \\leq 1.96\\), lhypothèse nulle ne sera pas rejetée. Calcul de la statistique observée: Les 25 unités sont essayées, et la vitesse moyenne de combustion de léchantillon obtenue est \\(\\overline{x}=41.25\\, cm/s\\). La valeur de la statistique dans léquation \\(Z_0\\) est \\[ z_{0}=\\frac{\\overline{x}-\\mu_{0}}{\\sigma / \\sqrt{n}}=\\frac{41.25-40}{2 / \\sqrt{25}}=3.125 \\] Décision: On remarque que \\(z_0\\) appartient à la région critique, car \\(3.125&gt;1.96\\). Donc, \\(H_0\\) est rejetée. Conclusion: La vitesse moyenne de combustion excède \\(40\\, cm/s\\) de façon significative au seuil de \\(5 \\%\\). 5.2.1.2 La procédure pour les tests unilatéraux Supposons maintenant quon désire tester lhypothèse alternative unilatérale à droite, soit \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] En définissant la région critique de ce test, on remarque quune valeur négative de la statistique \\(Z_0\\) ne conduira jamais à conclure que \\(H_0: \\mu=\\mu_0\\) est fausse. On doit donc placer la région critique dans laile supérieure de la distribution \\(\\mathcal{N}(0,1)\\) et rejeter \\(H_0\\) pour les trop grandes valeurs de \\(z_0\\). Autrement dit, on doit rejeter \\(H_0\\) si \\(z_0 &gt; z_{1-\\alpha}\\). De même, pour tester lhypothèse unilatérale à gauche \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] on doit calculer la statistique \\(Z_0\\) et rejeter \\(H_0\\) pour les trop petites valeurs de \\(z_0\\). Autrement dit, la région critique est dans laile inférieure de la distribution \\(\\mathcal{N}(0,1)\\), et on rejette \\(H_0\\) si \\(z_0 &lt; z_{\\alpha}\\) Figure 5.3: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie contre \\(H_1: \\mu &gt; \\mu_0\\) (Upper-tailed test) Figure 5.4: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie contre \\(H_1: \\mu &lt; \\mu_0\\) (Lower-tailed test) 5.2.1.3 Le calcul du risque de deuxième espèce et la puissance du test Lorsque lanalyste teste des hypothèses, il choisit directement la probabilité de lerreur de première espèce \\(\\alpha\\). Cependant, la probabilité de lerreur de deuxième espèce \\(\\beta\\) doit aussi être prise en compte. Considérons par exemple lhypothèse unilatérale: \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] Supposons que lhypothèse nulle est fausse et que la valeur réelle de la moyenne est \\(\\mu_1\\), une valeur plus grande que \\(\\mu_0\\). Puisque \\(H_1\\) est vraie, la moyenne échantillonnale \\(\\overline{X}\\) suit la loi \\(\\mathcal{N}(\\mu_1,\\sigma^2/n)\\). La figure 5.5 représente la distribution de \\(\\overline{X}\\) sous lhypothèse nulle \\(H_0\\) et sous lhypothèse alternative \\(H_1\\). On rejette \\(H_0\\) si \\(\\overline{x}\\) est supérieure à la valeur critique \\(v\\), qui vaut \\(\\mu_0 + z_{1-\\alpha} \\sigma/\\sqrt{n}\\). Rappelons quune erreur de deuxième espèce est commise si \\(H_0\\) nest pas rejetée alors quelle est fausse. Lexamen de cette figure révèle que si \\(H_1\\) est vraie, une erreur de deuxième espèce ne sera commise que si \\(\\overline{x} &lt; v\\). Autrement dit, la probabilité de lerreur de deuxième espèce \\(\\beta\\) correspond à la région verte de la figure 5.5. Figure 5.5: La distribution de \\(\\overline{X}\\) sous \\(H_0\\) et sous \\(H_1\\) Figure 5.6: La distribution de \\(Z_0\\) sous \\(H_0\\) et sous \\(H_1\\) Les facteurs qui influencent le risque de deuxième espèce \\(\\beta\\) du test sur une moyenne: \\(\\beta\\) diminue lorsque \\(\\mu_1\\) séloigne de \\(\\mu_0\\). \\(\\beta\\) diminue lorsque \\(n\\) augmente. \\(\\beta\\) diminue lorsque \\(\\alpha\\) augmente. Concernant la puissance du test, qui vaut \\(1 - \\beta\\): La puissance dun test est fonction de la nature de \\(H_1\\), un test unilatéral est plus puissant quun test bilatéral. La puissance dun test augmente lorsque \\(n\\) augmente. La puissance dun test diminue lorsque \\(\\alpha\\) diminue. Considérons le problème du combustible de fusée de lexemple 5.2 et tenons pour acquis quon réalise un test unilatérale à droite: \\[ H_{0}: \\mu=40\\, cm/{s} \\text { contre } H_{1}: \\mu &gt; 40 \\, cm/{s} \\] Supposons que lanalyste veut rejeter \\(H_0\\) avec une forte probabilité si la véritable vitesse moyenne de combustion est \\(\\mu=41\\, cm/{s}\\). En dautres termes, il veut une faible probabilité derreur de deuxième espèce si \\(\\mu=41\\, cm/{s}\\). Montrer que la puissance du test est égale à \\(80.5\\%\\). Indication: Commencer par calculer: \\[\\beta = P(\\text{Ne pas rejeter } H_0 | H_1 \\text{ est vraie})\\] Puisque la puissance du test est affectée par la taille de léchantillon, il est courant de fixer \\(1-\\beta\\) et de trouver la valeur de \\(n\\) qui nous permet de latteindre. 5.2.2 Les tests sur la moyenne dune distribution normale de variance inconnue On a élaboré la procédure de test pour lhypothèse nulle \\(H_0:\\mu=\\mu_0\\) en supposant que la variance \\(\\sigma^2\\) est connue. Cependant, dans de nombreuses situations pratiques, \\(\\sigma^2\\) nest pas connue. Pour tester des hypothèses sur la moyenne \\(\\mu\\) dune population lorsque la variance \\(\\sigma^2\\) est inconnue, on suppose que la distribution de la variable \\(X\\) est normale12. Cette supposition mène à une procédure de test souvent appelée test \\(t\\) ou test de Student, car elle fait appel à la loi de Student13. 5.2.2.1 La procédure pour le test bilatéral Supposons que \\(X\\) est une variable aléatoire de distribution normale de moyenne \\(\\mu\\) et de variance \\(\\sigma^{2}\\) inconnues. Testons lhypothèse que \\(\\mu\\) est égale a une constante \\(\\mu_{0}\\). On remarque que cette situation est similaire à celle qui a été traitée dans la section précédente sauf que, maintenant, les deux paramètres \\(\\mu\\) et \\(\\sigma^{2}\\) sont inconnus. Supposons quon dispose dun échantillon de taille \\(n\\), soit \\(X_{1}, X_{2}, \\ldots, X_{n}\\), et soit respectivement \\(\\overline{X}\\) et \\({S_n^{*}}^2=\\frac{n}{n-1} S_{n}^{2}\\) la moyenne et la variance de léchantillon. Supposons quon désire tester lhypothèse alternative bilatéral \\[ {H}_{0}: \\mu=\\mu_{0} \\text{ contre } {H}_{1}: \\mu\\neq \\mu_{0} \\] La procédure du test \\(t\\) repose sur la statistique \\[ T_{0}=\\frac{\\overline{X}-\\mu_{0}}{S^* / \\sqrt{n}} \\] qui suit la loi \\(St\\) avec \\(n-1\\) degrés de liberté si lhypothèse nulle \\({H}_{0}: \\mu=\\mu_{0}\\) est vraie. Critère de rejet de \\(H_0: \\mu = \\mu_0\\) (test bilatéral, loi normale, variance inconnue) On se donne comme règle de rejeter \\(H_0: \\mu = \\mu_0\\) au profit de \\(H_1: \\mu \\neq \\mu_0\\) si \\[|t_0| &gt; t_{n-1,\\frac{\\alpha}{2}}\\] où \\(t_0\\) est la valeur observée de la Statistique \\(T_0\\) et \\(t_{n-1,\\frac{\\alpha}{2}}\\) est le quantile dordre \\(1-\\alpha/2\\) de la loi \\(t\\) de Student avec \\(n-1\\) degrés de liberté. 5.2.2.2 La procédure pour les tests unilatéraux Pour lhypothèse alternative unilatérale à droite \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] On calcule la valeur observée \\(t_0\\) et on rejette \\(H_0\\) si \\(t_0 &gt; t_{n-1,{1-\\alpha}}\\). Pour lhypothèse alternative unilatérale à gauche \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] On rejette \\(H_0\\) si \\(t_0 &lt; t_{n-1,{\\alpha}}\\). Les tests sur la moyenne dans seffectuent à laide de la fonction t.test() 5.2.3 Les tests sur la variance dune distribution normale Soit un \\(n\\)-échantillon \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) issu dune population de loi normale, de moyenne \\(\\mu\\) et de variance \\(\\sigma^{2}\\). La normalité est indispensable pour ce test sur la variance. 5.2.3.1 La procédure pour le test bilatéral Supposons quon désire tester lhypothèse que la variance \\(\\sigma^2\\) dune distribution normale est égale à une valeur spécifiée \\(\\sigma_0^2\\), on teste \\[ H_{0}: \\sigma^2=\\sigma_{0}^2 \\text { contre } H_{1}: \\sigma^2 \\neq \\sigma_{0}^2 \\] On utilisera la statistique \\[ U_0 = \\frac{(n-1) {S_n^{*}}^2}{\\sigma_{0}^2} \\] où \\({S_n^{*}}^2\\) est la variance corrigée de léchantillon. Si \\(H_0:\\sigma^2=\\sigma_{0}^2\\) est vraie, alors la statistique \\(U_0\\) suit la loi du \\(\\chi^2(n-1)\\). Critère de rejet de \\(H_0:\\sigma^2=\\sigma_{0}^2\\) (test bilatéral, loi normale) On rejette \\(H_0:\\sigma^2=\\sigma_{0}^2\\) si \\[u_0 &lt; \\chi^2_{n-1,\\frac{\\alpha}{2}} \\quad \\text{ ou } \\quad u_0 &gt; \\chi^2_{n-1,1-\\frac{\\alpha}{2}}\\] où \\(u_0\\) est la valeur observée de la Statistique \\(U_0\\), \\(\\chi^2_{n-1,\\frac{\\alpha}{2}}\\) et \\(\\chi^2_{n-1,1-\\frac{\\alpha}{2}}\\) sont les quantile dordre \\(\\alpha/2\\) et \\(1-\\alpha/2\\) de la loi \\(\\chi^2\\) avec \\(n-1\\) degrés de liberté. Figure 5.7: La distribution de \\(U_0\\) lorsque \\(H_0 : \\sigma^2 = \\sigma^2_0\\) est vraie Attention, contrairement à la loi de Student et la loi normale centrée réduite, la densité de la loi du \\(\\chi^2\\) nest pas symétrique. 5.2.3.2 La procédure pour les tests unilatéraux sur la variance Pour lhypothèse alternative unilatérale à droite \\[ H_{0}: \\sigma^2=\\sigma^2_{0} \\text { contre } H_{1}: \\sigma^2&gt; \\sigma^2_{0} \\] On calcule la valeur observée \\(u_0\\) et on rejette \\(H_0\\) si \\(u_0 &gt; \\chi^2_{n-1,1-\\alpha}\\). Pour lhypothèse alternative unilatérale à gauche \\[ H_{0}: \\sigma^2=\\sigma^2_{0} \\text { contre } H_{1}: \\sigma^2&lt; \\sigma^2_{0} \\] On rejette \\(H_0\\) si \\(u_0 &lt; \\chi^2_{n-1,\\alpha}\\). 5.2.4 Les tests sur une proportion Dans la population étudiée, une proportion \\(p\\) des individus possèdent un certain caractère \\(C\\). On se propose de comparer cette proportion \\(p\\) à une valeur de référence \\(p_0\\). On considère un échantillon dindividus de taille \\(n\\) de cette population. La variable aléatoire \\(X_i\\) égale à \\(1\\) si lindividu \\(i\\) possède le caractère \\(C\\) suit une loi de Bernoulli \\(\\mathcal{B}(p)\\). Si \\(n\\) est suffisamment grand, on peut considérer que \\(\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(np,np(1-p))\\), doù la fréquence empirique \\(\\hat{p}_n= \\frac{1}{n}\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(p,\\frac{p(1-p)}{n})\\). Supposons quon désire tester \\[ H_0: p = p_0 \\text { contre } H_1: p \\neq p_0 \\] La statistique du test est donc la fréquence empirique \\(\\hat{p}_n\\) qui suit sous \\(H_0\\) la loi \\(\\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\\). Pour tester \\(H_0: p = p_0\\) on calcule la statistique du test \\[ Z_0 = \\frac{\\hat{p}_n - p_0}{\\sqrt{ \\frac{p_0(1-p_0)}{n}}} \\] qui suit approximativement la loi \\(\\mathcal{N}(0,1)\\) si lhypothèse nulle est vraie. Critère de rejet de \\(H_0:p = p_0\\) (test bilatéral, grand échantillon) On rejette \\(H_0:p = p_0\\) si \\[|z_0| &gt; z_{1-\\alpha/2}\\] où \\(z_{1-\\alpha/2}\\) est le quantile dordre \\(1-\\alpha/2\\) de la loi \\(\\mathcal{N}(0,1)\\). On situe comme dhabitude les régions critiques des hypothèses alternatives unilatérales. Il y a dualité entre intervalles de confiance et tests dhypothèses ou comparer la valeur \\(p\\) (p-value) avec le seuil du test. Cette valeur corresponds à une probabilité. Si cette probabilité est inférieure à \\(\\alpha\\), on rejette \\(H_0\\). Si elle est supérieure à \\(\\alpha\\), on ne rejette pas \\(H_0\\). supposition raisonnable dans de nombreux cas Si la supposition est déraisonnable, on peut spécifier une autre loi (exponentielle, Weibull, etc..) et utiliser une méthode générale de construction de test pour obtenir une procédure valide, ou on peut utiliser un des tests non paramétriques qui sont valides pour toute distribution. "],["exercices-4.html", "Exercices", " Exercices Exercice 5.1 Une entreprise commercialise des pieds de lit de type boule. Pour ces pieds on utilise une bague en matière plastique de diamètre intérieur \\(x\\). On définit ainsi une variable aléatoire \\(X\\) qui à chaque bague tirée au hasard dans la production, associe son diamètre intérieur \\(x\\) mesuré en millimètres. On admet que \\(X\\) suit la loi normale de moyenne \\(\\mu\\) et décart-type \\(0.04\\). Le fournisseur affirme que \\(\\mu=12.1 \\, mm\\). On a un doute sur cette affirmation. On prélève un échantillon de \\(64\\) pièces dans la livraison. Le diamètre intérieur moyen sur cet échantillon est de \\(12.095\\, mm\\). Que conclure au seuil de signification de \\(10\\%\\) quant au diamètre intérieur moyen des bagues? Exercice 5.2 Une usine fabrique des câbles. Un câble est considéré conforme si sa résistance à la rupture est supérieure à \\(3\\) tonnes. Lingénieur responsable de la production voudrait connaître, en moyenne, la résistance à la rupture des câbles fabriqués. Il nest, bien sûr, pas question de faire le test de rupture sur toute la production (lusine perdrait toute sa production!). Notons \\(X\\) la variable aléatoire correspondant à la force à exercer sur le câble pour le rompre (en tonnes). Pour cette question, on supposera que \\(X \\thicksim \\mathcal{N}(\\mu, \\sigma^2)\\) et que la valeur de \\(\\sigma\\) est ici connue et égale à \\(0.38\\). Un technicien prélève donc un échantillon de \\(100\\) câbles dans la production. Avec les données de léchantillon, le technicien obtient les résultats suivants : la résistance moyenne à la rupture des \\(100\\) câbles de léchantillon est de \\(3.5\\) tonnes. Décrire lexpérience aléatoire, la population, la probabilité utilisée sur lespace probabilisable et la variable aléatoire étudiée. Que représente le paramètre \\(\\mu\\)? Donner un estimateur puis une estimation de \\(\\mu\\). Peut-on dire, avec un risque derreur de \\(2.5\\%\\) que la résistance moyenne à la rupture de lensemble des câbles de la production est strictement supérieure à \\(3\\) tonnes? 2ème partie La proportion de câbles dont la résistance est supérieure à \\(3\\) tonnes dans cet échantillon est de \\(0.85\\). Décrire la nouvelle variable aléatoire étudiée \\(Y\\). Quelle est sa loi? Donner une estimation ponctuelle de la proportion \\(p\\) de câbles conformes dans la production. Peut-on dire, avec un risque derreur de \\(5\\%\\) que la proportion \\(p\\) de câbles conformes dans la production est strictement supérieure à \\(0.80\\)? Exercice 5.3 On utilise une nouvelle variété de pommes de terre dans une exploitation agricole. Le rendement moyen de lancienne variété était de \\(41.5\\) tonnes à lhectare. La nouvelle variété est cultivée sur \\(100\\) hectares, avec un rendement moyen de \\(45\\) tonnes à lhectare et un écart-type (échantillonnal) de \\(11.25\\). Faut-il, avec un risque derreur de \\(1\\%\\), favoriser la culture de la nouvelle variété? Calculer la puissance du test précédent si le vrai rendement moyen de la nouvelle variété est supposée égal à \\(44\\) tonnes. Calculez le risque derreur de deuxième espèce. Interpréter. Exercice 5.4 Un article dans le journal Growth14 a rapporté les résultats dune étude qui a mesuré le poids (en grammes) pour les cobayes à la naissance: 421 494.6 110.7 102.4 317 447.8 879 273 279.3 452.6 373.8 96.4 241 290.9 687.6 88.8 268 258.5 456.1 90.5 81.7 296 256.5 705.7 296 227.5 296 Tester lhypothèse que le poids moyen est \\(300\\) grammes avec niveau de confiance \\(95\\%\\). Expliquer comment répondre à la question précédente avec un IC bilatéral pour le poids moyen. Exercice 5.5 Un article dans un journal dagriculture15 a déterminé que la composition en acides aminés essentiels (Lysine) des repas de soja sont comme indiqué ici (g/kg): 22.2 24.7 20.9 26 27 24.8 26.5 23.8 25.6 23.9 Peut-on dire, à risque \\(1\\%\\) que la variance de la quantité de Lysine est égale à \\(1\\)? Confirmer le résultat du test avec un IC bilatéral pour cette variance. Article intitulé Comparison of Measured and Estimated Fat-Free Weight, Fat, Potassium and Nitrogen of Growing Guinea Pigs Article intitulé Non-Starch Polysaccharides and Broiler Performance on Diets Containing Soyabean Meal as the Sole Protein Concentrate dans le journal Australian Journal of Agricultural Research "],["tests-dhypothèses-sur-deux-échantillons.html", "Chapitre 6 Tests dhypothèses sur deux échantillons 6.1 Comparaison de deux échantillons gaussiens indépendants 6.2 Comparaison de deux proportions", " Chapitre 6 Tests dhypothèses sur deux échantillons Il est très fréquent que lon ait à comparer deux populations selon un critère quantitatif particulier. Par exemple: Comparer deux traitements médicaux au vu de leurs effets sur les patients. Comparer différents groupes détudiants au vu de leurs résultats scolaires. Comparer les fréquences doccurrences de maladies chez les fumeurs et les non fumeurs. Statistiquement, cela signifie que lon dispose dobservations de variables aléatoires \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) i.i.d. constituant le premier échantillon, et de variables aléatoires \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) i.i.d. constituant le deuxième échantillon. Comparer les deux échantillons revient à comparer les paramètres des lois de probabilité des \\(X_{1}\\) et des \\(X_{2}\\). Un test de comparaison de deux échantillons gaussiens indépendants consiste à supposer que les deux échantillons sont indépendants et de lois normales, et à comparer les moyennes et les variances de ces lois. Un test de comparaison de deux proportions consiste à supposer que les deux échantillons sont indépendants et de lois de Bernoulli, et à comparer les paramètres de ces lois. Un test de comparaison de deux échantillons gaussiens appariés consiste à supposer que les échantillons sont de lois normales, mais pas indépendants (en un certain sens) et à comparer les moyennes de ces lois. Il faut dans ce cas que les échantillons soient de même taille (ce dernier test nest pas abordé dans ce cours). 6.1 Comparaison de deux échantillons gaussiens indépendants Dans cette section, on supposera que les deux échantillons sont indépendants et de lois normales et comparera leurs moyennes et leurs variances. \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) sont supposées indépendantes et de même loi \\(\\mathcal{N}(m_1,\\sigma_1^2)\\) et \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) indépendantes et de même loi \\(\\mathcal{N}(m_2,\\sigma_2^2)\\). Les \\(X_{1}\\) et \\(X_{2}\\) sont indépendantes. Les moyennes empiriques et variances estimées de deux échantillons sont notées respectivement \\(\\overline{X}_{1}, S_1^2, {S_1^*}^2, \\overline{X}_{2}, S_2^2\\) et \\({S_2^*}^2\\). Exemple 6.1 Deux groupes détudiants de tailles respectives \\(n_1 = 25\\) et \\(n_2 = 31\\) ont suivi le même cours de statistique et passé le même examen. Les moyennes et écarts-types empiriques des notes obtenues dans les deux groupes sont respectivement: \\[ \\begin{array}{ll}{\\text { Premier groupe: }} &amp; {\\overline{x}_{{1}}=12.8, \\quad s_{1}^{*}=3.4} \\\\ {\\text { Deuxième groupe : }} &amp; {\\overline{x}_{{2}}=11.3, \\quad s_{2}^{*}=2.9}\\end{array} \\] On suppose que les notes sont réparties dans les deux groupes selon des lois normales et quelles sont toutes indépendantes. Peut-on considérer que le premier groupe est meilleur que le deuxième, cest-à-dire quun point et demi décart entre les moyennes est significatif dune différence de niveau? La procédure à suivre consiste à tester dabord légalité des variances, puis légalité des moyennes. 6.1.1 Test de Fisher de comparaison des variances Comparer les variances des deux échantillons, cest tester \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2\\neq\\sigma_2^2 \\] Pour tester cette hypothèse on utilise la statistique \\[ F_0 = \\frac{{S_1^*}^2}{{S_2^*}^2} \\] Cette statistique suit la loi \\(F\\) de Fisher, avec \\(n_1-1\\) et \\(n_2-1\\) degrés de liberté si lhypothèse \\(H_0\\) est vraie. (preuve ) Rappel de la définition de la loi de Fisher: Soit \\(U\\) et \\(V\\) deux variables aléatoires indépendantes suivant une loi de \\(\\chi^2\\) respectivement à \\(n\\) et \\(m\\) degrés de liberté. On dit que \\(F= \\frac{U/n}{V/m}\\) suit une loi de Fisher-Snedecor à \\((n,m)\\) degrés de liberté. \\(F \\thicksim \\mathcal{F}(n,m)\\) Figure 6.1: Densités de la loi de Fisher \\(F(n,m)\\) La procédure pour le test bilatéral Pour tester \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2\\neq\\sigma_2^2 \\] On calcule \\[ f_0 = \\frac{{s_1^*}^2}{{s_2^*}^2} \\] Critère de rejet de \\(H_{0}: \\sigma_1^2=\\sigma_2^2\\) On rejette \\(H_{0}: \\sigma_1^2=\\sigma_2^2\\) si \\[f_0 &gt; f_{1-\\alpha/2; \\, n_1-1,n_2-1} \\quad \\text{ ou } \\quad f_0 &lt; f_{\\alpha/2; \\, n_1-1,n_2-1}\\] où \\(f_{\\alpha/2; \\, n_1-1,n_2-1}\\) et \\(f_{1-\\alpha/2; \\, n_1-1,n_2-1}\\) sont les quantiles dordre \\(\\alpha/2\\) et \\(1-\\alpha/2\\) de la loi de Fisher \\(F\\) avec \\(n_1-1\\) et \\(n_2-1\\) degrés de liberté. Plusieurs tables de la loi de Fisher (selon la valeur de \\(\\alpha\\)) se trouvent dans lannexe F. Remarque: Ces tables ne donnent que les quantiles de laile supérieure de la loi \\(F\\), de sorte que pour trouver \\(f_{\\alpha/2; \\, n_1-1,n_2-1}\\) on doit utiliser légalité \\[ f_{\\alpha/2; \\, n_1-1,n_2-1} = \\frac{1}{f_{1-\\alpha/2; \\, n_2-1,n_1-1}} \\] Dans lexemple 6.1, \\({s_{1}^{*}}^2 &gt; {s_{2}^{*}}^2\\) et \\(\\frac{{s_{1}^{*}}^2}{{s_{2}^{*}}^2} = 1.37\\). Selon la table de la loi de Fisher dans lannexe F, au seuil \\(\\alpha=5\\%\\), \\(f_{1-0.025; \\, 24,30}=2.14\\). On trouve que \\(f_0 = \\frac{{s_{1}^{*}}^2}{{s_{2}^{*}}^2}=1.37 &lt; f_{1-0.025; \\, 24,30}=2.14\\) donc on nest pas dans la région de rejet de \\(H_0\\). On ne peut pas donc conclure que les variances des deux échantillons sont différentes. Des deux rapports \\(\\frac{{s_1^*}^2}{{s_2^*}^2}\\) et \\(\\frac{{s_2^*}^2}{{s_1^*}^2}\\), un seul est plus grand que \\(1\\). Or on peut montrer que pour \\(\\alpha &lt; 1/2\\), \\(f_{1-\\alpha; \\, n,m} &gt; 1\\). Donc, pour la règle de décision du rejet de \\(H_0\\), il suffit de retenir celui des deux rapports qui est supérieur à \\(1\\). La procédure pour les tests unilatéraux On peut utiliser la même statistique pour tester les hypothèses alternatives unilatérales. Comme la notation \\(X\\) et \\(Y\\) est arbitraire, on suppose que la population \\(X\\) a la plus grande variance. Donc les hypothèses du test unilatéral sont \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2 &gt; \\sigma_2^2 \\] On rejette donc \\(H_0:\\sigma_1^2=\\sigma_2^2\\) si \\[ f_0 &gt; f_{1-\\alpha; \\, n_1-1,n_2-1} \\] 6.1.2 Tests de comparaison de moyennes Dans cette section, nous introduisons les tests sur les moyennes de deux distributions normales avec variances connues et inconnues. 6.1.2.1 Cas de variances connues Supposons que \\(X_1\\) et \\(X_2\\) sont normalement distribuées ou, si elle ne le sont pas, que les conditions du théorème central limite sappliquent. Comparer les moyennes de deux échantillons, cest tester \\[ H_{0}: m_1 = m_2 \\text { contre } H_{1}: m_1 \\neq m_2 \\] La procédure de ce test repose sur la distribution de la différence des moyennes des échantillons, \\(\\overline{X}_1 - \\overline{X}_2\\). On peut résumer la situation ainsi: Populations \\(X_1 \\thicksim \\mathcal{N}(m_1,\\sigma_1^2)\\) \\(X_2 \\thicksim \\mathcal{N}(m_2,\\sigma_2^2)\\) Echantillons \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) Loi des moyennes empiriques \\(\\overline{X}_1 \\thicksim \\mathcal{N}(m_1,\\sigma_1^2/n_1)\\) \\(\\overline{X}_2 \\thicksim \\mathcal{N}(m_2,\\sigma_2^2/n_2)\\) Loi de la différence des moyennes empiriques \\(\\overline{X}_1 - \\overline{X}_2 \\thicksim \\mathcal{N}\\big(m_1-m_2,\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\big)\\) Donc, si lhypothèse nulle \\(H_0: m_1=m_2\\) est vraie, la statistique du test \\[\\begin{equation} Z_0 = \\frac{\\overline{X}_1 -\\overline{X}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\tag{6.1} \\end{equation}\\] suit la loi \\(\\mathcal{N}(0,1)\\). Critère de rejet de \\(H_{0}\\): Test de \\(H_0: m_1 = m_2\\) contre \\(m_1 \\neq m_2\\): On rejette \\(H_0\\) si \\(|z_0| &gt; z_{1-\\alpha/2}\\). Test de \\(H_0: m_1 \\leq m_2\\) contre \\(m_1 &gt; m_2\\): On rejette \\(H_0\\) si \\(z_0 &gt; z_{1-\\alpha}\\). Test de \\(H_0: m_1 \\geq m_2\\) contre \\(m_1 &lt; m_2\\): On rejette \\(H_0\\) si \\(z_0 &lt; z_{\\alpha}\\). où \\(z_{\\alpha}\\) est le quantile dordre \\(\\alpha\\) de la loi \\(\\mathcal{N}(0,1)\\). 6.1.2.2 Cas de variances inconnues mais égales \\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\) (test de Student) Les variances \\(\\sigma_1^2\\) et \\(\\sigma_2^2\\) étant inconnues, on ne peut pas utiliser directement la statistique de léquation (6.1) pour construire le test. Comme dans ce cas, \\({S_1^*}^2\\) et \\({S_2^*}^2\\) estiment tous les deux la variance commune \\(\\sigma^2\\), on peut les combiner pour obtenir une seule estimation de \\(\\sigma^2\\), soit \\[ S_p^2 = \\frac{(n_1-1){S_1^*}^2 + (n_2-1){S_2^*}^2}{n_1+n_2-2} \\] Pour tester \\(H_0:m_1=m_2\\), on calcule la statistique du test \\[ T_0 = \\frac{\\overline{X}_1 -\\overline{X}_2}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\] Cette statistique, sous \\(H_0\\) suit la loi de Student avec \\(n_1+n_2-2\\) degrés de libertés \\(St(n_1+n_2-2)\\). Critère de rejet de \\(H_{0}\\): Test de \\(H_0: m_1 = m_2\\) contre \\(m_1 \\neq m_2\\): On rejette \\(H_0\\) si \\(|t_0| &gt; t_{1-\\alpha/2; \\, n_1+n_2-2}\\). Test de \\(H_0: m_1 \\leq m_2\\) contre \\(m_1 &gt; m_2\\): On rejette \\(H_0\\) si \\(t_0 &gt; t_{1-\\alpha; \\, n_1+n_2-2}\\). Test de \\(H_0: m_1 \\geq m_2\\) contre \\(m_1 &lt; m_2\\): On rejette \\(H_0\\) si \\(t_0 &lt; t_{\\alpha; \\, n_1+n_2-2}\\). où \\(t_{\\alpha; \\, n}\\) est le quantile dordre \\(\\alpha\\) de la loi \\(St(n)\\). 6.1.2.3 Cas de variances inconnues mais inégales A priori, si le test de Fisher rejette légalité des variances, on ne peut pas appliquer le test sur les moyennes. En fait, le théorème central limite permet de montrer que, si \\(n_1\\) et \\(n_2\\) sont suffisamment grands (supérieurs à 30), alors la loi de \\(T_0\\) est approximativement la loi \\(\\mathcal{N}(0,1)\\) même si les deux variances sont différentes et en fait même si les deux échantillons ne sont pas de loi normale. Par conséquent, si on a beaucoup dobservations, le test de Student permet de comparer les moyennes déchantillons issus de nimporte quelle loi de probabilité. En revanche, si on a peu dobservations, ce test ne fonctionne pas. On utilise alors dautres tests16. La généralisation de ce problème à la comparaison des moyennes de \\(k\\) échantillons gaussiens indépendants fait lobjet dun domaine important de la statistique appelé lanalyse de variance (Anova). 6.2 Comparaison de deux proportions Dans cette section, on supposera que les deux échantillons sont indépendants et de loi de Bernoulli et on comparera leurs paramètres. \\(X_{11},X_{12} \\ldots, X_{1n_{1}}\\) sont indépendantes et de même loi \\(\\mathcal{B}\\left(p_{1}\\right)\\) et \\(X_{21},X_{22} \\ldots, X_{2n_{2}}\\) indépendantes et de même loi \\(\\mathcal{B}\\left(p_{2}\\right) .\\) Les \\(X_{1}\\) et les \\(X_{2}\\) sont indépendantes. On souhaite comparer \\(p_{1}\\) et \\(p_{2}\\), cest-à-dire effectuer des tests du type \\(p_{1} \\leq p_{2}\\) contre \\(p_{1}&gt;p_{2}\\) ou \\(p_{1}=p_{2}\\) contre \\(p_{1} \\neq p_2\\). Comme pour les tests sur une propotion vus dans le chapitre précédent, ce test est valable lorsque \\(n_1\\) et \\(n_2\\) sont suffisamment grands. Suuposons que lapproximation normale de la loi binomiale sapplique à chaque population, de sorte que les estimations des proportions \\(\\hat{p}_1=\\sum_{i=1}^{n_1}X_{1i}/ {n_1}\\) et \\(\\hat{p}_2=\\sum_{i=1}^{n_2}X_{2i}/ {n_2}\\) suivent approximativement les lois normales: \\[ \\hat{p}_1 \\thicksim \\mathcal{N}\\big(p_1, \\frac{p_1(1-p_1)}{n_1}\\big) \\quad \\text{ et } \\quad \\hat{p}_2 \\thicksim \\mathcal{N}\\big(p_2, \\frac{p_2(1-p_2)}{n_2}\\big) \\] Donc, si lhypothèse nulle \\(H_0\\) est vraie, en utilisant le fait que \\(p_1=p_2=p\\), la variable aléatoire \\(\\hat{p}_1-\\hat{p}_2\\) suit approximativement la loi normale: \\[ \\hat{p}_1-\\hat{p}_2 \\thicksim \\mathcal{N}\\big(0, p(1-p) \\big[ \\frac{1}{n_1} + \\frac{1}{n_2} \\big] \\big) \\] Un estimateur du paramètre commun \\(p\\) (quon pourrait appeler la proportion du succès global) est: \\[ \\hat{p} = \\frac{X_1 + X_2}{n_1+n_2} \\] La statistique du test pour \\(H_0: p_1=p_2\\), qui suit une loi \\(\\mathcal{N}(0,1)\\) lorsque \\(H_0\\) est vraie, est alors \\[ Z_0 = \\frac{\\hat{p}_1-\\hat{p}_2}{\\sqrt{ \\hat{p}(1-\\hat{p}) \\big[ \\frac{1}{n_1} + \\frac{1}{n_2} \\big]}} \\] Critère de rejet de \\(H_{0}\\): Test de \\(H_0: p_1 = p_2\\) contre \\(p_1 \\neq p_2\\): On rejette \\(H_0\\) si \\(|z_0| &gt; z_{1-\\alpha/2}\\). Test de \\(H_0: p_1 \\leq p_2\\) contre \\(p_1 &gt; p_2\\): On rejette \\(H_0\\) si \\(z_0 &gt; z_{1-\\alpha}\\). Test de \\(H_0: p_1 \\geq p_2\\) contre \\(p_1 &lt; p_2\\): On rejette \\(H_0\\) si \\(z_0 &lt; z_{\\alpha}\\). où \\(z_{\\alpha}\\) est le quantile dordre \\(\\alpha\\) de la loi \\(\\mathcal{N}(0,1)\\). comme le test de Smith-Satterthwaite ou le test dAspin-Welch "],["exercices-5.html", "Exercices", " Exercices Exercice 6.1 La directrice dune entreprise de mise en boîtes de jus dorange désire comparer la performance de deux chaînes de mise en boîte de son usine. Comme la chaîne \\(1\\) est relativement récente, elle pense quelle produit en moyenne un plus grand nombre de caisses par jour que la chaîne \\(2\\), plus ancienne. On considère que la loi normale est un bon modèle pour cette variable. Dix jours de production sont sélectionnés au hasard pour chaque chaîne. Selon ces données, \\(\\overline{x}_1=824.9\\) caisses par jour et \\(\\overline{x}_2=818.6\\) caisses par jour. On sait par expérience dexploitation de ce type déquipement que \\(\\sigma_1^2=40\\) et que \\(\\sigma_2^2=50\\). Est-ce que la directrice peut favoriser la chaîne \\(1\\) au seuil derreur \\(\\alpha = 5\\%\\)? Exercice 6.2 Une société de production délectricité éolienne, cherche à comparer lefficacité de deux types déoliennes: une éolienne à deux pales (E2p) et une éolienne à trois pales (E3p). Pour ce faire, elle a installé sur un même parc éolien une éolienne de chaque type, et a relevé les puissances de chaque éolienne (en kW) toutes les 10 minutes. Afin de comparer les productions des éoliennes, lingénieur statisticien a prélevé aléatoirement dans la base de données, et ce de façon indépendante pour chaque éolienne, les \\(9\\) puissances (en kW) suivantes: E2p 5 18 19 11 6 19 20 22 17 E3p 2 22 28 12 6 18 29 21 24 Définir les deux variables aléatoires étudiées. Donner une estimation ponctuelle de la puissance moyenne de chaque éolienne. Donner une estimation ponctuelle de la variabilité de la puissance de chaque éolienne. Donner un intervalle de confiance à \\(95\\%\\) à la puissance moyenne de chaque éolienne. Peut-on supposer que les puissances des deux éoliennes ont la même variabilité? Peut-on affirmer, avec un risque derreur de \\(1\\%\\), que la puissance moyenne de léolienne à 3 pales est supérieure à la puissance moyenne de léolienne à 2 pales? Pouvez-vous, avec cette étude, conseiller à la société un type particulier déolienne ? Exercice 6.3 Pour sa fabrication, un industriel utilise des machines de deux constructeurs différents. Après six mois dutilisation, il constate que sur les \\(80\\) machines du type \\(A\\), \\(50\\) ne sont jamais tombées en panne, alors que pour le type \\(B\\) la proportion est de \\(40\\) sur \\(60\\). Peut-on considérer que ces deux types de machines sont équivalents au seuil derreur \\(\\alpha = 5\\%\\)? "],["tests-de-khi-deux-chi2.html", "Chapitre 7 Tests de Khi-deux \\(\\chi^2\\) 7.1 Test dajustement de \\(\\chi^2\\) 7.2 Test dindépendance de \\(\\chi^2\\)", " Chapitre 7 Tests de Khi-deux \\(\\chi^2\\) Nous terminons ce module par une présentation du plus célèbre des tests dhypothèses, le test du \\(\\chi^2\\). Il y a en fait plusieurs variantes de ce test. 7.1 Test dajustement de \\(\\chi^2\\) Les procédures de tests dhypothèses discutées dans les chapitres précédents conviennent pour les problèmes dont on connaît la forme de la fonction de densité de la variable aléatoire, et dont les hypothèses portent sur les paramètres de la distribution. Par contre, il peut arriver quon ne connaisse pas la distribution de probabilité de la variable aléatoire étudiée, disons \\(X\\), et quon désire tester lhypothèse que \\(X\\) suit une loi de probabilité particulière. On veut, par exemple, tester lhypothèse que \\(X\\) suit la loi normale. Un graphique quantile-quantile (qqplot) peut nous donner une première idée de la réponse à cette question. Dans cette section, on décrira une procédure formelle de test dajustement basée sur la loi du \\(\\chi^2\\). La procédure du test La procédure du test requiert un échantillon aléatoire de taille \\(n\\) de la variable aléatoire \\(X\\), dont la fonction de densité de probabilité est inconnue. Ces \\(n\\) observations sont disposées dans un tableau de fréquences ayant \\(k\\) classes. Soit \\(O_{i}\\) la fréquence observée dans la \\(i\\)-ième classe. A laide de la distribution hypothétique de probabilité, on calcule la fréquence espérée dans la \\(i\\)-ième classe \\(E_{i}\\). (Notons que les fréquences espérées ne sont pas nécessairement des nombres entiers, et quelles nont pas à être arrondies puisquil sagit despérances). Pour tester les hypothèses \\[ \\begin{aligned} {H}_{0} &amp;: \\text { La loi proposée est un bon modèle pour cette variable; } \\\\ {H}_{1} &amp;: \\text { La loi proposée n&#39;est pas un bon modèle pour cette variable,} \\end{aligned} \\] La statistique du test est \\[ U_{0}= n \\sum_{i=1}^{k} \\frac{\\left(O_{i}-E_{i}\\right)^{2}}{E_{i}} = \\sum_{i=1}^{k} \\frac{\\left(N_{i}-n p_{i}\\right)^{2}}{np_{i}} \\] Cette statistique peut être considérée comme une distance entre le modèle proposé et les quantités observées. On peut démontrer que \\(U_{0}\\) suit approximativement la loi du \\(\\chi^2\\) avec \\(k-p-1\\) degrés de liberté, où \\(p\\) représente le nombre de paramètres de la distribution hypothétique estimée à partir de léchantillon. Cette approximation saméliore lorsque \\(n\\) augmente. Critère de rejet de \\(H_0\\) du test dajustement du \\(\\chi^2\\) On rejette lhypothèse que \\(X\\) suit la distribution hypothétique si \\(u_0 &gt; \\chi^2_{1-\\alpha,k-p-1}\\) 7.2 Test dindépendance de \\(\\chi^2\\) Les tests dhypothèses permettent également de vérifier, avec une certaine probabilité derreur, bien sûr, sil existe une interaction entre deux variables catégoriques (e.g. discrètes, qualitatives), ou si plusieurs populations présentent les mêmes proportions dans les diverses catégories dune variable. Nous discutons dans cette section le test dindépendance de deux variables qualitatives. La procédure du test On peut souvent classer les \\(n\\) éléments dun échantillon suivant deux critères différents, cest-à-dire selon les valeurs de deux variables aléatoires discrètes ou qualitatives. Il est alors intéressant de savoir si les deux variables sont statistiquement indépendantes. On peut, par exemple, considérer la population des ingénieurs du pays et vouloir déterminer si le salaire de départ est indépendant de la discipline étudiée. On considère dans ce paragraphe deux variables qualitatives observées simultanément sur \\(n\\) individus. On suppose que la première, notée \\(X,\\) possède \\(r\\) modalités notées \\(x_{1}, \\ldots, x_{\\ell}, \\ldots, x_{r},\\) et que la seconde, notée \\(Y,\\) possède \\(c\\) modalités notées \\(y_{1}, \\ldots, y_{h}, \\ldots, y_{c}\\). Ces données sont présentées dans un tableau à double entrée, appelé tables de contingence, dans lequel on dispose les modalités de \\(X\\) en lignes et celles de \\(Y\\) en colonnes. Ce tableau est donc de dimension \\(r \\times c\\) et a pour élément générique le nombre \\(n_{\\ell h}\\) dobservations conjointes des modalités \\(x_{\\ell}\\) de \\(X\\) et \\(y_{h}\\) de \\(Y\\); les quantités \\(n_{\\ell h}\\) sont appelées les effectifs conjoints. Une table de contingence se présente donc sous la forme suivante: \\(y_1\\) \\(\\ldots\\) \\(y_h\\) \\(\\ldots\\) \\(y_c\\) sommes \\(x_1\\) \\(n_{11}\\) \\(n_{1h}\\) \\(n_{1c}\\) \\(n_{1\\bullet}\\) \\(\\vdots\\) \\(x_{\\ell}\\) \\(n_{\\ell 1}\\) \\(n_{\\ell h}\\) \\(n_{\\ell c}\\) \\(n_{\\ell \\bullet}\\) \\(\\vdots\\) \\(x_r\\) \\(n_{r 1}\\) \\(n_{r h}\\) \\(n_{rc}\\) \\(n_{r\\bullet}\\) sommes \\(n_{\\bullet 1}\\) \\(n_{\\bullet h}\\) \\(n_{\\bullet c}\\) \\(n\\) Les quantités \\(n_{\\ell \\bullet}(\\ell=1, \\ldots, r)\\) et \\(n_{\\bullet h}(h=1, \\ldots, c)\\) sont appelées les effectifs marginaux; ils sont définis par \\(n_{\\ell \\bullet}=\\sum_{h=1}^{c} n_{\\ell h}\\) et \\(n_{\\bullet h}=\\sum_{\\ell=1}^{r} n_{\\ell h}\\) et ils vérifient \\(\\sum_{\\ell=1}^{r} n_{\\ell \\bullet}=\\sum_{h=1}^{c} n_{\\bullet h}=n\\). De façon analogue, on peut définir les notions de fréquences conjointes et de fréquences marginales. On désire tester lhypothèse que les variable \\(X\\) et \\(Y\\) sont indépendantes: \\[ \\begin{aligned} H_{0}&amp;: X \\text { et } Y \\text { sont indépendantes; } \\\\ H_{1}&amp;: X \\text { et } Y \\text { sont dépendantes. } \\end{aligned} \\] Si lon rejette cette hypothèse nulle, on conclut quil existe une certaine interaction entre les deux critères de classement. Les procédures de test exactes sont difficiles à obtenir, mais on présente ici une statistique de test approximative qui est valide pour les grandes tailles déchantillon. La statistique Khi-deux Il est courant en statistique de comparer une table de contingence observée, deffectif conjoint générique \\(n_{\\ell h},\\) à une table de contingence donnée a priori (et appelée standard), deffectif conjoint générique \\(s_{\\ell h},\\) en calculant la quantié \\[ \\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-s_{\\ell h}\\right)^{2}}{s_{\\ell h}} \\] De façon naturelle, pour mesurer la liaison sur une table de contingence, on utilise donc lindice appelé khi-deux (chi-square) et défini comme suit: \\[ U_0 = \\chi^{2}=\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-\\frac{n_{\\ell\\bullet} n_{\\bullet h}}{n}\\right)^{2}}{\\frac{n_{\\ell\\bullet} n_{\\bullet h}}{n}}=n\\left[\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{n_{\\ell h}^{2}}{n_{\\ell\\bullet} n_{\\bullet h}}-1\\right] \\thicksim \\chi^2_{(r-1)(c-1)} \\] Comme dans le cas du test dajustement du \\(\\chi^2\\) , on peut interpréter la statistique \\(U_0\\) comme une distance entre le modèle dindépendance hypothétique et les fréquences observées. Si cette distance est trop grande, il faut rejeter le modèle. Critère de rejet de \\(H_0\\) de lindépendance de deux variables qualitatives On rejette lhypothèse dindépendance si \\(u_0 &gt; \\chi^2_{1-\\alpha,(r-1)(c-1)}\\) "],["exercices-6.html", "Exercices", " Exercices Exercice 7.1 Un informaticien a développé un algorithme pour générer des nombres entiers pseudo-aléatoires de manière uniforme sur lintervalle \\([0,9]\\). Il code lalgorithme et génère \\(1000\\) nombres pseudo-aléatoires. Les données sur la fréquence dapparition de chacun des chiffres de \\(0\\) à \\(9\\) sont indiquées dans le tableau suivant. Chiffre généré Effectif observé, \\(N_i\\) Fréquence observée, \\(O_i\\) 0 94 0.09 1 93 0.09 2 112 0.11 3 101 0.10 4 104 0.10 5 95 0.10 6 100 0.10 7 99 0.10 8 108 0.11 9 94 0.09 \\(n\\) 1000 1.00 Le générateur de nombres aléatoires fonctionne-t-il correctement au seuil de \\(5\\%\\)? Exercice 7.2 On souhaite tester lhypothèse selon laquelle le nombre de défauts sur des cartes de circuit imprimé suit une loi de Poisson. On collecte un échantillon aléatoire de \\(n=60\\) cartes de circuit imprimé et on observe le nombre de défauts. On obtient les données suivantes: Nombre de défauts Effectif observé 0 32 1 15 2 9 3 4 Peut-on dire que le nombre de défauts suit une loi de Poisson? (Remarque: le paramètre \\(\\lambda\\) de la loi de Poisson est à estimer) Exercice 7.3 Une entreprise doit choisir un régime de pension parmi trois. La direction désire savoir si la préférence pour un régime est indépendante de la catégorie demploi au seuil de \\(5\\%\\). Le tableau suivant donne les opinions dun échantillon de \\(500\\) employés. \\(X_2 =\\) Régime de pension \\(X_1 =\\) Catégories d'emploi 1 2 3 Total 1: Cadres 160 140 40 340 2: Employés payés à l'heure 40 60 60 160 Total 200 200 100 500 Tester lindépendance entre la catégorie demploi et la préférence pour un régime. "],["tp.html", "TP", " TP Lexemple dans ce TP est un exemple historique dû à Fisher. Il étudie la couleur des cheveux de garçons et de filles dun district écossais, il obtient les données suivantes: Blond Roux Châtain Brun Noir de jais Garçon 592 119 849 504 36 Fille 544 97 677 451 14 Nous souhaitons savoir si la couleur des cheveux est indépendante du sexe avec une erreur de première espèce \\(5\\%\\). Si la couleur dépend du sexe, nous aimerions avoir une idée des couleurs qui sont les plus dépendantes du sexe. Saisir les données. Visualiser les données. (On pourra faire des diagrammes en bâtons) Construire le test du \\(\\chi^2\\) dindépendance manuellement, en calculant les profils lignes et les profils colonnes, la statistique du test et la valuer critique afin de prendre une décision. Que concluez vous? Retrouver les résultats du test en construisant le test du \\(\\chi^2\\) dindépendance avec la fonction chisq.test(). Calculer les contributions au \\(\\chi^2\\) pour savoir qui sont les combinaisons qui contribuent le plus à la non-dépendance des deux variables. Les contributions à la statistique \\(U_0\\) du \\(\\chi^2\\) sont les \\(\\frac{\\left(O_{i}-E_{i}\\right)^{2}}{E_{i}}\\). Les racines carrés de ces contributions sont dans lobjet residuals du test du \\(\\chi^2\\) réalisé. Le total est contenu dans lobjet statistic du test. "],["variables-aléatoires-discrètes.html", "Variables Aléatoires Discrètes Notions de Probabilités Notion de variable aléatoire réelle (v.a.r.) Variables aléatoires discrètes Moments dune variable aléatoire discrète Couple de variables aléatoires discrètes Lois usuelles discrètes", " Variables Aléatoires Discrètes Notions de Probabilités Espace Probabilisable Exemple fondamental: Considérons le jeu du lancé dun dé. Expérience aléatoire \\(\\varepsilon\\) : lancer un dé équilibré \\(\\longleftarrow\\) Action. Univers: lensemble de tous les résultats possibles de cette expérience aléatoire \\[\\Omega= \\{1,2,3,4,5,6\\}\\] Evénements: Dans cette expérience aléatoire, on peut sintéresser à des événements plus complexes quun simple résultat élémentaire. Lensemble de parties de \\(\\Omega\\), appelé \\(\\mathcal{P}({\\Omega})\\), est lensemble des sous-ensembles de \\(\\Omega\\). Une famille \\(\\mathcal{A}\\) de parties (i.e. de sous ensembles) de \\(\\Omega\\). Ces parties sont appelées des événements. On dit que lévénement \\(A\\) sest réalisé si et seulement si le résultat \\(\\Omega\\) de \\(\\Omega\\) qui sest produit appartient à \\(A\\). Tribu: On appelle tribu sur \\(\\Omega\\), toute famille \\(\\mathcal{A}\\) de parties de \\(\\Omega\\) vérifiant: \\(\\Omega \\in \\mathcal{A}\\). si \\(A \\in \\mathcal{A}\\), alors \\(\\bar{A} \\in \\mathcal{A}\\). si \\((A_n)_{n\\in\\mathbb{N}}\\) est une suite déléments de \\(\\mathcal{A}\\), alors \\(\\bigcup\\limits_{n\\in\\mathbb{N}} A_n \\in \\mathcal{A}\\). \\(({\\Omega},\\mathcal{A})\\) est un espace probibilisable. Notions sur les Evénements Soit \\(({\\Omega},\\mathcal{A})\\) un espace probibilisable: Lensemble \\(\\mathcal{A}\\) est appelé tribu des événements. Les éléments de \\(\\mathcal{A}\\) sappellent les événements. Lévénement \\(\\Omega\\) est appelé événement certain. Lévénement \\(\\emptyset\\) est appelé événement impossible. Opérations sur les événements. Soient \\(A\\) et \\(B\\) deux événements: \\(\\bar{A}\\) est lévénement contraire de \\(A\\) (on note aussi \\(A^c\\)). \\(\\bar{A}={\\Omega}\\setminus A\\). \\(\\bar{A}\\) se réalise si et seulement si \\(A\\) ne se réalise pas. \\(A\\, {\\cap} \\,B\\) est lévénement &lt;&lt;\\(A\\) et \\(B\\)&gt;&gt;. \\(A\\, {\\cap} \\,B\\) se réalise lorsque les deux événements se réalisent. \\(A {\\cup} B\\) est lévénement &lt;&lt;\\(A\\) ou \\(B\\)&gt;&gt;. \\(A {\\cup} B\\) se réalise lorsque au moins un des deux événements se réalise. Incompatibilité: \\(A\\) et \\(B\\) sont incompatibles si leur réalisation simultanée est impossible: \\(A \\cap B = \\emptyset\\). Implication: \\(A\\) implique \\(B\\) signifie que si \\(A\\) se réalise, alors \\(B\\) se réalise aussi: \\(A \\subset B\\). Espace Probabilisé Soit \\(({\\Omega},\\mathcal{A})\\) un espace probabilisable. On appelle probabilité sur \\(({\\Omega},\\mathcal{A})\\), toute application \\[P : \\mathcal{A} \\rightarrow \\mathbb{R}\\] vérifiant: \\(\\forall A \\in \\mathcal{A}, P(A) \\geq 0\\). \\(P({\\Omega})=1\\). \\(\\forall (A_n)_{n\\in\\mathbb{N}^*} \\in \\mathcal{A}^{\\mathbb{N}^*}\\), une suite déléments de \\(\\mathcal{A}\\) deux à deux incompatibles, on a: \\[P(\\bigcup\\limits_{n\\in\\mathbb{N}^*} A_n) = \\sum_{n=1}^{+\\infty} P(A_n)\\] Le triplet \\(({\\Omega},\\mathcal{A},P)\\) est appelé espace probabilisé. Probabilité: Propriétés \\(P(\\emptyset) = 0\\). \\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )-P(A_1 \\cap A_2 )\\). Si \\(A_1\\) et \\(A_2\\) sont incompatibles, \\(A_1 \\cap A_2 = \\emptyset\\), \\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )\\). \\(P(A_1 \\cup A_2 \\cup A_3 ) = P(A_1 ) + P(A_2 ) + P(A_3 ) - P(A_1 \\cap A_2 ) - P(A_1 \\cap A_3 ) - P(A_2 \\cap A_3 )+P(A_1 \\cap A_2 \\cap A_3 )\\). \\(P(\\bar{A}) = 1-P(A)\\). \\(P(B\\setminus A)=P(B)-P(B\\cap A)\\). \\(A \\subset B \\Rightarrow P(A) \\leq P(B)\\). Probabilité uniforme sur \\(\\Omega\\) fini Soit \\(\\Omega\\) un univers fini. On dit que \\(P\\) est la probabilité uniforme sur lespace probabilisable \\(({\\Omega},P({\\Omega}))\\) si: \\[\\forall {\\omega},{\\omega}&#39; \\in {\\Omega}, \\quad \\quad P(\\{{\\omega}\\})=P(\\{{\\omega}&#39;\\})\\] On dit aussi quil y a équiprobabilité des événements élémentaires. Soit \\(({\\Omega}, \\mathcal{P}({\\Omega}), P)\\) un espace probabilisé fini. Si \\(P\\) est la probabilité uniforme, alors \\[\\forall A \\in \\mathcal{A}, \\quad \\quad P(A)=\\frac{Card(A)}{Card({\\Omega})}\\] Probabilité conditionnelle Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé et \\(B \\in \\mathcal{A}\\) tel que \\(P(B) &gt; 0\\). Lapplication \\(P_B\\) définie sur \\(\\mathcal{A}\\) par: \\[P_B(A) = P(A|B) =\\frac{P(A\\cap B)}{P(B)}, \\quad \\quad \\forall A \\in \\mathcal{A}\\] est une probabilité sur \\(({\\Omega}, \\mathcal{A})\\); elle est appelée la probabilité conditionnelle sachant \\(B\\). Cest la probabilité pour que lévénement \\(A\\) se produise sachant que lévénement \\(B\\) sest produit. Remarque: \\((A|B)\\) nest pas un événement! On utilise la notation \\(P(A|B)\\) par simplicité, mais cest \\(P_B (A)\\) qui est correcte. Formule des probabilités composées: \\[P(A\\cap B) = P(A|B)P(B) = P(B|A)P(A)\\] Formule des probabilités totales: \\(\\forall A \\in \\mathcal{A}, \\quad P(A) = P(A \\cap B) + P(A \\cap \\bar{B} )\\) On appelle système complet dévénements (SCE), toute partition dénombrable de \\(\\Omega\\) formée déléments de \\(A\\); c-à-d tout ensemble dénombrable dévénements, deux à deux incompatibles et dont lunion dénombrable est lévénement certain. Soit \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\). On a: \\[\\forall A \\in \\mathcal{A},\\quad \\quad P(A)=\\sum_{n\\geq 0} P(A \\cap B_n)\\] Indépendance: Les événement \\(A\\) et \\(B\\) sont indépendants ssi \\(P(A\\cap B)=P(A)P(B)\\). Formule de Bayes Première formule de Bayes Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé. Pour tous événements \\(A\\) et \\(B\\) tels que \\(P(A) \\neq 0\\) et \\(P(B) \\neq 0\\), on a: \\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\] Deuxième formule de Bayes Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé et \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\) t.q. pour tout \\(n\\geq 0 \\,\\, P(B_n)\\neq 0\\). On a pour tout \\(A \\in \\mathcal{A}\\) t.q. \\(P(A)\\neq 0\\) \\[P(B_i|A) = \\frac{P(A|B_i) P(B_i)}{\\sum_{n\\geq 0} P(A|B_n) P(B_n)} \\quad \\quad \\forall i \\geq 0\\] Notion de variable aléatoire réelle (v.a.r.) Après avoir réalisé une expérience aléatoire, il arrive bien souvent quon sintéresse plus à une fonction du résultat quau résultat lui-même. Expliquons ceci au moyen des exemples suivants: lorsquon joue au dés, certains jeux accordent de limportance à la somme obtenue sur deux dés, 7 par exemple, plutôt quà la question de savoir si cest la paire (1,6) qui est apparue, ou (2,5), (3,4), (4,3), (5,2) ou plutôt (6,1). Dans le cas du jet dune pièce, il peut être plus intéressant de connaître le nombre de fois où le côté pile est apparue plutôt que la séquence détaillée des jets pile et face. Ces grandeurs auxquelles on sintéresse sont en fait des fonctions réelles définies sur lensemble fondamental et sont appelées variables aléatoires. Du fait que la valeur dune variable aléatoire est déterminée par le résultat de lexpérience, il est possible dattribuer une probabilité aux différentes valeurs que la variable aléatoire peut prendre. Soient \\(\\varepsilon\\) une expérience aléatoire et \\((\\Omega,\\mathcal{A},P)\\) un espace probabilisé lié à cette expérience. Dans de nombreuses situations, on associe à chaque résultat \\(\\omega \\in \\Omega\\) un nombre réel noté \\(X(\\omega)\\); on construit ainsi une application \\(X : \\Omega \\rightarrow \\mathbb{R}\\). Historiquement, \\(\\varepsilon\\) était un jeu et \\(X\\) représentait le gain du joueur. Exemple: Un joueur lance un dé équilibré à 6 faces numérotées de 1 à 6, et on observe le numéro obtenu. Si le joueur obtient 1, 3 ou 5, il gagne 1 euro. Sil obtient 2 ou 4, il gagne 5 euros. Sil obtient 6, il perd 10 euros. Selon lexpérience aléatoire (lancer dun dé équilibré) lensemble fondamental est \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{A} = \\mathcal{P}(\\Omega)\\) et \\(P\\) léquiprobabilité sur \\((\\Omega,\\mathcal{A})\\). Soit \\(X\\) lapplication de \\(\\Omega\\) dans \\(\\mathbb{R}\\) qui à tout \\(\\omega \\in \\Omega\\) associe le gain correspondant. On a donc \\(X(1) = X(3) = X(5) = 1\\) \\(X(2) = X(4) = 5\\) \\(X(6) = -10\\) On dit que \\(X\\) est une variable aléatoire sur \\(\\Omega\\). On peut sintéresser à la probabilité de gagner 1 euro, cest-à-dire davoir \\(X(\\omega) = 1\\), ce qui se réalise si et seulement si \\(\\omega \\in \\{1,3,5\\}\\). La probabilité cherchée est donc égale à \\(P(\\{1,3,5\\}) = 1/2\\). On écrira aussi \\(P(X=1) = 1/2\\). On pourra donc considérer lévénement : \\[\\{X=1\\} = \\{\\omega \\in \\Omega / X(\\omega) = 1\\} = \\{\\omega \\in \\Omega / X(\\omega) \\in \\{1\\}\\} = X^{-1} (\\{1\\}) = \\{1,3,5\\}.\\] On aura du même \\(P(X=5) = 1/3\\) et \\(P(X=-10) = 1/6\\). Ce que lon peut présenter dans un tableau \\(x_i\\) \\(-10\\) \\(1\\) \\(5\\) \\(p_i=P(X = x_i)\\) \\(1/6\\) \\(1/2\\) \\(1/3\\) Cela revient à considérer un nouvel ensemble dévénements élémentaires: \\[\\Omega_X = X(\\Omega)= \\{-10,1,5\\}\\] et à munir cet ensemble de la probabilité \\(P_X\\) définie par le tableau des \\(P(X=x_i)\\) ci dessus. Cette nouvelle probabilité sappelle loi de la variable aléatoire X. Remarquer que \\[P(\\bigcup_{x_i \\in \\Omega_X} \\{X=x_i\\}) = \\sum_{x_i \\in \\Omega_X} P(X=x_i) = 1\\] Dans ce chapitre, nous traitons le cas où \\(X(\\Omega)\\) est dénombrable. La variable aléatoire est alors dite discrète. Sa loi de probabilité, qui peut être toujours définie par sa fonction de répartition, le sera plutôt par les probabilités individuelles. Nous définirons les deux caractéristiques numériques principales dune variable aléatoire discrète, lespérance caractéristique de valeur centrale, et la variance, caractéristique de dispersion. Nous définirons aussi les couples de variables aléatoires. Variables aléatoires discrètes Définition, loi de probabilité Définition 7.1 On dit quune variable aléatoire réelle (v.a.r.) \\(X\\) est discrète (v.a.r.d.) si lensemble des valeurs que prend \\(X\\) est fini ou infini dénombrable. Si on suppose \\(X(\\Omega)\\) lensemble des valeurs de \\(X\\) qui admet un plus petit élément \\(x_1\\). Alors la v.a.r.d. \\(X\\) est entièrement définie par: Lensemble \\(X(\\Omega)\\) des valeurs prises par \\(X\\), rangées par ordre croissant: \\(X(\\Omega) = \\{x_1, x_2,\\ldots,x_i,\\ldots\\}\\) avec \\(x_1 \\leq x_2 \\leq \\ldots \\leq x_i \\leq \\ldots\\). La loi de probabilité définie sur \\(X(\\Omega)\\) par \\[p_i = P(X=x_i) \\,\\,\\,\\,\\, \\forall \\,\\, i=1,2,\\ldots\\] Remarques: Soit \\(B\\) un ensemble de \\(\\mathbb{R}\\), \\[P(X \\in B) = \\sum_{i / x_i \\in B} p(x_i)\\] En particulier \\[P( a &lt; X \\leq b) = \\sum_{i / a &lt; x_i \\leq b} p(x_i)\\] Bien sûr tous les \\(p(x_i)\\) sont positives et \\(\\sum_{i=1}^{\\infty} p(x_i) =1\\). Si \\(X\\) ne prend quun petit nombre de valeurs, cette loi est généralement présentée dans un tableau. Fonction de répartition dune variable aléatoire discrète Définition 7.2 On appelle fonction de répartition de la v.a. \\(X\\), quon note \\(F(a)\\) de la v.a.r.d. \\(X\\), ou \\(F_X(a)\\), la fonction définie pour tout réel \\(a\\), \\(-\\infty &lt; a &lt; \\infty\\), par \\[F(a)=P(X \\leq a)=\\sum_{i / x_{i}\\leq a} P(X=x_{i})\\] Cette valeur représente la probabilité de toutes les réalisations inférieures ou égales au réel \\(a\\). Propriétés: Voici quelques propriétés de cette fonction: Cest une fonction en escalier (constante par morceaux). \\(F(a) \\leq 1\\) car cest une probabilité. \\(F(a)\\) est continue à droite. \\(\\lim\\limits_{a\\to - \\infty} F(a) = 0\\) et \\(\\lim\\limits_{a\\to\\infty} F(a) = 1\\) La fonction de répartition caractérise la loi de \\(X\\), autrement dit: \\(F_{X} = F_{Y}\\) si et seulement si les variables aléatoires \\(X\\) et \\(Y\\) ont la même loi de probabilité. Fonction de répartition et probabilités sur \\(X\\) Tous les calculs de probabilité concernant \\(X\\) peuvent être traités en termes de fonction de répartition. Par exemple, \\[P(a &lt; X \\leq b) = F(b) - F(a) \\quad \\quad \\text{pour tout } a &lt; b\\] On peut mieux sen rendre compte en écrivant \\(\\{X \\leq b\\}\\) comme union des deux événements incompatibles \\(\\{X \\leq a\\}\\) et \\(\\{ a &lt; X \\leq b\\}\\), soit \\[\\{X \\leq b\\} = \\{X \\leq a\\} \\cup \\{ a &lt; X \\leq b\\}\\] et ainsi \\[P(X \\leq b) = P(X \\leq a) + P(a &lt; X \\leq b)\\] ce qui établit légalité ci dessus. On peut déduire de \\(F\\) les probabilités individuelles par: \\[p_{i}=F(x_{i})-F(x_{i-1})\\quad \\quad \\text{pour } 1 \\leq i \\leq n\\] Exemple: On joue trois fois à pile ou face. Soit \\(X\\) la variable aléatoire nombre de pile obtenus. Ici \\(\\Omega=\\{P, F\\}^3\\), et donc \\[X(\\Omega)=\\{0, 1, 2, 3\\}.\\] On a \\(card(\\Omega)=2^3=8\\). Calculons par exemple \\(P(X=1)\\), cest à dire la probabilité davoir exactement une pile. \\[X^{-1}(1)=\\{(P, F, F), (F, P, F), (F, F, P) \\}\\] Doù \\(P(X=1)=\\frac{3}{8}\\). En procédant de la même façon, on obtient la loi de probabilité de \\(X\\): \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(k\\) \\(P(X = k)\\) \\(\\frac{1}{8}\\) \\(\\frac{3}{8}\\) \\(\\frac{3}{8}\\) \\(\\frac{1}{8}\\) La fonction de répartition de \\(X\\) est donc donnée par: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 1/8 &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1/2 &amp; \\quad \\text{si $1 \\leq x &lt; 2$}\\\\ 7/8 &amp; \\quad \\text{si $2 \\leq x &lt; 3$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 3$}\\\\ \\end{array} \\right.\\] Le graphe de cette dernière est représentée dans la figure suivante: Exemple: Soit \\(A\\) un événement quelconque. On appelle variable aléatoire indicatrice de cet événement \\(A\\), la variable aléatoire définie par: \\[X(\\omega) = \\left\\{ \\begin{array}{l l} 1 &amp; \\quad \\text{si $\\omega \\in A$}\\\\ 0 &amp; \\quad \\text{si $\\omega \\in \\bar{A}$}\\\\ \\end{array} \\right.\\] et notée \\(X=1_A\\). Ainsi: \\[P(X=1)=P(A)=p\\] \\[P(X=0)=P(\\bar{A})=1-p\\] La fonction de répartition de \\(X\\) est donc donnée par: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 1-p &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}\\\\ \\end{array} \\right.\\] On peut prendre par exemple le cas dun tirage dune boule dans une urne contenant 2 boules blanches et 3 boules noires. Soit \\(A:\\text{&quot;obtenir une boule blanche&quot;}\\) et \\(X\\) la variable indicatrice de \\(A\\). La loi de probabilité de \\(X\\) est alors \\(k\\) \\(0\\) \\(1\\) \\(P(X = k)\\) \\(\\frac{3}{5}\\) \\(\\frac{2}{5}\\) et sa fonction de répartition est: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 3/5 &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}\\\\ \\end{array} \\right.\\] Moments dune variable aléatoire discrète Espérance mathématique Définition 7.3 Pour une variable aléatoire discrète \\(X\\) de loi de probabilité \\(p(.)\\), on définit lespérance de \\(X\\), notée \\(E(X)\\), par lexpression \\[E(X)=\\sum_{i \\in \\mathbb{N}} x_{i} p(x_i)\\] En termes concrets, lespérance de \\(X\\) est la moyenne pondérée des valeurs que \\(X\\) peut prendre, les poids étant les probabilités que ces valeurs soient prises. Reprenons lexemple où on joue 3 fois à pile ou face. Lespérance de \\(X=\\)nombre de pile obtenus est égal à: \\[E(X)=0 \\times \\frac{1}{8}+1 \\times \\frac{3}{8}+2 \\times \\frac{3}{8}+3 \\times \\frac{1}{8}=1.5\\] Dans le cas de la loi uniforme sur \\(X(\\Omega)=\\{x_{1},\\ldots, x_{k}\\}\\), cest à dire avec équiprobabilité de toutes les valeurs \\(p_{i}=1/k\\), on obtient: \\[E(X)=\\frac{1}{k} \\sum_{i=1}^k x_{i}\\] et dans ce cas \\(E(X)\\) se confond avec la moyenne arithmétique simple \\(\\bar{x}\\) des valeurs possibles de \\(X\\). Pour le jet dun dé équilibré par exemple: \\[E(X)=\\frac{1}{6} \\sum_{i=1}^6 i=\\frac{7}{2}=3.5\\] Espérance dune fonction dune variable aléatoire Théorème 7.1 (Théorème du transfert) Si X est une variable aléatoire discrète pouvant prendre ses valeurs parmi les valeurs \\(x_i\\), \\(i \\geq 1\\), avec des probabilités respectives \\(p(x_i)\\), alors pour toute fonction réelle \\(g\\) on a \\[E(g(X)) = \\sum_i g(x_i)p(x_i)\\] Exemple 7.1 Soit \\(X\\) une variable aléatoire qui prend une des trois valeurs \\(\\{-1,0,1\\}\\) avec les probabilités respectives \\[P(X=-1) = 0.2 \\quad \\quad P(X=0)=0.5 \\quad \\quad P(X=1) = 0.3\\] Calculer \\(E(X^2)\\). Solution: Première approche: Soit \\(Y=X^2\\). La distribution de \\(Y\\) est donnée par \\[\\begin{aligned} P(Y=1) &amp;= P(X=-1) + P(X=1) = 0.5 \\\\ P(Y=0) &amp;= P(X=0) = 0.5 \\end{aligned}\\] Donc \\[E(X^2)=E(Y) = 1(0.5) + 0(0.5) = 0.5\\] Deuxième approche: En utilisant le théorème \\[\\begin{aligned} E(X^2) &amp;= (-1)^2(0.2) + 0^2(0.5) + 1^2 (0.3) \\\\ &amp;= 1(0.2+0.3)+0(0.5)=0.5 \\end{aligned}\\] Remarquer que \\[0.5=E(X^2) \\neq (E(X))^2 = 0.01\\] Linéarité de lespérance Propriétés de lespérance \\(E(X+a)=E(X)+a, \\quad a \\in \\mathbb{R}\\) résultat qui se déduit de: \\[\\sum_{i}p_{i}(x_{i}+a)= \\sum_{i}p_{i}x_{i}+\\sum_{i}ap_{i}=\\sum_{i}p_{i}x_{i}+a \\sum_{i}p_{i}=\\sum_{i}p_{i}x_{i}+a\\] \\(E(aX)=aE(X), \\quad a\\in \\mathbb{R}\\) il suffit décrire: \\[\\sum_{i}p_{i}a x_{i}=a\\sum_{i}p_{i}x_{i}\\] \\(E(X+Y)=E(X)+E(Y)\\), \\(X\\) et \\(Y\\) étant deux variables aléatoire. On peut résumer ces trois propriétés en disant que lespérance mathématique est linéaire: \\[E(\\lambda X + \\mu Y)= \\lambda E(X)+\\mu E(Y), \\quad \\forall \\lambda \\in \\mathbb{R}, \\, \\forall \\mu \\in \\mathbb{R}.\\] Variance Définition 7.4 La variance est un indicateur mesurant la dispersion des valeurs \\(x_{i}\\) que peut prendre la v.a. \\(X\\) et son espérance \\(E(X)\\). On appelle variance de X, que lon note \\(V(X)\\), la quantité \\[V(X)=E\\big[ (X-E(X))^2 \\big]\\] lorsque cette quantité existe. Cest lespérance mathématique du carré de la v.a. centrée \\(X-E(X)\\). On peut établir une autre formule pour le calcul de \\(V(X)\\): \\[V(X)=E(X^2)-E^2(X)\\] Or: \\[\\begin{aligned} V(X)&amp;= E\\left[X^2-2XE(X)+E^2(X)\\right] \\\\ &amp;=E(X^2)-E[2XE(X)]+ E[E^2(X)]\\\\ &amp;=E(X^2)-2E^2(X)+E^2(X) \\\\ &amp;=E(X^2)-E^2(X) \\end{aligned}\\] On cherche \\(V(X)\\) où \\(X\\) est le nombre obtenu lors du jet dun dé équilibré. On a vu dans lexemple que \\(E(X) = \\frac{7}{2}\\). De plus, \\[\\begin{aligned} E(X^2) &amp;= 1^2 \\bigg(\\frac{1}{6}\\bigg) + 2^2 \\bigg(\\frac{1}{6}\\bigg) + 3^2 \\bigg(\\frac{1}{6}\\bigg) + 4^2 \\bigg(\\frac{1}{6}\\bigg) + 5^2 \\bigg(\\frac{1}{6}\\bigg) + 6^2 \\bigg(\\frac{1}{6}\\bigg) \\\\ &amp;=\\bigg(\\frac{1}{6}\\bigg) (91) = \\frac{91}{6}.\\end{aligned}\\] Et donc \\[V(X) = \\frac{91}{6} - \\bigg(\\frac{7}{2}\\bigg)^2 = \\frac{35}{12}\\] Propriétés de la variance \\(V(X) \\geq 0\\) \\(V(X+a)=V(X)\\) en effet: \\[\\begin{aligned} V(X+a) &amp;= E\\big[\\left[X+a-E(X+a)\\right]^2\\big] \\\\ &amp;=E\\big[\\left[X+a-E(X)-a\\right]^2\\big] \\\\ &amp;=E\\big[\\left[X-E(X)\\right]^2\\big] \\\\ &amp;=V(X). \\end{aligned}\\] \\(V(aX)=a^2V(X)\\) en effet: \\[\\begin{aligned} V(aX) &amp;= E\\big[\\left[aX-E(aX)\\right]^2\\big] \\\\ &amp;=E\\big[\\left[aX-aE(X)\\right]^2\\big] \\\\ &amp;=E\\big[a^2\\left[X-E(X)\\right]^2\\big] \\\\ &amp;=a^2\\big[E\\left[X-E(X)\\right]^2\\big] \\\\ &amp;= a^2V(X). \\end{aligned}\\] Ecart-type Définition 7.5 La racine carrée de \\(V(X)\\) est appelée lécart-type de \\(X\\), qui se note \\(\\sigma_{X}\\). On a \\[\\sigma_{X} = \\sqrt{V(X)}\\] \\(\\sigma_{X}\\) sexprime dans les mêmes unités de mesure que la variable aléatoire \\(X\\). A noter: Lécart type sert à mesurer la dispersion dun ensemble de données. Plus il est faible, plus les valeurs sont regroupées autour de la moyenne. Exemple: La répartition des notes dune classe. Plus lécart type est faible, plus la classe est homogène. Lespérance et lécart-type sont reliés par linégalité de Bienaymé-Tchebychev. Inégalité de Bienaymé-Tchebychev Théorème 7.2 Soit \\(X\\) une variable aléatoire despérance \\(\\mu\\) et de variance \\(\\sigma^2\\). Pour tout \\(\\varepsilon &gt; 0\\), on a linégalité suivante: \\[P\\left(|X-E(X)| \\geq \\varepsilon \\right) \\leq \\frac{\\sigma^2}{\\varepsilon^2}\\] On peut lécrire autrement. Soit \\(k=\\varepsilon/\\sigma\\). \\[P\\left(|X-E(X)| \\geq k\\sigma \\right) \\leq \\frac{1}{k^2}\\] Importance: Cette inégalité relie la probabilité pour \\(X\\) de sécarter de sa moyenne \\(E(X)\\), à sa variance qui est justement un indicateur de dispersion autour de la moyenne de la loi. Elle montre quantitativement que plus lécart type est faible, plus la probabilité de sécarter de la moyenne est faible. Théorème 7.3 (Inégalité de Markov) Soit \\(X\\) une variable aléatoire à valeur non négatives. Pour tout réel \\(a &gt; 0\\) \\[P(X&gt;a) \\leq \\frac{E(X)}{a}\\] Moments non centrés et centrés On appelle moment non centré dordre \\(r \\in \\mathbb{N^*}\\) de \\(X\\) la quantité, lorsquelle existe: \\[m_{r}(X)=\\sum_{i \\in \\mathbb{N} } x_{i}^r p(x_{i})=E(X^r).\\] Le moment centré dordre \\(r \\in \\mathbb{N^*}\\) est la quantité, lorsquelle existe: \\[\\mu_{r}(X)=\\sum_{i \\in \\mathbb{N} } p_{i}\\left[x_{i}-E(X)\\right]^r=E\\left[X-E(X)\\right]^r.\\] Les premiers moments sont: \\[m_{1}(X)=E(X), \\quad \\mu_{1}(X)=0\\] \\[\\mu_{2}(X)=V(X)=m_{2}(X)-m_{1}^2(X)\\] Couple de variables aléatoires discrètes Considérons deux variables aléatoires discrètes \\(X\\) et \\(Y\\). Il nous faut pour modéliser le problème une fonction qui nous donne la probabilité que \\((X = x_i )\\) en même temps que \\((Y = y_j )\\). Cest la loi de probabilité conjointe. Soit \\(X\\) et \\(Y\\) deux variables aléatoires réelles discrètes, définies sur un espace probabilisé \\((\\Omega,\\mathcal{A},P)\\) et que \\[\\begin{aligned} X(\\Omega) &amp;= \\{x_1,x_2,\\ldots,x_l\\} \\\\ Y(\\Omega) &amp;= \\{y_1,y_2,\\ldots,y_k\\} \\\\ &amp; \\quad (l \\text{ et } k \\in \\mathbb{N})\\end{aligned}\\] La loi du couple \\((X,Y)\\), dite loi de probabilité conjointe ou simultanée, est entièrement définie par les probabilités: \\[p_{ij} = P(X=x_i;Y=y_j) = P(\\{X=x_i\\}\\cap\\{Y=y_j\\})\\] On a \\[p_{ij} \\geq 0 \\quad \\text{et} \\quad \\sum_{i=1}^{l} \\sum_{j=1}^{k} p_{ij} = 1\\] Le couple \\((X,Y)\\) sappelle variable aléatoire à deux dimensions et peut prendre \\(l\\times k\\) valeurs. Table de probabilité conjointe Les probabilités \\(p_{ij}\\) peuvent être présentées dans un tableau à deux dimensions quon appelle table de probabilité conjointe: \\(X\\backslash Y\\) \\(y_1\\) \\(y_2\\) \\(\\ldots\\) \\(y_j\\) \\(\\ldots\\) \\(y_k\\) \\(x_1\\) \\(p_{11}\\) \\(p_{12}\\) \\(p_{1j}\\) \\(p_{1k}\\) \\(x_2\\) \\(p_{21}\\) \\(p_{22}\\) \\(p_{2j}\\) \\(p_{2k}\\) \\(\\vdots\\) \\(x_i\\) \\(p_{i1}\\) \\(p_{i2}\\) \\(p_{ij}\\) \\(p_{ik}\\) \\(\\vdots\\) \\(x_l\\) \\(p_{l1}\\) \\(p_{l2}\\) \\(p_{lj}\\) \\(p_{lk}\\) A la première ligne figure lensemble des valeurs de \\(Y\\) et à la première colonne figure lensemble des valeurs de \\(X\\). La probabilité \\(p_{ij} = P(X=x_i;Y=y_j)\\) est à lintersection de la \\(i^{e}\\) et de la \\(j^{e}\\) colonne. Lois marginales Lorsquon connaît la loi conjointe des variables aléatoires \\(X\\) et \\(Y\\), on peut aussi sintéresser à la loi de probabilité de \\(X\\) seule et de \\(Y\\) seule. Ce sont les lois de probabilité marginales. Loi marginale de \\(X\\): \\[p_{i.} = P(X=x_i) = P[\\{X=x_i\\}\\cap \\Omega] = \\sum_{j=1}^k p_{ij} \\quad \\quad \\forall \\, i=1,2,\\ldots,l\\] Loi marginale de \\(Y\\): \\[p_{.j} = P(Y=y_j) = P[ \\Omega \\cap \\{Y=y_j\\}] = \\sum_{i=1}^l p_{ij} \\quad \\quad \\forall \\, j=1,2,\\ldots,k\\] On peut calculer les lois marginales directement depuis la table de la loi conjointe. La loi marginale de \\(X\\) est calculée en faisant les totaux par ligne, tandis que celle de \\(Y\\) lest en faisant les totaux par colonne. Cest le fait que les lois de \\(X\\) et \\(Y\\) individuellement puissent être lues dans les marges du tableau qui leur vaut leur nom de lois marginales. \\(X\\backslash Y\\) \\(y_1\\) \\(y_2\\) \\(\\ldots\\) \\(y_j\\) \\(\\ldots\\) \\(y_k\\) Marginale de \\(X\\) \\(x_1\\) \\(p_{11}\\) \\(p_{12}\\) \\(p_{1j}\\) \\(p_{1k}\\) \\(p_{1.}\\) \\(x_2\\) \\(p_{21}\\) \\(p_{22}\\) \\(p_{2j}\\) \\(p_{2k}\\) \\(p_{2.}\\) \\(\\vdots\\) \\(x_i\\) \\(p_{i1}\\) \\(p_{i2}\\) \\(p_{ij}\\) \\(p_{ik}\\) \\(p_{i.}\\) \\(\\vdots\\) \\(x_l\\) \\(p_{l1}\\) \\(p_{l2}\\) \\(p_{lj}\\) \\(p_{lk}\\) \\(p_{l.}\\) Marginale de \\(Y\\) \\(p_{.1}\\) \\(p_{.2}\\) \\(p_{.j}\\) \\(p_{.k}\\) \\(1\\) On tire au hasard 3 boules dune urne contenant 3 boules rouges, 4 blanches et 5 noires. \\(X\\) et \\(Y\\) désignent respectivement le nombre de boules rouges et celui de boules blanches tirées. Déterminer la loi de probabilité conjointe du couple \\((X,Y)\\) ainsi que les lois marginales de \\(X\\) et de \\(Y\\). Lois conditionnelles Pour chaque valeur \\(y_j\\) de \\(Y\\) telle que \\(p_{.j} = P(Y=y_j) \\neq 0\\) on peut définir la loi conditionnelle de \\(X\\) sachant \\(Y=y_j\\) par \\[p_{i/j} = P(X=x_i / Y=y_j) = \\frac{P(X=x_i;Y=y_j)}{P(Y=y_j)} = \\frac{p_{ij}}{p_{.j}} \\quad \\quad \\forall i = 1,2,\\ldots,l\\] De même on définit la loi de \\(Y\\) sachant \\(X=x_i\\) par \\[p_{j/i} = P(Y=y_j / X=x_i) = \\frac{P(X=x_i;Y=y_j)}{P(X=x_i)} = \\frac{p_{ij}}{p_{i.}} \\quad \\quad \\forall j = 1,2,\\ldots,k\\] Indépendance de variables aléatoires Théorème 7.4 On dit que deux v.a.r.d sont indépendantes si et seulement si \\[P(X=x_i;Y=y_j) = P(X=x_i) P(Y=y_j) \\quad \\quad \\forall \\, i = 1,2,\\ldots,l \\text{ et } j = 1,2,\\ldots,k\\] On montre que \\[P(\\{X\\in A\\} \\cap \\{Y \\in B\\}) = P(\\{X\\in A\\}) P(\\{Y \\in B\\}) \\quad \\quad \\forall \\,\\, A \\text{ et } B \\in \\mathcal{A}\\] Propriétés Soit deux v.a.r.d. \\(X\\) et \\(Y\\), \\(E(X+Y)=E(X)+E(Y)\\) Si \\(X\\) et \\(Y\\) sont indépendantes alors \\(E(XY)=E(X)E(Y)\\). Mais la réciproque nest pas toujours vraie. Covariance Soit \\(X\\) et \\(Y\\) deux v.a.r.d. On appelle covariance de \\(X\\) et de \\(Y\\) la valeur si elle existe de \\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))] = \\sum_i \\sum_j (x_i-E(X))(y_j-E(Y)) p_{ij}\\] quon peut calculer en utilisant la formule suivante \\[Cov(X,Y) = E(XY) - E(X)E(Y)\\] Propriétés \\(Cov(X,Y)=Cov(Y,X)\\) \\(Cov(aX_1+bX_2,Y) = a Cov(X_1,Y) + b Cov(X_2,Y)\\) \\(V(X+Y)= V(X) + V(Y) + 2 Cov(X,Y)\\) Si \\(X\\) et \\(Y\\) sont indépendantes alors \\(Cov(X,Y) = 0\\) (la réciproque nest pas vraie) \\(V(X+Y) = V(X) + V(Y)\\) (la réciproque nest pas vraie) Coefficient de corrélation linéaire On appelle coefficient de corrélation linéaire de \\(X\\) et de \\(Y\\) la valeur définie par \\[\\rho = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] On peut montrer que \\[-1 \\leq \\rho(X,Y) \\leq 1\\] Pour le montrer on peut partir du fait que la variance est toujours positive ou nulle. Donc \\(V(\\frac{X}{\\sigma_X} + \\frac{Y}{\\sigma_Y}) \\geq 0\\) et \\(V(\\frac{X}{\\sigma_X} - \\frac{Y}{\\sigma_Y}) \\geq 0\\). Interprétation de \\(\\rho\\) Le coefficient de corrélation est une mesure du degré de linéarité entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proches de \\(1\\) ou \\(-1\\) indiquent une linéarité quasiment rigoureuse entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proche de 0 indiquent une absence de toute relation linéaire. Lorsque \\(\\rho(X,Y)\\) est positif, \\(Y\\) a tendance à augmenter si \\(X\\) en fait autant. Lorsque \\(\\rho(X,Y) &lt; 0\\), \\(Y\\) a tendance à diminuer si \\(X\\) augmente. Si \\(\\rho(X,Y) =0\\), on dit que ces deux statistiques sont non corrélées. Lois usuelles discrètes Loi uniforme discrète \\(\\mathcal{U}(n)\\) Définition 7.6 Une distribution de probabilité suit une loi uniforme lorsque toutes les valeurs prises par la variable aléatoire sont équiprobables. Si \\(n\\) est le nombre de valeurs différentes prises par la variable aléatoire alors on a: \\[\\label{eq:unif} P(X=x_i)=\\frac{1}{n} \\qquad \\forall \\, i \\in \\{1,\\ldots, n\\}\\] Exemple: La distribution des chiffres obtenus au lancer de dé (si ce dernier est non pipé) suit une loi uniforme dont la loi de probabilité est la suivante : \\(x_i\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(P(X = x_i)\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) Moments de loi uniforme discrète Dans le cas particulier dune loi uniforme discrète où chaque valeur de la variable aléatoire \\(X\\) correspond à son rang, i.e. \\(x_i=i \\, \\, \\forall i \\in \\{1,\\ldots, n\\}\\), on a: \\[E(X)=\\frac{n+1}{2} \\quad \\text{et} \\quad V(X)=\\frac{n^2-1}{12}\\] La démonstration de ces résultats est établie en utilisant les égalités (cf. Annexe) \\[\\sum_{i=1}^n i=\\frac{n(n+1)}{2} \\quad \\text{et} \\quad \\sum_{i=1}^n i^2=\\frac{n(n+1)(2n+1)}{6}.\\] En revenant à lexemple du lancer du dé de cette section, on peut calculer directement les moments de \\(X\\): \\[E(X)=\\frac{6+1}{2}=3.5\\] et \\[V(X)=\\frac{6^2-1}{12}=\\frac{35}{12}\\simeq 2.92.\\] Loi de Bernoulli \\(\\mathcal{B}(p)\\) Définition 7.7 On réalise une expérience dont le résultat sera interprété soit comme un succès soit comme un échec. On définit alors la variable aléatoire \\(X\\) en lui donnant la valeur 1 lors dun succès et 0 lors dun échec (variable indicatrice). La loi de probabilité de \\(X\\) est alors \\[\\begin{align} &amp;p(1)=P(X=1)=p \\tag{7.1} \\\\ &amp;p(0)=P(X=0)= 1-p=q \\notag \\end{align}\\] où \\(p\\) est la probabilité dun succès, \\(0 \\leq p \\leq 1\\). Une variable aléatoire \\(X\\) est dite de Bernoulli \\(X \\sim \\mathcal{B} \\left({p}\\right)\\) sil existe un nombre \\(p \\, \\in \\, ]0,1[\\) tel que la loi de probabilité de \\(X\\) soit donnée par (7.1). La fonction de répartition est définie par: \\[F(x) = \\left\\{ \\begin{array}{ll} 0 &amp; \\quad \\text{si $x &lt; 0$} \\\\ 1 - p &amp; \\quad \\text{si $0 \\leq x &lt; 1$} \\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}. \\end{array} \\right.\\] Lespérance la loi de Bernoulli est \\(p\\), en effet \\[E(X) =1 \\times P(X=1)+0 \\times P(X=0)=P(X=1)=p\\] La variance la loi de Bernoulli est \\(np\\), en effet \\[V(X) =E(X^2)-E^2(X)=p-p^2=p(1-p)=pq\\] car \\[E(X^2) =1^2\\times P(X=1)+0^2 \\times P(X=0)=P(X=1)=p\\] Loi Binomiale \\(\\mathcal{B}(n,p)\\) Décrite pour la première fois par Isaac Newton en 1676 et démontrée pour la première fois par le mathématicien suisse Jacob Bernoulli en 1713, la loi binomiale est lune des distributions de probabilité les plus fréquemment rencontrées en statistique appliquée. Supposons quon exécute maintenant \\(n\\) épreuves indépendantes, chacune ayant \\(p\\) pour probabilité de succès et \\(1-p\\) pour probabilité déchec. La variable aléatoire \\(X\\) qui compte le nombre de succès sur lensemble des \\(n\\) épreuves est dite variable aléatoire binomiale de paramètres \\(n\\) et \\(p\\). Une variable de Bernoulli nest donc quune variable binomiale de paramètres \\((1,p)\\). Définition 7.8 Si on effectue \\(n\\) épreuves successives indépendantes où on note à chaque fois la réalisation ou non dun certain événement \\(A\\), on obtient une suite de la forme \\(AA\\bar{A}A\\bar{A}\\ldots \\bar{A}AA\\). Soit \\(X\\) le nombre de réalisations de \\(A\\). On définit ainsi une v.a. \\(X\\) qui suit une loi binomiale de paramètres \\(n\\) et \\(p=P(A)\\), caractérisée par \\(X(\\Omega)=\\{0, 1,\\ldots, n\\}\\) : \\[\\begin{equation} P(X=k)=\\binom{n}{k}p^k (1-p)^{n-k} \\qquad 0\\leq k \\leq n \\tag{7.2} \\end{equation}\\] On écrit \\(X \\sim \\mathcal{B} \\left({n, p}\\right)\\). Donc la loi binomiale modélise le nombre de réalisations de \\(A\\) (succès) obtenues lors de la répétition indépendante et identique de \\(n\\) épreuves de Bernoulli. Pour établir (7.2) il faut remarquer que \\(\\binom{n}{k}\\) est le nombre déchantillons de taille \\(n\\) comportant exactement \\(k\\) événements \\(A\\), de probabilité \\(p^k\\), indépendamment de lordre, et donc \\(n-k\\) événements \\(\\bar{A}\\), de probabilité \\((1-p)^{n-k}\\). Remarque: Il est possible dobtenir aisément les valeurs des combinaisons de la loi binomiale en utilisant le triangle de Pascal. En utilisant la formule du binôme de Newton, on vérifie bien que cest une loi de probabilité: \\[{\\sum_{k=0}^nP(X=k)=\\sum_{k=0}^n\\binom{n}{k} p^{k}(1-p)^{n-k}=[p+(1-p)]^n=1}\\] Exemple: On jette cinq pièces équilibrées. Les résultats sont supposés indépendants. Donner la loi de probabilité de la variable \\(X\\) qui compte le nombre de piles obtenus. Moments de la loi Binomiale Pour calculer facilement les moments de cette loi, nous allons associer à chaque épreuve \\(i\\), \\(1\\leq i \\leq n\\), une v.a. de Bernoulli (variable indicatrice sur \\(A\\)): \\[{1}_A=X_i = \\left\\{ \\begin{array}{l l} 1 &amp; \\quad \\text{si $A$ est réalisé}\\\\ 0 &amp; \\quad \\text{si $\\bar{A}$ est réalisé}\\\\ \\end{array} \\right.\\] On peut écrire alors: \\(X=\\sum_{i=1}^nX_i=X_1+X_2+\\ldots+X_n\\), ce qui nous permet de déduire aisément: \\[\\begin{aligned} E(X)&amp;=E\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^nE(X_i)=np \\\\ \\text{et} \\nonumber \\\\ V(X)&amp;=V\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^nV(X_i)=np(1-p) \\quad \\text{car les v.a. $X_i$ sont indépendantes.} \\end{aligned}\\] Le calcul direct des moments de \\(X\\) peut seffectuer à partir de la définition générale, mais de façon beaucoup plus laborieuse: \\[\\begin{aligned} E(X)&amp;= \\sum_{k=0}^nk \\binom{n}{k} p^{k}(1-p)^{n-k}=\\sum_{k=1}^nk \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &amp;= \\sum_{k=1}^n\\frac{n!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k}= np \\sum_{k=1}^n\\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1}(1-p)^{n-k} \\\\ &amp;= np \\sum_{j=0}^{n-1}\\frac{(n-1)!}{j!(n-1-j)!}p^j (1-p)^{n-1-j} =np \\sum_{j=0}^{n-1}\\binom{n-1}{j} p^{j}(1-p)^{n-1-j} \\\\ &amp;= np [p+(1-p)]^{n-1}=np \\end{aligned}\\] Pour obtenir \\(E(X^2)\\) par un procédé de calcul identique, on passe par lintermédiaire du moment factoriel \\(E[X(X-1)]=E(X^2)-E(X)\\): \\[\\begin{aligned} E[X(X-1)]&amp;= \\sum_{k=0}^nk(k-1) \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &amp;= n(n-1)p^2 \\sum_{k=2}^{n}\\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2}(1-p)^{n-k} \\\\ &amp;= n(n-1)p^2 \\sum_{j=0}^{n-2}\\binom{n-2}{j} p^{j}(1-p)^{n-2-j} \\\\ &amp;= n(n-1)p^2[p+(1-p)]^{n-2}= n(n-1)p^2 \\end{aligned}\\] On en déduit alors: \\[E(X^2)=E[X(X-1)]+E(X)= n(n-1)p^2+np,\\] puis: \\[\\begin{aligned} V(X)&amp;=n(n-1)p^2+np-(np)^2 \\\\ &amp;=n^2p^2+np(1-p)-n^2p^2 \\\\ &amp;=np(1-p). \\end{aligned}\\] Le nombre de résultats pile apparus au cours de \\(n\\) jets dune pièce de monnaie suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/2}\\right)\\): \\[P(X=k)=\\binom{n}{k}\\left(\\frac{1}{2}\\right)^k \\left(\\frac{1}{2}\\right)^{n-k}=\\frac{\\binom{n}{k}}{2^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/2\\) et \\(V(X)=n/4\\). Le nombre \\(N\\) de boules rouges apparues au cours de \\(n\\) tirages avec remise dans une urne contenant deux rouges, trois vertes et une noire suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/3}\\right)\\): \\[P(N=k)=\\binom{n}{k}\\left(\\frac{1}{3}\\right)^k \\left(\\frac{2}{3}\\right)^{n-k}=\\binom{n}{k} \\frac{2^{n-k}}{3^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/3\\) et \\(V(X)=2n/9\\). Théorème 7.5 Si \\(X_1 \\sim \\mathcal{B} \\left({n_1, p}\\right)\\) et \\(X_2 \\sim \\mathcal{B} \\left({n_2, p}\\right)\\), les v.a. \\(X_1\\) et \\(X_2\\) étant indépendantes, alors \\(X_1+X_2 \\sim \\mathcal{B} \\left({n_1+n_2, p}\\right)\\). Ceci résulte de la définition dune loi binomiale puisquon totalise ici le résultat de \\(n_1+n_2\\) épreuves indépendantes. Loi de Poisson \\(\\mathcal{P}(\\lambda)\\) La loi de Poisson est découverte au début du XIX\\(^e\\) siècle par le magistrat français Siméon-Denis Poisson. Les variables aléatoires de Poisson ont un champ dapplication fort vaste, en particulier du fait quon peut les utiliser pour approximer des variables aléatoires binomiales de paramètres \\((n,p)\\) pour autant que \\(n\\) soit grand et \\(p\\) assez petit pour que \\(np\\) soit dordre de grandeur moyen. Définition 7.9 Une v.a. \\(X\\) suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) si cest une variable à valeurs entières, \\(X(\\Omega)=\\mathbb{N}\\), donc avec une infinité de valeurs possibles, de probabilité: \\[\\label{eq:poisson} P(X=k)=e^{-\\lambda} \\frac{\\lambda^k}{k!}, \\quad k \\in \\mathbb{N}\\] Cette loi ne dépend quun seul paramètre réel positif \\(\\lambda\\), avec lécriture symbolique \\(X \\sim \\mathcal{P}(\\lambda)\\). Le développement en série entière de lexponentielle \\(e^\\lambda=\\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\\) permet de vérifier quil sagit bien dune loi de probabilité: \\[\\sum_{k=0}^{\\infty} P(X=k)=\\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!}=e^{-\\lambda}\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}=e^{-\\lambda}e^{\\lambda}=1\\] Moments de loi de Poisson Le calcul de lespérance mathématique se déduit du développement en série entière de lexponentielle: \\[\\begin{aligned} E(X)&amp;=\\sum_{k=0}^{\\infty} k P(X=k)=\\sum_{k=1}^{\\infty} k e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\ &amp;=e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}=\\lambda e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!} \\\\ &amp;= \\lambda e^{-\\lambda} \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}= \\lambda e^{-\\lambda} e^{\\lambda} \\\\ &amp;= \\lambda.\\end{aligned}\\] Pour calculer la variance nous nallons pas calculer \\(E(X^2)\\) mais le moment factoriel \\(E[X(X-1)]\\) qui sobtient plus facilement, selon la méthode précédente: \\[\\begin{aligned} E[X(X-1)] &amp;=\\sum_{k=0}^{\\infty} k(k-1)P(X=k)=\\sum_{k=2}^{\\infty} k(k-1) \\,e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\ &amp;=e^{-\\lambda} \\sum_{k=2}^{\\infty} \\frac{\\lambda^k}{(k-2)!}=\\lambda^2 e^{-\\lambda} \\sum_{k=2}^{\\infty} \\frac{\\lambda^{k-2}}{(k-2)!} \\\\ &amp;= \\lambda^2 e^{-\\lambda} \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}= \\lambda^2 e^{-\\lambda} e^{\\lambda} = \\lambda^2.\\end{aligned}\\] On en déduit: \\[\\begin{aligned} V(X)&amp;=E(X^2)-E^2(X)=E[X(X-1)]+E(X)-E^2(X) \\\\ &amp;=\\lambda^2+\\lambda-\\lambda^2=\\lambda.\\end{aligned}\\] Théorème 7.6 Si \\(X\\) et \\(Y\\) sont deux variables indépendantes suivant des lois de Poisson \\[X \\sim \\mathcal{P}(\\lambda) \\quad \\text{et} \\quad Y \\sim \\mathcal{P}(\\mu)\\] alors leur somme suit aussi une loi de Poisson: \\[X+Y \\sim \\mathcal{P}(\\lambda+\\mu).\\] Exemple: Soit \\(X\\) la variable aléatoire associée au nombre de micro-ordinateurs vendus chaque jour dans le magasin. On suppose que \\(X\\) suit une loi de Poisson de paramètre \\(\\lambda=5\\). On écrit alors \\(X \\sim \\mathcal{P}(5).\\) La probabilité associée à la vente de 5 micro-ordinateurs se détermine par : \\[P(X=5)=e^{-5} \\frac{5^5}{5!}=e^{-5}\\simeq 0.1755\\] La probabilité de vendre au moins 2 micro-ordinateurs est égal à: \\[\\begin{aligned} P(X \\geq 2)&amp;=1-\\left(e^{-5} \\frac{5^0}{0!}+e^{-5} \\frac{5^1}{1!}\\right)\\simeq 0.9596\\end{aligned}\\] Le nombre moyen de micro-ordinateurs vendus chaque jour dans le magasin est égal à 5 puisque \\(E(X)=\\lambda=5\\). Approximation dune loi binomiale Le théorème de Poisson nous montre que si \\(n\\) est suffisamment grand et \\(p\\) assez petit, alors on peut approcher la distribution dune loi binomiale de paramètres \\(n\\) et \\(p\\) par celle dune loi de Poisson de paramètre \\(\\lambda=np\\), en effet \\[\\text{si} \\; n \\rightarrow \\infty \\; \\text{et}\\; p \\rightarrow 0 \\; \\text{alors} \\; X: \\mathcal{B}(n, p) \\rightarrow \\mathcal{P}(\\lambda).\\] Une bonne approximation est obtenue si \\(n \\geq 50\\) et \\(np \\leq 5\\). Dans ce contexte, la loi de Poisson est souvent utilisée pour modéliser le nombre de succès lorsquon répète un très grand nombre de fois une expérience ayant une chance très faible de réussir par une loi de Poisson (nombre de personnes dans la population française atteints dune maladie rare, par exemple). On cherche la probabilité de trouver au moins un centenaire parmi 200 personnes dans une population où une personne sur cent est un centenaire. La probabilité \\(p=1/100=0.01\\) étant faible et \\(n=200\\) étant suffisamment grand, on peut modéliser le nombre \\(X\\) de centenaires pris parmi 200 personnes par la loi de Poisson de paramètre \\(\\lambda=200 \\times 0.01=2\\). Donc on a: \\[P(X\\geq 1)=1-P(X=0)=1-e^{-2}\\simeq 0.86\\] Soit une v.a. \\(X\\) telle que \\(X \\sim \\mathcal{B}(100, 0.01)\\), les valeurs des probabilités pour \\(k\\) de 0 à 5 ainsi que leur approximation à \\(10^{-3}\\) avec une loi de Poisson de paramètre \\(\\lambda= np =1\\) sont données dans le tableau ci-dessous : \\(k\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(k\\) \\(P(X = k)\\) \\(0.366\\) \\(0.370\\) \\(0.185\\) \\(0.061\\) \\(0.015\\) \\(0.000\\) Approximation \\(0.368\\) \\(0.368\\) \\(0.184\\) \\(0.061\\) \\(0.015\\) \\(0.003\\) Dans le cas de cet exemple où \\(n =100\\) et \\(np =1\\), lapproximation de la loi binomiale par une loi de poisson donne des valeurs de probabilités identiques à \\(10^{-3}\\) près. Loi Géométrique ou de Pascal \\(\\mathcal{G}(p)\\) On effectue des épreuves successives indépendantes jusquà la réalisation dun événement particulier \\(A\\) de probabilité \\(p=P(A)\\) et on note \\(X\\) le nombre aléatoire dépreuves effectuées. On définit ainsi une v.a. à valeurs entières de loi géométrique, ou de Pascal. A chaque épreuve est associé lensemble fondamental \\(\\Omega=\\{A, \\bar{A}\\}\\) et lévénement \\(\\{X=k\\}\\) pour \\(k\\in \\mathbb{N^*}\\) est représenté par une suite de \\(k-1\\) événements \\(\\bar{A}\\), terminée par lévénement \\(A\\): \\[\\underbrace{\\bar{A}\\bar{A}\\ldots \\bar{A}}_{k-1}A\\] Doù: \\[\\begin{equation} P(X=k)=(1-p)^{k-1}p \\quad \\forall \\, k \\in \\mathbb{N^*} \\tag{7.3} \\end{equation}\\] Cette loi peut servir à modéliser des temps de vie, ou des temps dattente, lorsque le temps est mesuré de manière discrète (nombre de jours par exemple). En utilisant la série entière \\[\\label{eq:serie_entiere} \\sum_{k=0}^\\infty x^k = 1/(1-x) \\quad \\text{pour} \\quad |x|&lt;1\\] on vérifie bien que cest une loi de probabilité: \\[\\begin{aligned} \\sum_{k=1}^\\infty P(X=k)&amp;= \\sum_{k=1}^\\infty (1-p)^{k-1}p = p \\sum_{j=0}^\\infty (1-p)^{j} \\\\ &amp;= p \\frac{1}{1-(1-p)}=1\\end{aligned}\\] Moments de loi Géométrique En dérivant la série entière (7.3) ci-dessus, on obtient \\(\\sum_{k=1}^\\infty k x^{k-1}=1/(1-x)^2\\). Ceci permet dobtenir lespérance: \\[E(X)=\\sum_{k=1}^\\infty kp(1-p)^{k-1}=\\frac{p}{[1-(1-p)]^2}=\\frac{1}{p}\\] En dautres termes, si des épreuves indépendantes ayant une probabilité \\(p\\) dobtenir un succès sont réalisés jusquà ce que le premier succès se produise, le nombre espéré dessais nécessaires est égal à \\(1/p\\). Par exemple, le nombre espéré de jets dun dé équilibré quil faut pour obtenir la valeur 1 est 6. Le calcul de la variance se fait à partir du moment factoriel et en utilisant la dérivée seconde de la série entière (7.3): \\(\\sum_{k=2}^\\infty k(k-1) x^{k-2} = 2/(1-x)^3\\), Donc \\[\\begin{aligned} E[X(X-1)]&amp;=\\sum_{k=2}^\\infty k(k-1)p(1-p)^{k-1} \\\\ &amp;= p(1-p)\\sum_{k=2}^\\infty k(k-1)(1-p)^{k-2} \\\\ &amp;= \\frac{2p(1-p)}{[1-(1-p)]^3}=\\frac{2(1-p)}{p^2}\\end{aligned}\\] doù on déduit: \\[V(X)=E[X(X-1)]+E(X)-E^2(X)=\\frac{1-p}{p^2}.\\] Si lon considère la variable aléatoire \\(X\\) nombre de naissances observées jusquà lobtention dune fille avec p = 1/2 (même probabilité de naissance dune fille ou dun garçon), alors X suit une loi géométrique et on a pour tout \\(k\\in \\mathbb{N^*}\\): \\[P(X=k)=(1-1/2)^{k-1}(1/2)=1/2^k\\] avec \\(E(X)=2\\) et \\(V(X)=2.\\) Loi Binomiale Négative \\(\\mathcal{BN}(r,p)\\) \\(\\varepsilon\\): On répéte lépreuve de Bernoulli jusquà obtenir un total de \\(r\\) succès. Exemple avec : \\[\\bar{A} \\quad {A} \\quad \\bar{A} \\quad \\bar{A} \\quad \\bar{A} \\quad {A} \\quad \\bar{A} \\quad \\bar{A} \\quad {A}\\] \\[{E} \\quad {S} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {E} \\quad {S}\\] Mais on peut obtenir dautres façons: \\[{S} \\quad {E} \\quad {E} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {S}\\] \\[{E} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {S} \\quad {E} \\quad {S}\\] Chaque épreuve a \\({p}\\) pour probabilité de succès et \\({1-p}\\) pour probabilité déchec. Désignons \\(X=\\)le nombre dépreuves nécessaires pour atteindre ce résultat. \\[\\underbrace{\\overbrace{{E} \\quad {S} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {E}}^{ {r-1 \\, succès}\\, et \\, {k-r \\, échecs}} \\quad {S}}_{X=k}\\] \\(X(\\Omega)=\\{r,r+1,r+2,\\ldots\\}\\). On dit \\(X \\sim \\mathcal{BN}(r,p)\\). \\(\\forall \\, k \\in X(\\Omega),\\) \\[P(X=k) = \\binom{{k-1}}{{r-1}} {p^r} {(1-p)^{k-r}}\\] \\(\\mathcal{G}(p)=\\mathcal{BN}(1,p)\\) \\(\\varepsilon\\): On répéte lépreuve de Bernoulli jusquà obtenir un total de \\(r\\) succès. Soit, \\[{E} \\quad \\ldots \\quad {E} \\quad {S} \\quad {E} \\quad \\ldots \\quad {E} \\quad {S} \\ldots \\quad {E} \\ldots \\quad {E} \\quad {S}\\] Soit, \\(Y_1\\) le nombre dépreuves nécessaires jusquau premier succès, \\(Y_2\\) le nombre dépreuves supplémentaires nécessaires pour obtenir un deuxième succès, \\(Y_3\\) celui menant au 3ème et ainsi de suite. Càd, \\[\\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_1} \\quad \\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_2} \\quad \\underbrace{\\ldots}_{\\ldots} \\quad \\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_r}\\] Les tirages étants indépendantes et ayant toujours la même probabilité de succès, chacune des variables \\(Y_1,Y_2,\\ldots,Y_r\\) est géométrique \\(\\mathcal{G}(p)\\). \\(X=\\)le nombre dépreuves nécessaires à lobtention de \\(r\\) succès\\(=Y_1 + Y_2 + \\ldots + Y_r\\). Donc, \\[E(X)= E(Y_1) + E(Y_2) + \\ldots + E(Y_r) = \\sum_{i=1}^r \\frac{1}{p} = \\frac{r}{p}\\] et \\[V(X)= \\sum_{i=1}^r V(Y_i) = \\frac{r(1-p)}{p^2}\\] car les \\(Y_i\\) sont indépendantes. "],["variables-aléatoires-continues.html", "Variables Aléatoires Continues Densité dune variable aléatoire continue Fonction de répartition dune v.a.c Fonction dune variable aléatoire continue Espérance et variance de variables aléatoires continues Lois usuelles de v.a.c Couple de variables aléatoires continues", " Variables Aléatoires Continues Densité dune variable aléatoire continue Dans les chapitres précédents nous avons traité des variables aléatoires discrètes, cest-à-dire de variables dont lunivers est fini ou infini dénombrable. Il existe cependant des variables dont lunivers est infini non dénombrable. On peut citer par exemple, lheure darrivée dun train à une gare donnée ou encore la durée de vie dun transistor. Désignons par \\(X\\) une telle variable. Définition 7.10 \\(X\\) est une variable aléatoire continue sil existe une fonction \\(f\\) non négative définie pour tout \\(x \\in \\mathbb{R}\\) et vérifiant pour tout ensemble \\(B\\) de nombres réels la propriété \\[\\begin{equation} P(X \\in B) = \\int_B f(x)dx \\tag{7.4} \\end{equation}\\] La fonction \\(f\\) est appelée densité de probabilité de la variable aléatoire \\(X\\). Tous les problèmes de probabilité relatifs à \\(X\\) peuvent être traités grâce à \\(f\\). Par exemple pour \\(B=[a,b]\\), on obtient grâce à léquation (7.4) \\[\\begin{equation} P(a\\le X \\le b) = \\int_a^bf(x)dx \\tag{7.5} \\end{equation}\\] Graphiquement, \\(P(a\\le X \\le b)\\) est laire de la surface entre laxe de \\(x\\), la courbe correspondante à \\(f(x)\\) et les droites \\(x=a\\) et \\(x=b\\). Voire Figure 7.1 et Figure 7.2. Figure 7.1: \\(P(a \\leq X \\leq B)=\\) surface grisée Figure 7.2: Laire hachurée correspond à des probabilités. \\(f(x)\\) étant une fonction densité de probabilité Définition 7.11 Pour toute variable aléatoire continue \\(X\\) de densité \\(f\\): \\(f(x) \\ge 0 \\quad \\forall \\, x \\in \\mathbb{R}\\) \\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\) Si lon pose \\(a=b\\) dans (7.5), il résulte \\[P(X=a)=\\int_a^a f(x)dx = 0\\] Ceci siginifie que la probabilité quune variable aléatoire continue prenne une valeur isolée fixe est toujours nulle. Aussi on peut écrire \\[P(X &lt; a) = P( X \\le a) = \\int_{-\\infty}^a f(x)dx\\] Soit \\(X\\) la variable aléatoire réelle de densité de probabilité \\[f(x)= \\left\\lbrace \\begin{array}{ll} kx &amp; \\mbox{si} \\quad 0\\le x \\le 5\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer \\(k\\). Calculer: \\(P(1 \\le X \\le 3), P(2 \\le X \\le 4)\\) et \\(P(X &lt; 3)\\). Soit \\(X\\) une variable aléatoire réelle continue ayant pour densité de probabilité \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{6} x + k &amp; \\mbox{si} \\quad 0\\le x \\le 3\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer \\(k\\). Calculer \\(P(1 \\le X \\le 2)\\) Fonction de répartition dune v.a.c Définition 7.12 Si comme pour les variables aléatoires discrètes, on définit la fonction de répartition de \\(X\\) par: \\[\\begin{aligned} F_X \\colon \\mathbb{R} &amp;\\longrightarrow \\mathbb{R} \\\\ x &amp;\\longmapsto F_X(a) = P(X \\le a)\\end{aligned}\\] alors la relation entre la fonction de répartition \\(F_X\\) et la fonction densité de probabilité \\(f(x)\\) est la suivante: \\[\\forall \\quad a \\in \\mathbb{R} \\quad F_X(a)= P(X \\le a) = \\int_{-\\infty}^a f(x)dx\\] La fonction de répartition \\(F_X(a)\\) est la primitive de la fonction densité de probabilité \\(f(x)\\) (donc la densité dune v.a.c est la dérivée de la fonction de répartition), et permet dobtenir les probabilités associées à la variable aléatoire \\(X\\), en effet: Propriétés: Pour une variable aléatoire continue X: \\(F&#39;_X(x) = \\frac{\\text{d}}{\\text{d} x} F_X(x) = f(x)\\). Pour tous réels \\(a \\le b\\), \\[\\begin{aligned} P(a &lt; X &lt; b) &amp; = P(a &lt; X \\le b) \\\\ &amp; = P(a \\le X &lt; b) \\\\ &amp; = P( a \\le X \\le b) \\\\ &amp; = F_X(b) - F_X(a) = \\int_a^bf(x)dx \\end{aligned}\\] La fonction de répartition correspond aux probabilités cumulées associées à la variable aléatoire continue sur lintervalle détude (Figure 7.3). Figure 7.3: Laire hachurée en vert sous la courbe de la fonction densité de probabilité correspond à la probabilité \\(P ( X &lt; a ) = F_X ( a )\\) et vaut 0.5 car ceci correspond exactement à la moitié de laire totale sous la courbe Propriétés: Les propriétés associées à la fonction de répartition sont les suivantes: \\(F_X\\) est continue sur \\(\\mathbb{R}\\), dérivable en tout point où \\(f\\) est continue. \\(F_X\\) est croissante sur \\(\\mathbb{R}\\). \\(F_X\\) est à valeurs dans \\([0,1]\\). \\(\\lim\\limits_{x\\to - \\infty} F_X(x) = 0\\) et \\(\\lim\\limits_{x\\to +\\infty} F_X(x) = 1\\). Fonction dune variable aléatoire continue Soit \\(X\\) une variable aléatoire continue de densité \\(f_X\\) et de fonction de répartition \\(F_X\\). Soit \\(h\\) une fonction continue définie sur \\(X(\\Omega)\\), alors \\(Y=h(X)\\) est une variable aléatoire. Pour déterminer la densité de \\(Y\\), notée \\(f_Y\\), on commence par calculer la fonction de répartition de \\(Y\\), notée \\(F_Y\\), ensuite nous dérivons pour déterminer \\(f_Y\\). Calcul de densités pour \\(h(X)=aX+b\\) \\(\\forall \\quad y \\in \\mathbb{R}\\), \\[F_Y(y) = P(Y\\leq y)=P(h(X) \\le y) = P(aX+b \\le y)\\] si \\(a&gt;0\\), \\[F_Y(y) = P(aX+b \\le y) = P(X\\leq \\frac{y-b}{a})=F_X(\\frac{y-b}{a})\\] si \\(a&lt;0\\), \\[F_Y(y) = P(aX+b \\le y) =P(X\\geq \\frac{y-b}{a})=1-F_X(\\frac{y-b}{a})\\] En dérivant on obtient la densité de \\(Y\\) \\[f_Y(y)=\\frac{1}{|a|}f_X(\\frac{y-b}{a}).\\] Calcul de densités pour \\(h(X)=X^2\\) si \\(y&lt;0\\), \\(F_Y(y) =P(Y\\leq y)=0\\). si \\(y&gt;0\\), \\[F_Y(y) =P(Y\\leq y)=P(X^2 \\le y)=P(-\\sqrt{y}\\leq X \\leq \\sqrt{y})=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\] En dérivant on obtient la densité de \\(Y\\), \\[f_Y(y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{2\\sqrt{y}}\\big[f_X(\\sqrt{y})+f_X(-\\sqrt{y})\\big] &amp; \\mbox{si} \\quad y \\ge 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calcul de densités pour \\(h(X)=e^X\\) si \\(y&lt;0\\), \\(F_Y(y) = P(Y\\leq y)=0\\). si \\(y&gt;0\\), \\(F_Y(y) = P(Y\\leq y)=P(e^X \\le y)=P( X \\leq \\ln (y))=F_X(\\ln(y))\\). En dérivant on obtient la densité de \\(Y\\) \\[f_Y(y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{y} f\\big(\\ln (y)\\big) &amp; \\mbox{si} \\quad y \\ge 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Soit la v.a.c \\(X\\) ayant la fonction de densité \\[f_X(x)= \\left\\lbrace \\begin{array}{ll} 2 x &amp; \\mbox{si} \\quad 0 \\le x \\le 1\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Déterminer la densité de: \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\). Espérance et variance de variables aléatoires continues Espérance dune v.a.c Définition 7.13 Si \\(X\\) est une variable aléatoire absolument continue de densité \\(f\\), on appelle espérance de X, le réel \\(E(X)\\), défini par: \\[E(X)= \\int_{-\\infty}^{+\\infty}x f(x) dx\\] si cette intégrale est convergente. Les propriétés de lespérance dune variable aléatoire continue sont les mêmes que pour une variable aléatoire discrète. Propriétés: Soit \\(X\\) une variable aléatoire continue, \\(E(aX+b)=aE(X)+b \\quad \\quad a \\ge 0 \\,\\, \\text{et} \\,\\, b \\in \\mathbb{R}\\). Si \\(X \\ge 0\\) alors \\(E(X) \\ge 0\\). Si \\(X\\) et \\(Y\\) sont deux variables aléatoires définies sur un même univers \\(\\Omega\\) alors \\[E(X+Y)=E(X)+E(Y)\\] Théorème 7.7 (Théorème de transfert) Si \\(X\\) est une variable aléatoire de densité \\(f(x)\\), alors pour toute fonction réelle \\(g\\) on aura \\[E[g(X)] = \\int_{-\\infty}^{+\\infty}g(x) f(x) dx\\] Soit la v.a.c \\(X\\) ayant la fonction de densité \\[f_X(x)= \\left\\lbrace \\begin{array}{ll} 2 x &amp; \\mbox{si} \\quad 0 \\le x \\le 1\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer lespérance des variables aléatoires \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\). Variance dune v.a.c La variance dune variable aléatoire \\(V(X)\\) est lespérance mathématique du carré de lécart à lespérance mathématique. Cest un paramètre de dispersion qui correspond au moment centré dordre 2 de la variable aléatoire \\(X\\). Définition 7.14 Si \\(X\\) est une variable aléatoire ayant une espérance \\(E(X)\\), on appelle variance de \\(X\\) le réel \\[V(X)=E\\big([X-E(X)]^2\\big) = E(X^2) - [E(X)]^2\\] Si \\(X\\) est une variable aléatoire continue, on calcule \\(E(X^2)\\) en utilisant le théorème 7.7, \\[E(X^2) = \\int_{-\\infty}^{+\\infty}x^2 f(x)dx\\] Propriétés: Si \\(X\\) est une variable aléatoire admettant une variance alors: \\(V(X) \\ge 0\\), si elle existe. \\(\\forall \\quad a \\in \\mathbb{R}, V(aX) = a^2 V(X)\\) \\(\\forall \\quad (a,b) \\in \\mathbb{R}, V(aX+b) = a^2 V(X)\\) Si \\(X\\) et \\(Y\\) sont deux variables aléatoires indépendantes, \\(V(X+Y)=V(X)+V(Y)\\) Définition 7.15 (Ecart-type) Si \\(X\\) est une variable aléatoire ayant une variance \\(V(X)\\), on appelle écart-type de \\(X\\), le réel: \\[\\sigma_X = \\sqrt{V(X)}\\] Lois usuelles de v.a.c Loi uniforme \\(U(a,b)\\) La loi uniforme est la loi exacte de phénomènes continus uniformément répartis sur un intervalle. Définition 7.16 La variable aléatoire \\(X\\) suit une loi uniforme sur le segment \\([a,b]\\) avec \\(a &lt; b\\) si sa densité de probabilité est donnée par \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{b-a} &amp; \\mbox{si} \\quad x \\in [a,b]\\\\ 0 &amp; \\mbox{si} \\quad x \\notin [a,b] \\end{array} \\right. = \\frac{1}{b-a} {1}_{[a,b]}(x)\\] Figure 7.4: Fonction de densité de \\(U([a,b]\\)) Quelques commentaires: La loi uniforme continue étant une loi de probabilité, laire hachurée en bleu sur la Figure 7.4 vaut \\(1\\). La fonction de répartition associée à la loi uniforme continue est \\[F_X(x)= \\left\\lbrace \\begin{array}{ll} 0 &amp; \\mbox{si} \\quad x &lt; a \\\\ \\frac{x-a}{b-a} &amp; \\mbox{si} \\quad a \\le x \\le b \\\\ 1 &amp; \\mbox{si} \\quad x &gt; b \\end{array} \\right.\\] Propriétés: Si \\(X\\) est une v.a.c qui suit la loi uniforme sur \\([a,b]\\): \\(E(X) = \\frac{b+a}{2}\\) \\(V(X) =\\frac{(b-a)^2}{12}\\) Soit \\(X \\thicksim U(0,10)\\). Calculer: \\(P(X &lt;3)\\) \\(P(X\\ge 6)\\) \\(P(3 &lt; X &lt; 8)\\) Loi exponentielle \\(\\mathcal{E}(\\lambda)\\) Définition 7.17 On dit quune variable aléatoire \\(X\\) est exponentielle (ou suit la loi exponentielle) de paramètre \\(\\lambda\\) si sa densité est donnée par \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\lambda e^{- \\lambda x} &amp; \\mbox{si} \\quad x \\ge 0\\\\ 0 &amp; \\mbox{si} \\quad x &lt; 0 \\end{array} \\right. = \\lambda e^{- \\lambda x} {1}_{\\mathbb{R}^{+}}(x)\\] On dit \\(X \\thicksim \\mathcal{E}(\\lambda)\\) La fonction de répartition \\(F\\) dune variable aléatoire exponentielle est donnée par \\[\\mbox{Si}\\,\\, x \\ge 0 \\quad F(x) = P(X \\le x) = \\int_0^x f(t)dt = \\int_0^x \\lambda e^{- \\lambda t} dt = \\big[ -e^{- \\lambda t} \\big]_0^x = 1-e^{- \\lambda x} \\quad\\] Propriétés: Si \\(X \\thicksim \\mathcal{E}(\\lambda)\\) \\(E(X) = \\frac{1}{\\lambda}\\) \\(V(X)= \\frac{1}{\\lambda^2}\\) Cas dutilisations de la loi exponentielle : Dans la pratique, on rencontre souvent la distribution exponentielle lorsquil sagit de représenter le temps dattente avant larrivée dun événement spécifié. Une loi exponentielle modélise la durée de vie dun phénomène sans mémoire, ou sans vieillissement, ou sans usure. En dautres termes, le fait que le phénomène ait duré pendant un temps \\(t\\) ne change rien à son espérance de vie à partir du temps \\(t\\). On dit quune variable aléatoire non négative \\(X\\) est sans mémoire lorsque \\[P(X &gt; t+h | X &gt; t) = P(X &gt; h) \\quad \\quad \\forall \\quad t,h \\ge 0\\] Par exemple, la durée de vie de la radioactivité ou dun composant électronique, le temps qui nous sépare dun prochain tremblement de terre ou du prochain appel téléphonique mal aiguillé sont toutes des variables aléatoires dont les distributions tendent en pratique à se rapprocher de distributions exponentielles. Loi Normale ou de Laplace-Gauss \\(\\mathcal{N}(\\mu,\\sigma^2)\\) Définition 7.18 Une variable aléatoire \\(X\\) est dite normale avec paramètres \\(\\mu\\) et \\(\\sigma^2\\) si la densité de \\(X\\) est donnée par \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(x - \\mu)^2/2\\sigma^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R}\\] Avec \\(\\mu \\in \\mathbb{R}\\) et \\(\\sigma \\in \\mathbb{R}^{+}\\). On dit que \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\). Remarque: On admet que \\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\) dans la mesure où lintégration analytique est impossible. Étude de la densité de la loi Normale La fonction \\(f\\) est paire autour dun axe de symétrie \\(x = \\mu\\) car \\(f(x + \\mu ) = f(\\mu - x)\\). \\(f&#39;(x)=0\\) pour \\(x=\\mu\\), \\(f&#39;(x) &lt; 0\\) pour \\(x &lt; \\mu\\) et \\(f&#39;(x) &gt; 0\\) pour \\(x &gt; \\mu\\) Figure 7.5: Représentation graphique de la densité dune loi normale. Remarque: Le paramètre \\(\\mu\\) représente laxe de symétrie et s le degré daplatissement de la courbe de la loi normale dont la forme est celle dune courbe en cloche Propriétés: Soit \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\), on a: \\(E(X)=\\mu\\) \\(V(X)=\\sigma^2\\) Théorème 7.8 (Stabilité de la loi normale) Soit \\(X_1\\) et \\(X_2\\) deux variables aléatoires normales et indépendantes de paramètres respectifs \\((\\mu_1,\\sigma_1^2)\\) et \\((\\mu_2,\\sigma_2^2)\\), alors leur somme \\(X_1+X_2\\) est une variable aléatoire normale de paramètres \\((\\mu_1 + \\mu_2,\\sigma_1^2+\\sigma_2^2)\\). Loi Normale centrée réduite \\(\\mathcal{N}(0,1)\\) Définition 7.19 Une variable aléatoire continue \\(X\\) suit une loi normale centrée réduite si sa densité de probabilité est donnée par \\[\\begin{equation} f(x) = \\frac{1}{{\\sqrt {2\\pi } }}e^{- \\frac{1}{2} x^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R} \\end{equation}\\] On dit \\(X \\thicksim \\mathcal{N}(0,1)\\). Remarque: \\(E(X)=0\\) et \\(V(X)=1\\). Figure 7.6: (gauche): Densité dune loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). (droite): Fonction de répartition de \\(\\mathcal{N}(0,1)\\). Relation entre loi normale et loi normale centrée réduite Théorème 7.9 (Relation avec la loi normale) Si \\(X\\) suit une loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\), alors \\(Z= \\frac{X-\\mu}{\\sigma}\\) est une variable centrée réduite qui suit la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). Calcul des probabilités dune loi normale La fonction de répartition de la loi normale réduite permet dobtenir les probabilités associées à toutes variables aléatoires normales \\(\\mathcal{N}(\\mu,\\sigma^2)\\) après transformation en variable centrée réduite. Définition 7.20 On appelle fonction \\(\\Phi\\), la fonction de répartition de la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\), telle que \\[\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) = P(X \\le x) = \\frac{1}{{\\sqrt {2\\pi}}} \\int_{-\\infty}^x f(t)dt\\] Propriétés: Les propriétés associées à la fonction de répartition \\(\\Phi\\) sont: \\(\\Phi\\) est croissante, continue et dérivable sur \\(\\mathbb{R}\\) et vérifie: \\(\\lim\\limits_{x\\to - \\infty} \\Phi(x) = 0\\) et \\(\\lim\\limits_{x\\to\\infty} \\Phi(x) = 1\\) \\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) + \\Phi(-x) = 1\\) \\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) - \\Phi(-x) = 2\\Phi(x) -1\\) Une application directe de la fonction \\(\\Phi\\) est la lecture des probabilités de la loi normale sur la table de la loi normale centrée réduite. Soit \\(X\\) une variable aléatoire normale de paramètres \\(\\mu =3\\) et \\(\\sigma^2=4\\). Calculer: \\(P(X &gt; 0)\\) \\(P(2 &lt; X &lt; 5)\\) \\(P(|X-3| &gt; 4)\\) Approximation normale dune répartition binomiale Un résultat important de la théorie de probabilité est connu sous le nom de théorème limite de Moivre-Laplace. Il dit que pour \\(n\\) grand, une variable binomiale \\(\\mathcal{B}(n,p)\\) suivra approximativement la même loi quune variable aléatoire normale avec même moyenne et même variance. Ce théorème énonce que si on standardise une variable aléatoire binomiale \\(\\mathcal{B}(n,p)\\) en soustrayant dabord sa moyenne \\(np\\) puis en divisant le résultat par son écart-type \\(\\sqrt{np(1-p)}\\), alors la variable aléatoire standardisée (de moyenne 0 et variance 1) suivra approximativement, lorsque \\(n\\) est grand, une distribution normale standard. Ce résultat fut ensuite progressivement généralisé par Laplace, Gauss et dautres pour devenir le théorème actuellement connu comme théorème centrale limite qui est un des deux résultats les plus importants de la théorie de probabilités. Ce théorème sert de base théorique pour expliquer un fait empirique souvent relevé, à savoir quen pratique de très nombreux phénomènes aléatoires suivent approximativement une distribution normale. On remarquera quà ce stade deux approximations de la répartition binomiale ont été proposées: lapproximation de Poisson, satisfaisante lorsque \\(n\\) est grand et lorsque \\(np\\) nest pas extrême; lapproximation normale pour laquelle on peut montrer quelle est de bonne qualité lorsque \\(np(1-p)\\) est grand (dès que \\(np(1-p)\\) dépasse 10). Figure 7.7: La loi de probabilité dune variable aléatoire \\(B( n,p )\\) devient de plus en plus normale à mesure que \\(n\\) augmente. Loi de \\(\\chi^{2}\\) de Pearson Définition 7.21 Soit \\(X_1,X_2,\\ldots,X_n\\), \\(n\\) variables normales centrées réduites, et \\(Y\\) la variable aléatoire définie par \\[Y = X_1^2 + X_2^2 + \\ldots + X_i^2 + \\ldots + X_n^2 = \\sum_{i=1}^n X_i^2\\] On dit que \\(Y\\) suit la loi de \\(\\chi^2\\) (ou loi de Pearson) à \\(n\\) degrés de liberté, \\(Y \\thicksim \\chi^2 (n)\\) La loi de \\(\\chi^2\\) trouve de nombreuses applications dans le cadre de la comparaison de proportions, des tests de conformité dune distribution observée à une distribution théorique et le test dindépendance de deux caractères qualitatifs. Ce sont les tests du khi-deux. Remarque: Si \\(n=1\\), la variable du \\(\\chi^2\\) correspond au carré dune variable normale centrée réduite \\(\\mathcal{N}(0,1)\\). Propriétés: Si \\(Y \\thicksim \\chi^2 (n)\\), alors: \\(E(Y)= n\\) \\(V(Y) = 2n\\) Laugmentation de degré de liberté (\\(n\\)) déplace la distribution vers la droite (le mode devient plus grand) et augmente la dispersion (la variance de la distribution augmente). Loi de Student \\(St(n)\\) Définition 7.22 Soit \\(U\\) une variable aléatoire suivant une loi normale centrée réduite \\(\\mathcal{N}(0,1)\\) et \\(V\\) une variable aléatoire suivant une loi de \\(\\chi^2(n)\\), \\(U\\) et \\(V\\) étant indépendantes, on dit alors que \\(T_n = \\frac{U}{\\sqrt{\\frac{V}{n}}}\\) suit une loi de Student à \\(n\\) degrés de liberté. \\(T_n \\thicksim St(n)\\) La loi de Student est utilisée lors des tests de comparaison de paramètres comme la moyenne et dans lestimation de paramètres de la population à partir de données sur un échantillon (Test de Student). Le graphique de plusieurs densités de la loi de Student illustre le suivant: à mesure que les degrés de liberté augmentent, la forme de la densité de Student se rapproche de celle dune courbe de la loi normale centrée réduite. Déjà pour \\(n = 25\\), nous trouvons peu de différence avec la densité normale standard. Si \\(n\\) est petit, nous trouvons que la distribution a des queues plus lourdes quune normale, cest-à-dire quelle a la forme dune cloche plus grosse. Loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\) Définition 7.23 Soit \\(U\\) et \\(V\\) deux variables aléatoires indépendantes suivant une loi de \\(\\chi^2\\) respectivement à \\(n\\) et \\(m\\) degrés de liberté. On dit que \\(F= \\frac{U/n}{V/m}\\) suit une loi de Fisher-Snedecor à \\((n,m)\\) degrés de liberté. \\(F \\thicksim \\mathcal{F}(n,m)\\) La loi de Fisher-Snedecor est utilisée pour comparer deux variances observées et sert surtout dans les très nombreux tests danalyse de variance et de covariance. Couple de variables aléatoires continues Densité conjointe Définition 7.24 On dit que \\((X,Y)\\) est un couple aléatoire continu sil existe une fonction \\(f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) telle que pour tout \\(D \\subseteq \\mathbb{R}\\) on a \\[P\\{(X,Y) \\in D\\} = \\iint\\limits_{(x,y) \\in D} f(x,y) dx dy\\] Remarque: On a la condition de normalité \\(\\iint\\limits_{\\mathbb{R}^2} f(x,y)dxdy=1\\) La fonction \\(f\\) sappelle densité conjointe de \\(X\\) et \\(Y\\). Notons par \\(A\\) et \\(B\\) deux ensembles de nombres réels. En définissant \\(D=\\{(x,y) : x \\in A, y \\in B\\}\\), on obtient \\[P(X\\in A, Y \\in B) = \\int_A \\int_B f(x,y) dxdy\\] La fonction de répartition du \\((X,Y)\\) est définie par \\[F(a,b)=P(X \\le a, Y \\le b) = \\int_{- \\infty}^b \\int_{- \\infty}^a f(x,y) dx dy\\] \\(f\\) est le dérivé de \\(F\\): \\(f(a,b)= \\frac{\\partial^2}{\\partial a \\partial b} F(a,b)\\) Soit \\((X,Y)\\) un couple aléatoire continu de densité \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} a x y^2 &amp; \\mbox{si} \\quad 0 \\le x \\le y \\le 1 \\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Trouver la constante \\(a\\). Soit \\((X,Y)\\) un couple aléatoire continu de densité \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} 2 e^{-x} e^{-2y} &amp; \\mbox{si} \\quad x &gt; 0, \\,\\, y &gt; 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Montrer que: \\(P(X &gt; 1, Y &lt; 1)=e^{-1}(1-e^{-2})\\) \\(P(X &lt; a) = 1-e^{-a}\\) \\(P(X &lt; Y ) = 1/3\\) Densités marginales Si on dispose de la densité du couple, on peut retrouver les densités de \\(X\\) et de \\(Y\\), appelées les densités marginales: Densité marginale de X: \\[f(x,.)=f_X(x)=\\int_{\\mathbb{R}} f(x,y)dy\\] Densité marginale de Y: \\[f(.,y)=f_Y(y)=\\int_{\\mathbb{R}} f(x,y)dx\\] Espérance dune fonction du couple Si \\((X,Y)\\) est un couple continu de densité \\(f(x,y)\\) et \\(g: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) on a \\[E[ g(X,Y)] = \\iint\\limits_{\\mathbb{R}^2} g(x,y) f(x,y)dxdy\\] Indépendance Les v.a. \\(X\\) et \\(Y\\) sont indépendantes ssi \\(\\forall \\, (x,y) \\in \\mathbb{R}^2\\) on a \\[f(x,y)=f_X(x) f_Y(y)\\] Distribution conditionnelle Si \\((X,Y)\\) est un couple continu de densité \\(f(x,y)\\), on définit densité conditionnelle de \\(X\\), sous la condition \\(Y=y\\) et lorsque \\(f_Y(y) &gt; 0\\) par la relation \\[f_{X|Y} (x|y) = \\frac{f(x,y)}{f_Y(y)}\\] Supposons que \\(X\\) et \\(Y\\) aient pour densité conjointe \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{y} e^{- x/y}e^{-y} &amp; \\mbox{si} \\quad x &gt; 0, \\,\\, y &gt; 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Déterminer la densité conditionnelle de \\(X\\) lorsque \\(Y=y\\). Calculer \\(P(X&gt;1 | Y = y)\\) "],["app-introRStudio.html", "A Introduction to RStudio", " A Introduction to RStudio RStudio is the most employed Integrated Development Environment (IDE) for nowadays. When you start RStudio you will see a window similar to Figure A.1. There are a lot of items in the GUI, most of them described in the RStudio IDE Cheat Sheet . The most important things to keep in mind are: The code is written in scripts in the source panel (upper-right panel in Figure A.1); for running a line or code selection from the script in the console (first tab in the lower-right panel in Figure A.1), you do it with the keyboard shortcut 'Ctrl+Enter' (Windows and Linux) or 'Cmd+Enter' (Mac OS X). Figure A.1: Main window of RStudio. The red shows the code panel and the yellow shows the console output. Extracted from here. "],["tab-normale1.html", "B Table 1 de la loi Normale centrée réduite", " B Table 1 de la loi Normale centrée réduite \\(X\\) étant une variable aléatoire de loi \\(\\mathcal{N}(0,1)\\), la table donne la valeur de \\(\\Phi(a)=P(X\\leq a)\\). En , la commande correspondante est pnorm(a). \\(a\\) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 Par exemple, pour \\(x = 1.23\\) (intersection de la ligne 1.2 et de la colonne 0.03), on obtient : \\(\\Phi(1.23) \\approx 0.8907\\). "],["tab-normale2.html", "C Table 2 de la loi Normale centrée réduite", " C Table 2 de la loi Normale centrée réduite \\(X\\) étant une variable aléatoire de loi \\(\\mathcal{N}(0,1)\\) et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(z_{1-\\frac{\\alpha}{2}}=\\Phi^{-1}(1-\\frac{\\alpha}{2})\\) telle que \\(P(|X| &gt; z_{1-\\frac{\\alpha}{2}})=\\alpha\\). En , la commande correspondante est qnorm(1-alpha/2). \\(\\alpha\\) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 Inf 2.5758 2.3263 2.1701 2.0537 1.9600 1.8808 1.8119 1.7507 1.6954 0.1 1.6449 1.5982 1.5548 1.5141 1.4758 1.4395 1.4051 1.3722 1.3408 1.3106 0.2 1.2816 1.2536 1.2265 1.2004 1.1750 1.1503 1.1264 1.1031 1.0803 1.0581 0.3 1.0364 1.0152 0.9945 0.9741 0.9542 0.9346 0.9154 0.8965 0.8779 0.8596 0.4 0.8416 0.8239 0.8064 0.7892 0.7722 0.7554 0.7388 0.7225 0.7063 0.6903 0.5 0.6745 0.6588 0.6433 0.6280 0.6128 0.5978 0.5828 0.5681 0.5534 0.5388 0.6 0.5244 0.5101 0.4959 0.4817 0.4677 0.4538 0.4399 0.4261 0.4125 0.3989 0.7 0.3853 0.3719 0.3585 0.3451 0.3319 0.3186 0.3055 0.2924 0.2793 0.2663 0.8 0.2533 0.2404 0.2275 0.2147 0.2019 0.1891 0.1764 0.1637 0.1510 0.1383 0.9 0.1257 0.1130 0.1004 0.0878 0.0753 0.0627 0.0502 0.0376 0.0251 0.0125 "],["tab-student.html", "D Table de la loi de Student", " D Table de la loi de Student Attention pour la description de cette table. Ici on donne directement le quantile \\(t_{n,1-\\frac{\\alpha}{2}}\\). \\(X\\) étant une variable aléatoire de loi de Student à \\(n\\) degrés de liberté \\(St(n)\\) et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(t_{n,1-\\frac{\\alpha}{2}} = F^{-1}(1-\\frac{\\alpha}{2})\\) telle que \\(P(|X| &gt; t_{n,1-\\frac{\\alpha}{2}})=\\alpha\\). En , la commande correspondante est qt(1-alpha/2, n). \\(n \\backslash \\alpha\\) 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.05 0.02 0.01 0.001 1 0.158 0.325 0.510 0.727 1.000 1.376 1.96 3.08 6.31 12.71 31.82 63.66 636.62 2 0.142 0.289 0.445 0.617 0.816 1.061 1.39 1.89 2.92 4.30 6.96 9.93 31.60 3 0.137 0.277 0.424 0.584 0.765 0.978 1.25 1.64 2.35 3.18 4.54 5.84 12.92 4 0.134 0.271 0.414 0.569 0.741 0.941 1.19 1.53 2.13 2.78 3.75 4.60 8.61 5 0.132 0.267 0.408 0.559 0.727 0.920 1.16 1.48 2.02 2.57 3.37 4.03 6.87 6 0.131 0.265 0.404 0.553 0.718 0.906 1.13 1.44 1.94 2.45 3.14 3.71 5.96 7 0.130 0.263 0.402 0.549 0.711 0.896 1.12 1.42 1.90 2.37 3.00 3.50 5.41 8 0.130 0.262 0.399 0.546 0.706 0.889 1.11 1.40 1.86 2.31 2.90 3.35 5.04 9 0.129 0.261 0.398 0.543 0.703 0.883 1.10 1.38 1.83 2.26 2.82 3.25 4.78 10 0.129 0.260 0.397 0.542 0.700 0.879 1.09 1.37 1.81 2.23 2.76 3.17 4.59 11 0.129 0.260 0.396 0.540 0.697 0.876 1.09 1.36 1.80 2.20 2.72 3.11 4.44 12 0.128 0.259 0.395 0.539 0.695 0.873 1.08 1.36 1.78 2.18 2.68 3.06 4.32 13 0.128 0.259 0.394 0.538 0.694 0.870 1.08 1.35 1.77 2.16 2.65 3.01 4.22 14 0.128 0.258 0.393 0.537 0.692 0.868 1.08 1.34 1.76 2.14 2.62 2.98 4.14 15 0.128 0.258 0.393 0.536 0.691 0.866 1.07 1.34 1.75 2.13 2.60 2.95 4.07 16 0.128 0.258 0.392 0.535 0.690 0.865 1.07 1.34 1.75 2.12 2.58 2.92 4.01 17 0.128 0.257 0.392 0.534 0.689 0.863 1.07 1.33 1.74 2.11 2.57 2.90 3.96 18 0.127 0.257 0.392 0.534 0.688 0.862 1.07 1.33 1.73 2.10 2.55 2.88 3.92 19 0.127 0.257 0.391 0.533 0.688 0.861 1.07 1.33 1.73 2.09 2.54 2.86 3.88 20 0.127 0.257 0.391 0.533 0.687 0.860 1.06 1.32 1.73 2.09 2.53 2.85 3.85 21 0.127 0.257 0.391 0.532 0.686 0.859 1.06 1.32 1.72 2.08 2.52 2.83 3.82 22 0.127 0.256 0.390 0.532 0.686 0.858 1.06 1.32 1.72 2.07 2.51 2.82 3.79 23 0.127 0.256 0.390 0.532 0.685 0.858 1.06 1.32 1.71 2.07 2.50 2.81 3.77 24 0.127 0.256 0.390 0.531 0.685 0.857 1.06 1.32 1.71 2.06 2.49 2.80 3.75 25 0.127 0.256 0.390 0.531 0.684 0.856 1.06 1.32 1.71 2.06 2.48 2.79 3.73 26 0.127 0.256 0.390 0.531 0.684 0.856 1.06 1.31 1.71 2.06 2.48 2.78 3.71 27 0.127 0.256 0.389 0.531 0.684 0.855 1.06 1.31 1.70 2.05 2.47 2.77 3.69 28 0.127 0.256 0.389 0.530 0.683 0.855 1.06 1.31 1.70 2.05 2.47 2.76 3.67 29 0.127 0.256 0.389 0.530 0.683 0.854 1.05 1.31 1.70 2.04 2.46 2.76 3.66 30 0.127 0.256 0.389 0.530 0.683 0.854 1.05 1.31 1.70 2.04 2.46 2.75 3.65 40 0.126 0.255 0.388 0.529 0.681 0.851 1.05 1.30 1.68 2.02 2.42 2.70 3.55 80 0.126 0.254 0.387 0.526 0.678 0.846 1.04 1.29 1.66 1.99 2.37 2.64 3.42 120 0.126 0.254 0.386 0.526 0.677 0.845 1.04 1.29 1.66 1.98 2.36 2.62 3.37 \\(+\\infty\\) 0.126 0.253 0.385 0.524 0.674 0.842 1.04 1.28 1.64 1.96 2.33 2.58 3.29 "],["tab-khideux.html", "E Table de la loi de Khi-deux \\(\\chi^2\\)", " E Table de la loi de Khi-deux \\(\\chi^2\\) \\(X\\) étant une variable aléatoire de loi de \\(\\chi^2\\) à \\(n\\) degrés de liberté et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(z_{n,\\alpha} = F_{\\chi_n^2}^{-1}(1-\\alpha)\\) telle que \\(P(X &gt; z_{n,\\alpha})=\\alpha\\). En , la commande correspondante est qchisq(1-alpha, n). \\(n \\backslash \\alpha\\) 0.995 0.99 0.975 0.95 0.9 0.8 0.7 0.5 0.3 0.2 0.1 0.05 0.025 0.01 0.005 0.001 1 0.000 0.000 0.001 0.004 0.016 0.064 0.148 0.455 1.07 1.64 2.71 3.84 5.02 6.63 7.88 10.8 2 0.010 0.020 0.051 0.103 0.211 0.446 0.713 1.386 2.41 3.22 4.61 5.99 7.38 9.21 10.60 13.8 3 0.072 0.115 0.216 0.352 0.584 1.005 1.424 2.366 3.66 4.64 6.25 7.82 9.35 11.35 12.84 16.3 4 0.207 0.297 0.484 0.711 1.064 1.649 2.195 3.357 4.88 5.99 7.78 9.49 11.14 13.28 14.86 18.5 5 0.412 0.554 0.831 1.145 1.610 2.343 3.000 4.351 6.06 7.29 9.24 11.07 12.83 15.09 16.75 20.5 6 0.676 0.872 1.237 1.635 2.204 3.070 3.828 5.348 7.23 8.56 10.64 12.59 14.45 16.81 18.55 22.5 7 0.989 1.239 1.690 2.167 2.833 3.822 4.671 6.346 8.38 9.80 12.02 14.07 16.01 18.48 20.28 24.3 8 1.344 1.646 2.180 2.733 3.490 4.594 5.527 7.344 9.52 11.03 13.36 15.51 17.54 20.09 21.95 26.1 9 1.735 2.088 2.700 3.325 4.168 5.380 6.393 8.343 10.66 12.24 14.68 16.92 19.02 21.67 23.59 27.9 10 2.156 2.558 3.247 3.940 4.865 6.179 7.267 9.342 11.78 13.44 15.99 18.31 20.48 23.21 25.19 29.6 11 2.603 3.053 3.816 4.575 5.578 6.989 8.148 10.341 12.90 14.63 17.27 19.68 21.92 24.73 26.76 31.3 12 3.074 3.571 4.404 5.226 6.304 7.807 9.034 11.340 14.01 15.81 18.55 21.03 23.34 26.22 28.30 32.9 13 3.565 4.107 5.009 5.892 7.042 8.634 9.926 12.340 15.12 16.98 19.81 22.36 24.74 27.69 29.82 34.5 14 4.075 4.660 5.629 6.571 7.790 9.467 10.821 13.339 16.22 18.15 21.06 23.68 26.12 29.14 31.32 36.1 15 4.601 5.229 6.262 7.261 8.547 10.307 11.721 14.339 17.32 19.31 22.31 25.00 27.49 30.58 32.80 37.7 16 5.142 5.812 6.908 7.962 9.312 11.152 12.624 15.338 18.42 20.46 23.54 26.30 28.84 32.00 34.27 39.3 17 5.697 6.408 7.564 8.672 10.085 12.002 13.531 16.338 19.51 21.61 24.77 27.59 30.19 33.41 35.72 40.8 18 6.265 7.015 8.231 9.390 10.865 12.857 14.440 17.338 20.60 22.76 25.99 28.87 31.53 34.80 37.16 42.3 19 6.844 7.633 8.907 10.117 11.651 13.716 15.352 18.338 21.69 23.90 27.20 30.14 32.85 36.19 38.58 43.8 20 7.434 8.260 9.591 10.851 12.443 14.578 16.266 19.337 22.77 25.04 28.41 31.41 34.17 37.57 40.00 45.3 21 8.034 8.897 10.283 11.591 13.240 15.445 17.182 20.337 23.86 26.17 29.61 32.67 35.48 38.93 41.40 46.8 22 8.643 9.542 10.982 12.338 14.041 16.314 18.101 21.337 24.94 27.30 30.81 33.92 36.78 40.29 42.80 48.3 23 9.260 10.196 11.689 13.091 14.848 17.187 19.021 22.337 26.02 28.43 32.01 35.17 38.08 41.64 44.18 49.7 24 9.886 10.856 12.401 13.848 15.659 18.062 19.943 23.337 27.10 29.55 33.20 36.41 39.36 42.98 45.56 51.2 25 10.520 11.524 13.120 14.611 16.473 18.940 20.867 24.337 28.17 30.68 34.38 37.65 40.65 44.31 46.93 52.6 26 11.160 12.198 13.844 15.379 17.292 19.820 21.792 25.336 29.25 31.80 35.56 38.88 41.92 45.64 48.29 54.1 27 11.808 12.879 14.573 16.151 18.114 20.703 22.719 26.336 30.32 32.91 36.74 40.11 43.20 46.96 49.65 55.5 28 12.461 13.565 15.308 16.928 18.939 21.588 23.647 27.336 31.39 34.03 37.92 41.34 44.46 48.28 50.99 56.9 29 13.121 14.256 16.047 17.708 19.768 22.475 24.577 28.336 32.46 35.14 39.09 42.56 45.72 49.59 52.34 58.3 30 13.787 14.953 16.791 18.493 20.599 23.364 25.508 29.336 33.53 36.25 40.26 43.77 46.98 50.89 53.67 59.7 "],["tab-fisher.html", "F Tables de la loi de Fisher \\(F\\) F.1 Pour \\(\\alpha = 5\\%\\) F.2 Pour \\(\\alpha = 2.5\\%\\) F.3 Pour \\(\\alpha = 1\\%\\) F.4 Pour \\(\\alpha = 0.05\\%\\)", " F Tables de la loi de Fisher \\(F\\) \\(X\\) étant une variable aléatoire de loi \\(F(\\nu_1,\\nu_2)\\), les tables donnent les valeurs de \\(f_{1-\\alpha; \\,\\nu_1,\\nu_2}= F_{F(\\nu_1,\\nu_2)}^{-1}(1-\\alpha)\\) telles que \\(P(X&lt; f_{1-\\alpha; \\,\\nu_1,\\nu_2})=1-\\alpha\\) pour \\(\\alpha = 5\\%\\) et \\(\\alpha =1\\%\\). En , la commande correspondante est qf(1-alpha, nu1, nu2). Remarque: \\(f_{\\alpha; \\, \\nu_1,\\nu_2} = \\frac{1}{f_{1-\\alpha; \\, \\nu_2,\\nu_1}}\\) F.1 Pour \\(\\alpha = 5\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88 243.91 246.46 248.01 249.05 251.14 252.20 253.04 254.31 2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.39 19.40 19.41 19.43 19.45 19.45 19.47 19.48 19.49 19.50 3 10.13 9.55 9.28 9.12 9.01 8.94 8.89 8.85 8.81 8.79 8.74 8.69 8.66 8.64 8.59 8.57 8.55 8.53 4 7.71 6.94 6.59 6.39 6.26 6.16 6.09 6.04 6.00 5.96 5.91 5.84 5.80 5.77 5.72 5.69 5.66 5.63 5 6.61 5.79 5.41 5.19 5.05 4.95 4.88 4.82 4.77 4.74 4.68 4.60 4.56 4.53 4.46 4.43 4.41 4.37 6 5.99 5.14 4.76 4.53 4.39 4.28 4.21 4.15 4.10 4.06 4.00 3.92 3.87 3.84 3.77 3.74 3.71 3.67 7 5.59 4.74 4.35 4.12 3.97 3.87 3.79 3.73 3.68 3.64 3.58 3.49 3.44 3.41 3.34 3.30 3.27 3.23 8 5.32 4.46 4.07 3.84 3.69 3.58 3.50 3.44 3.39 3.35 3.28 3.20 3.15 3.12 3.04 3.00 2.98 2.93 9 5.12 4.26 3.86 3.63 3.48 3.37 3.29 3.23 3.18 3.14 3.07 2.99 2.94 2.90 2.83 2.79 2.76 2.71 10 4.96 4.10 3.71 3.48 3.33 3.22 3.13 3.07 3.02 2.98 2.91 2.83 2.77 2.74 2.66 2.62 2.59 2.54 11 4.84 3.98 3.59 3.36 3.20 3.10 3.01 2.95 2.90 2.85 2.79 2.70 2.65 2.61 2.53 2.49 2.46 2.40 12 4.75 3.88 3.49 3.26 3.11 3.00 2.91 2.85 2.80 2.75 2.69 2.60 2.54 2.50 2.43 2.38 2.35 2.30 13 4.67 3.81 3.41 3.18 3.02 2.92 2.83 2.77 2.71 2.67 2.60 2.52 2.46 2.42 2.34 2.30 2.26 2.21 14 4.60 3.74 3.34 3.11 2.96 2.85 2.76 2.70 2.65 2.60 2.53 2.44 2.39 2.35 2.27 2.22 2.19 2.13 15 4.54 3.68 3.29 3.06 2.90 2.79 2.71 2.64 2.59 2.54 2.48 2.38 2.33 2.29 2.20 2.16 2.12 2.07 16 4.49 3.63 3.24 3.01 2.85 2.74 2.66 2.59 2.54 2.49 2.42 2.33 2.28 2.23 2.15 2.11 2.07 2.01 17 4.45 3.59 3.20 2.96 2.81 2.70 2.61 2.55 2.49 2.45 2.38 2.29 2.23 2.19 2.10 2.06 2.02 1.96 18 4.41 3.56 3.16 2.93 2.77 2.66 2.58 2.51 2.46 2.41 2.34 2.25 2.19 2.15 2.06 2.02 1.98 1.92 19 4.38 3.52 3.13 2.90 2.74 2.63 2.54 2.48 2.42 2.38 2.31 2.21 2.15 2.11 2.03 1.98 1.94 1.88 20 4.35 3.49 3.10 2.87 2.71 2.60 2.51 2.45 2.39 2.35 2.28 2.18 2.12 2.08 1.99 1.95 1.91 1.84 21 4.33 3.47 3.07 2.84 2.68 2.57 2.49 2.42 2.37 2.32 2.25 2.16 2.10 2.05 1.97 1.92 1.88 1.81 22 4.30 3.44 3.05 2.82 2.66 2.55 2.46 2.40 2.34 2.30 2.23 2.13 2.07 2.03 1.94 1.89 1.85 1.78 23 4.28 3.42 3.03 2.80 2.64 2.53 2.44 2.38 2.32 2.27 2.20 2.11 2.05 2.00 1.91 1.86 1.82 1.76 24 4.26 3.40 3.01 2.78 2.62 2.51 2.42 2.35 2.30 2.25 2.18 2.09 2.03 1.98 1.89 1.84 1.80 1.73 25 4.24 3.38 2.99 2.76 2.60 2.49 2.40 2.34 2.28 2.24 2.16 2.07 2.01 1.96 1.87 1.82 1.78 1.71 26 4.22 3.37 2.98 2.74 2.59 2.47 2.39 2.32 2.27 2.22 2.15 2.05 1.99 1.95 1.85 1.80 1.76 1.69 27 4.21 3.35 2.96 2.73 2.57 2.46 2.37 2.31 2.25 2.20 2.13 2.04 1.97 1.93 1.84 1.78 1.74 1.67 28 4.20 3.34 2.95 2.71 2.56 2.44 2.36 2.29 2.24 2.19 2.12 2.02 1.96 1.92 1.82 1.77 1.73 1.65 29 4.18 3.33 2.93 2.70 2.54 2.43 2.35 2.28 2.22 2.18 2.10 2.01 1.95 1.90 1.81 1.75 1.71 1.64 30 4.17 3.32 2.92 2.69 2.53 2.42 2.33 2.27 2.21 2.16 2.09 2.00 1.93 1.89 1.79 1.74 1.70 1.62 40 4.08 3.23 2.84 2.61 2.45 2.34 2.25 2.18 2.12 2.08 2.00 1.90 1.84 1.79 1.69 1.64 1.59 1.51 50 4.03 3.18 2.79 2.56 2.40 2.29 2.20 2.13 2.07 2.03 1.95 1.85 1.78 1.74 1.63 1.58 1.52 1.44 60 4.00 3.15 2.76 2.52 2.37 2.25 2.17 2.10 2.04 1.99 1.92 1.81 1.75 1.70 1.59 1.53 1.48 1.39 100 3.94 3.09 2.70 2.46 2.31 2.19 2.10 2.03 1.98 1.93 1.85 1.75 1.68 1.63 1.51 1.45 1.39 1.28 \\(+\\infty\\) 3.84 3.00 2.60 2.37 2.21 2.10 2.01 1.94 1.88 1.83 1.75 1.64 1.57 1.52 1.39 1.32 1.24 1.00 F.2 Pour \\(\\alpha = 2.5\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 647.79 799.50 864.16 899.58 921.85 937.11 948.22 956.66 963.28 968.63 976.71 986.92 993.10 997.25 1005.60 1009.80 1013.17 1018.26 2 38.51 39.00 39.16 39.25 39.30 39.33 39.35 39.37 39.39 39.40 39.41 39.44 39.45 39.46 39.47 39.48 39.49 39.50 3 17.44 16.04 15.44 15.10 14.88 14.73 14.62 14.54 14.47 14.42 14.34 14.23 14.17 14.12 14.04 13.99 13.96 13.90 4 12.22 10.65 9.98 9.61 9.36 9.20 9.07 8.98 8.90 8.84 8.75 8.63 8.56 8.51 8.41 8.36 8.32 8.26 5 10.01 8.43 7.76 7.39 7.15 6.98 6.85 6.76 6.68 6.62 6.53 6.40 6.33 6.28 6.17 6.12 6.08 6.01 6 8.81 7.26 6.60 6.23 5.99 5.82 5.70 5.60 5.52 5.46 5.37 5.24 5.17 5.12 5.01 4.96 4.92 4.85 7 8.07 6.54 5.89 5.52 5.29 5.12 5.00 4.90 4.82 4.76 4.67 4.54 4.47 4.42 4.31 4.25 4.21 4.14 8 7.57 6.06 5.42 5.05 4.82 4.65 4.53 4.43 4.36 4.29 4.20 4.08 4.00 3.95 3.84 3.78 3.74 3.67 9 7.21 5.71 5.08 4.72 4.48 4.32 4.20 4.10 4.03 3.96 3.87 3.74 3.67 3.61 3.50 3.45 3.40 3.33 10 6.94 5.46 4.83 4.47 4.24 4.07 3.95 3.85 3.78 3.72 3.62 3.50 3.42 3.37 3.25 3.20 3.15 3.08 11 6.72 5.26 4.63 4.28 4.04 3.88 3.76 3.66 3.59 3.53 3.43 3.30 3.23 3.17 3.06 3.00 2.96 2.88 12 6.55 5.10 4.47 4.12 3.89 3.73 3.61 3.51 3.44 3.37 3.28 3.15 3.07 3.02 2.91 2.85 2.80 2.73 13 6.41 4.96 4.35 4.00 3.77 3.60 3.48 3.39 3.31 3.25 3.15 3.03 2.95 2.89 2.78 2.72 2.67 2.60 14 6.30 4.86 4.24 3.89 3.66 3.50 3.38 3.29 3.21 3.15 3.05 2.92 2.84 2.79 2.67 2.61 2.56 2.49 15 6.20 4.76 4.15 3.80 3.58 3.42 3.29 3.20 3.12 3.06 2.96 2.84 2.76 2.70 2.58 2.52 2.47 2.40 16 6.12 4.69 4.08 3.73 3.50 3.34 3.22 3.12 3.05 2.99 2.89 2.76 2.68 2.62 2.51 2.45 2.40 2.32 17 6.04 4.62 4.01 3.66 3.44 3.28 3.16 3.06 2.98 2.92 2.83 2.70 2.62 2.56 2.44 2.38 2.33 2.25 18 5.98 4.56 3.95 3.61 3.38 3.22 3.10 3.00 2.93 2.87 2.77 2.64 2.56 2.50 2.38 2.32 2.27 2.19 19 5.92 4.51 3.90 3.56 3.33 3.17 3.05 2.96 2.88 2.82 2.72 2.59 2.51 2.45 2.33 2.27 2.22 2.13 20 5.87 4.46 3.86 3.52 3.29 3.13 3.01 2.91 2.84 2.77 2.68 2.55 2.46 2.41 2.29 2.22 2.17 2.08 21 5.83 4.42 3.82 3.48 3.25 3.09 2.97 2.87 2.80 2.73 2.64 2.51 2.42 2.37 2.25 2.18 2.13 2.04 22 5.79 4.38 3.78 3.44 3.21 3.06 2.93 2.84 2.76 2.70 2.60 2.47 2.39 2.33 2.21 2.14 2.09 2.00 23 5.75 4.35 3.75 3.41 3.18 3.02 2.90 2.81 2.73 2.67 2.57 2.44 2.36 2.30 2.18 2.11 2.06 1.97 24 5.72 4.32 3.72 3.38 3.15 3.00 2.87 2.78 2.70 2.64 2.54 2.41 2.33 2.27 2.15 2.08 2.02 1.94 25 5.69 4.29 3.69 3.35 3.13 2.97 2.85 2.75 2.68 2.61 2.52 2.38 2.30 2.24 2.12 2.05 2.00 1.91 26 5.66 4.26 3.67 3.33 3.10 2.94 2.82 2.73 2.65 2.59 2.49 2.36 2.28 2.22 2.09 2.03 1.97 1.88 27 5.63 4.24 3.65 3.31 3.08 2.92 2.80 2.71 2.63 2.57 2.47 2.34 2.25 2.19 2.07 2.00 1.95 1.85 28 5.61 4.22 3.63 3.29 3.06 2.90 2.78 2.69 2.61 2.55 2.45 2.32 2.23 2.17 2.05 1.98 1.92 1.83 29 5.59 4.20 3.61 3.27 3.04 2.88 2.76 2.67 2.59 2.53 2.43 2.30 2.21 2.15 2.03 1.96 1.90 1.81 30 5.57 4.18 3.59 3.25 3.03 2.87 2.75 2.65 2.58 2.51 2.41 2.28 2.19 2.14 2.01 1.94 1.88 1.79 40 5.42 4.05 3.46 3.13 2.90 2.74 2.62 2.53 2.45 2.39 2.29 2.15 2.07 2.01 1.88 1.80 1.74 1.64 50 5.34 3.98 3.39 3.05 2.83 2.67 2.55 2.46 2.38 2.32 2.22 2.08 1.99 1.93 1.80 1.72 1.66 1.54 60 5.29 3.92 3.34 3.01 2.79 2.63 2.51 2.41 2.33 2.27 2.17 2.03 1.94 1.88 1.74 1.67 1.60 1.48 100 5.18 3.83 3.25 2.92 2.70 2.54 2.42 2.32 2.24 2.18 2.08 1.94 1.85 1.78 1.64 1.56 1.48 1.35 \\(+\\infty\\) 5.02 3.69 3.12 2.79 2.57 2.41 2.29 2.19 2.11 2.05 1.95 1.80 1.71 1.64 1.48 1.39 1.30 1.00 F.3 Pour \\(\\alpha = 1\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 4052.18 4999.50 5403.35 5624.58 5763.65 5858.99 5928.36 5981.07 6022.47 6055.85 6106.32 6170.10 6208.73 6234.63 6286.78 6313.03 6334.11 6365.86 2 98.50 99.00 99.17 99.25 99.30 99.33 99.36 99.37 99.39 99.40 99.42 99.44 99.45 99.46 99.47 99.48 99.49 99.50 3 34.12 30.82 29.46 28.71 28.24 27.91 27.67 27.49 27.34 27.23 27.05 26.83 26.69 26.60 26.41 26.32 26.24 26.12 4 21.20 18.00 16.69 15.98 15.52 15.21 14.98 14.80 14.66 14.55 14.37 14.15 14.02 13.93 13.74 13.65 13.58 13.46 5 16.26 13.27 12.06 11.39 10.97 10.67 10.46 10.29 10.16 10.05 9.89 9.68 9.55 9.47 9.29 9.20 9.13 9.02 6 13.74 10.93 9.78 9.15 8.75 8.47 8.26 8.10 7.98 7.87 7.72 7.52 7.40 7.31 7.14 7.06 6.99 6.88 7 12.25 9.55 8.45 7.85 7.46 7.19 6.99 6.84 6.72 6.62 6.47 6.28 6.16 6.07 5.91 5.82 5.75 5.65 8 11.26 8.65 7.59 7.01 6.63 6.37 6.18 6.03 5.91 5.81 5.67 5.48 5.36 5.28 5.12 5.03 4.96 4.86 9 10.56 8.02 6.99 6.42 6.06 5.80 5.61 5.47 5.35 5.26 5.11 4.92 4.81 4.73 4.57 4.48 4.42 4.31 10 10.04 7.56 6.55 5.99 5.64 5.39 5.20 5.06 4.94 4.85 4.71 4.52 4.41 4.33 4.16 4.08 4.01 3.91 11 9.65 7.21 6.22 5.67 5.32 5.07 4.89 4.74 4.63 4.54 4.40 4.21 4.10 4.02 3.86 3.78 3.71 3.60 12 9.33 6.93 5.95 5.41 5.06 4.82 4.64 4.50 4.39 4.30 4.16 3.97 3.86 3.78 3.62 3.54 3.47 3.36 13 9.07 6.70 5.74 5.21 4.86 4.62 4.44 4.30 4.19 4.10 3.96 3.78 3.66 3.59 3.42 3.34 3.27 3.16 14 8.86 6.51 5.56 5.04 4.70 4.46 4.28 4.14 4.03 3.94 3.80 3.62 3.50 3.43 3.27 3.18 3.11 3.00 15 8.68 6.36 5.42 4.89 4.56 4.32 4.14 4.00 3.90 3.81 3.67 3.48 3.37 3.29 3.13 3.05 2.98 2.87 16 8.53 6.23 5.29 4.77 4.44 4.20 4.03 3.89 3.78 3.69 3.55 3.37 3.26 3.18 3.02 2.93 2.86 2.75 17 8.40 6.11 5.18 4.67 4.34 4.10 3.93 3.79 3.68 3.59 3.46 3.27 3.16 3.08 2.92 2.83 2.76 2.65 18 8.29 6.01 5.09 4.58 4.25 4.01 3.84 3.71 3.60 3.51 3.37 3.19 3.08 3.00 2.83 2.75 2.68 2.57 19 8.19 5.93 5.01 4.50 4.17 3.94 3.77 3.63 3.52 3.43 3.30 3.12 3.00 2.92 2.76 2.67 2.60 2.49 20 8.10 5.85 4.94 4.43 4.10 3.87 3.70 3.56 3.46 3.37 3.23 3.05 2.94 2.86 2.69 2.61 2.54 2.42 21 8.02 5.78 4.87 4.37 4.04 3.81 3.64 3.51 3.40 3.31 3.17 2.99 2.88 2.80 2.64 2.55 2.48 2.36 22 7.95 5.72 4.82 4.31 3.99 3.76 3.59 3.45 3.35 3.26 3.12 2.94 2.83 2.75 2.58 2.50 2.42 2.31 23 7.88 5.66 4.76 4.26 3.94 3.71 3.54 3.41 3.30 3.21 3.07 2.89 2.78 2.70 2.54 2.45 2.37 2.26 24 7.82 5.61 4.72 4.22 3.90 3.67 3.50 3.36 3.26 3.17 3.03 2.85 2.74 2.66 2.49 2.40 2.33 2.21 25 7.77 5.57 4.67 4.18 3.85 3.63 3.46 3.32 3.22 3.13 2.99 2.81 2.70 2.62 2.45 2.36 2.29 2.17 26 7.72 5.53 4.64 4.14 3.82 3.59 3.42 3.29 3.18 3.09 2.96 2.78 2.66 2.58 2.42 2.33 2.25 2.13 27 7.68 5.49 4.60 4.11 3.79 3.56 3.39 3.26 3.15 3.06 2.93 2.75 2.63 2.55 2.38 2.29 2.22 2.10 28 7.64 5.45 4.57 4.07 3.75 3.53 3.36 3.23 3.12 3.03 2.90 2.72 2.60 2.52 2.35 2.26 2.19 2.06 29 7.60 5.42 4.54 4.04 3.73 3.50 3.33 3.20 3.09 3.00 2.87 2.69 2.57 2.50 2.33 2.23 2.16 2.03 30 7.56 5.39 4.51 4.02 3.70 3.47 3.30 3.17 3.07 2.98 2.84 2.66 2.55 2.47 2.30 2.21 2.13 2.01 40 7.31 5.18 4.31 3.83 3.51 3.29 3.12 2.99 2.89 2.80 2.66 2.48 2.37 2.29 2.11 2.02 1.94 1.80 50 7.17 5.06 4.20 3.72 3.41 3.19 3.02 2.89 2.79 2.70 2.56 2.38 2.27 2.18 2.01 1.91 1.82 1.68 60 7.08 4.98 4.13 3.65 3.34 3.12 2.95 2.82 2.72 2.63 2.50 2.31 2.20 2.12 1.94 1.84 1.75 1.60 100 6.89 4.82 3.98 3.51 3.21 2.99 2.82 2.69 2.59 2.50 2.37 2.18 2.07 1.98 1.80 1.69 1.60 1.43 \\(+\\infty\\) 6.63 4.61 3.78 3.32 3.02 2.80 2.64 2.51 2.41 2.32 2.18 2.00 1.88 1.79 1.59 1.47 1.36 1.00 F.4 Pour \\(\\alpha = 0.05\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88 243.91 246.46 248.01 249.05 251.14 252.20 253.04 254.31 2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.39 19.40 19.41 19.43 19.45 19.45 19.47 19.48 19.49 19.50 3 10.13 9.55 9.28 9.12 9.01 8.94 8.89 8.85 8.81 8.79 8.74 8.69 8.66 8.64 8.59 8.57 8.55 8.53 4 7.71 6.94 6.59 6.39 6.26 6.16 6.09 6.04 6.00 5.96 5.91 5.84 5.80 5.77 5.72 5.69 5.66 5.63 5 6.61 5.79 5.41 5.19 5.05 4.95 4.88 4.82 4.77 4.74 4.68 4.60 4.56 4.53 4.46 4.43 4.41 4.37 6 5.99 5.14 4.76 4.53 4.39 4.28 4.21 4.15 4.10 4.06 4.00 3.92 3.87 3.84 3.77 3.74 3.71 3.67 7 5.59 4.74 4.35 4.12 3.97 3.87 3.79 3.73 3.68 3.64 3.58 3.49 3.44 3.41 3.34 3.30 3.27 3.23 8 5.32 4.46 4.07 3.84 3.69 3.58 3.50 3.44 3.39 3.35 3.28 3.20 3.15 3.12 3.04 3.00 2.98 2.93 9 5.12 4.26 3.86 3.63 3.48 3.37 3.29 3.23 3.18 3.14 3.07 2.99 2.94 2.90 2.83 2.79 2.76 2.71 10 4.96 4.10 3.71 3.48 3.33 3.22 3.13 3.07 3.02 2.98 2.91 2.83 2.77 2.74 2.66 2.62 2.59 2.54 11 4.84 3.98 3.59 3.36 3.20 3.10 3.01 2.95 2.90 2.85 2.79 2.70 2.65 2.61 2.53 2.49 2.46 2.40 12 4.75 3.88 3.49 3.26 3.11 3.00 2.91 2.85 2.80 2.75 2.69 2.60 2.54 2.50 2.43 2.38 2.35 2.30 13 4.67 3.81 3.41 3.18 3.02 2.92 2.83 2.77 2.71 2.67 2.60 2.52 2.46 2.42 2.34 2.30 2.26 2.21 14 4.60 3.74 3.34 3.11 2.96 2.85 2.76 2.70 2.65 2.60 2.53 2.44 2.39 2.35 2.27 2.22 2.19 2.13 15 4.54 3.68 3.29 3.06 2.90 2.79 2.71 2.64 2.59 2.54 2.48 2.38 2.33 2.29 2.20 2.16 2.12 2.07 16 4.49 3.63 3.24 3.01 2.85 2.74 2.66 2.59 2.54 2.49 2.42 2.33 2.28 2.23 2.15 2.11 2.07 2.01 17 4.45 3.59 3.20 2.96 2.81 2.70 2.61 2.55 2.49 2.45 2.38 2.29 2.23 2.19 2.10 2.06 2.02 1.96 18 4.41 3.56 3.16 2.93 2.77 2.66 2.58 2.51 2.46 2.41 2.34 2.25 2.19 2.15 2.06 2.02 1.98 1.92 19 4.38 3.52 3.13 2.90 2.74 2.63 2.54 2.48 2.42 2.38 2.31 2.21 2.15 2.11 2.03 1.98 1.94 1.88 20 4.35 3.49 3.10 2.87 2.71 2.60 2.51 2.45 2.39 2.35 2.28 2.18 2.12 2.08 1.99 1.95 1.91 1.84 21 4.33 3.47 3.07 2.84 2.68 2.57 2.49 2.42 2.37 2.32 2.25 2.16 2.10 2.05 1.97 1.92 1.88 1.81 22 4.30 3.44 3.05 2.82 2.66 2.55 2.46 2.40 2.34 2.30 2.23 2.13 2.07 2.03 1.94 1.89 1.85 1.78 23 4.28 3.42 3.03 2.80 2.64 2.53 2.44 2.38 2.32 2.27 2.20 2.11 2.05 2.00 1.91 1.86 1.82 1.76 24 4.26 3.40 3.01 2.78 2.62 2.51 2.42 2.35 2.30 2.25 2.18 2.09 2.03 1.98 1.89 1.84 1.80 1.73 25 4.24 3.38 2.99 2.76 2.60 2.49 2.40 2.34 2.28 2.24 2.16 2.07 2.01 1.96 1.87 1.82 1.78 1.71 26 4.22 3.37 2.98 2.74 2.59 2.47 2.39 2.32 2.27 2.22 2.15 2.05 1.99 1.95 1.85 1.80 1.76 1.69 27 4.21 3.35 2.96 2.73 2.57 2.46 2.37 2.31 2.25 2.20 2.13 2.04 1.97 1.93 1.84 1.78 1.74 1.67 28 4.20 3.34 2.95 2.71 2.56 2.44 2.36 2.29 2.24 2.19 2.12 2.02 1.96 1.92 1.82 1.77 1.73 1.65 29 4.18 3.33 2.93 2.70 2.54 2.43 2.35 2.28 2.22 2.18 2.10 2.01 1.95 1.90 1.81 1.75 1.71 1.64 30 4.17 3.32 2.92 2.69 2.53 2.42 2.33 2.27 2.21 2.16 2.09 2.00 1.93 1.89 1.79 1.74 1.70 1.62 40 4.08 3.23 2.84 2.61 2.45 2.34 2.25 2.18 2.12 2.08 2.00 1.90 1.84 1.79 1.69 1.64 1.59 1.51 50 4.03 3.18 2.79 2.56 2.40 2.29 2.20 2.13 2.07 2.03 1.95 1.85 1.78 1.74 1.63 1.58 1.52 1.44 60 4.00 3.15 2.76 2.52 2.37 2.25 2.17 2.10 2.04 1.99 1.92 1.81 1.75 1.70 1.59 1.53 1.48 1.39 100 3.94 3.09 2.70 2.46 2.31 2.19 2.10 2.03 1.98 1.93 1.85 1.75 1.68 1.63 1.51 1.45 1.39 1.28 \\(+\\infty\\) 3.84 3.00 2.60 2.37 2.21 2.10 2.01 1.94 1.88 1.83 1.75 1.64 1.57 1.52 1.39 1.32 1.24 1.00 "],["notes-et-examens.html", "G Notes et examens Sujets &amp; Corrections Notes Analyse Notes", " G Notes et examens Cette annexe est dédiée pour les examens. Ici vous pouvez trouver les sujets, les corrections, vos notes et une analyse des notes. Sujets &amp; Corrections Sujet CC: Sujet examen: Notes Dans lapplication suivante vous pouvez récupérer vos notes (CC, examen et Note Finale) en saisissant votre n° de badge (par exemple 709954). Analyse Notes Comparaison des notes de lexamen entre les groupes: Comparaison des notes finales entre les groupes. Sur cette figure, votre nom est affiché si votre note est &gt; 15. Un histogramme des notes finales, on dirait que la loi de la note est une loi Normale "],["références-et-crédits.html", "Références et Crédits", " Références et Crédits Le contenu de ce cours a été majoritairement inspiré des références suivantes (certaines parties ont été transcrites mots par mots): Principes et Méthodes Statistiques: Olivier Gaudoin. Statistiques Inférentielles: Julien Jacques. Probabilités et Statistique pour ingénieurs: W.Hines, D. Montgomery et autres. Ce site de cours a été rédigé en Rmarkdown et compilé sur en utilisant bookdown. "]]
