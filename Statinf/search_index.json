[
["index.html", "Statistique Inférentielle Overview Plan du cours", " Statistique Inférentielle Mohamad Ghassany 2019-12-15 Overview Ce document est dédié pour le cours de Statistique Inférentielle destiné aux étudiants en année 3 de l’École supérieure d’ingénieurs Léonard-de-Vinci. Plan du cours Séance Contenu Séance 1 Statistique descriptive, TP Prise en main sur R Séance 2 Echantillonage, Théorèmes limites, lois des \\(\\overline{X}_n, S_n^2\\) et \\(P_n\\), TP sur l’illustration des théorèmes limites Séance 3 Estimation ponctuelle et recherche d’estimateur, Méthode des moments, Méthode du maximum de vraissemblance Séance 4 Séance de TP: Suite du TP sur les théorèmes limites, TP sur l’estimation ponctuelle Contrôle continu Samedi le 30 novembre 2019 Séance 5 Estimation par Intervalle de Confiance Séance 6 Suite exercices IC et TP Séance 7 Tests d’Hypothèses: Introduction Tests d’hypothèses à partir d’un seul échantillon Séance 8 Tests d’hypothèses à partir de deux échantillons Séance 9 Examen Semaine du 20 janvier 2020 "],
["introduction.html", "Introduction", " Introduction La statistique est la science dont l’objet est de recueillir, de traiter et d’analyser des données issues de l’observation de phénomènes aléatoires, c’est-à-dire dans lesquels le hasard intervient. L’analyse des données est utilisée pour décrire les phénomènes étudiés, faire des prévisions et prendre des décisions à leur sujet. En cela, la statistique est un outil essentiel pour la compréhension et la gestion des phénomènes complexes. Les données étudiées peuvent être de toute nature, ce qui rend la statistique utile dans tous les champs disciplinaires et explique pourquoi elle est enseignée dans toutes les filières universitaires, de l’économie à la biologie en passant par la psychologie, et bien sûr les sciences de l’ingénieur. Le point fondamental est que les données sont entâchées d’incertitudes et présentent des variations pour plusieurs raisons : le déroulement des phénomènes observés n’est pas prévisible à l’avance avec certitude (par exemple on ne sait pas prévoir avec certitude les cours de la bourse ou les pannes des voitures) toute mesure est entâchée d’erreur etc… Il y a donc intervention du hasard et des probabilités. L’objectif essentiel de la statistique est de maîtriser au mieux cette incertitude pour extraire des informations utiles des données, par l’intermédiaire de l’analyse des variations dans les observations. Les méthodes statistiques se répartissent en deux classes : La statistique descriptive, statistique exploratoire ou analyse des données, a pour but de résumer l’information contenue dans les données de façon synthétique et efficace. Elle utilise pour cela des représentations de données sous forme de graphiques, de tableaux et d’indicateurs numériques (par exemple des moyennes). Elle permet de dégager les caractéristiques essentielles du phénomène étudié et de suggérer des hypothèses pour une étude ultérieure plus sophistiquée. Les probabilités n’ont ici qu’un rôle mineur. La statistique inférentielle va au delà de la simple description des données. Elle a pour but de faire des prévisions et de prendre des décisions au vu des observations. En général, il faut pour cela proposer des modèles probabilistes du phénomène aléatoire étudié et savoir gérer les risques d’erreurs. Les probabilités jouent ici un rôle fondamental. Pour le grand public, les statistiques désignent les résumés de données fournis par la statistique descriptive. Par exemple, on parle des “statistiques du chômage” ou des “statistiques de l’économie américaine”. Mais on oublie en général les aspects les plus importants liés aux prévisions et à l’aide à la décision apportés par la statistique inférentielle. L’informatique et la statistique sont deux éléments du traitement de l’information : l’informatique acquiert et traite l’information tandis que la statistique l’analyse. Les deux disciplines sont donc étroitement liées. En particulier, l’augmentation considérable de la puissance des ordinateurs et la facilité de transmission des données par internet ont rendu possible l’analyse de très grandes masses de données, ce qui nécessite l’utilisation de méthodes de plus en plus sophistiquées, connues sous le nom de data mining, fouille de données ou Big Data. Enfin, l’informatique décisionnelle ou business intelligence regroupe les outils d’aide à la décision devenus essentiels dans la gestion des entreprises. Ces outils nécessitent un recours important aux méthodes statistiques. Plus généralement, tout ingénieur est amené à prendre des décisions au vu de certaines informations, dans des contextes où de nombreuses incertitudes demeurent. Il importe donc qu’un ingénieur soit formé aux techniques de gestion du risque et de traitement de données expérimentales. Aujourd’hui, les statistiques sont partout. Les statistiques descriptives sont présentées dans tous les journaux et magazines. L’inférence statistique est devenue indispensable au santé public et la recherche médicale, à l’ingénierie et les études scientifiques, à la commercialisation et l’éducation, la comptabilité, l’économie, les prévisions météorologiques, les sondages, aux sports, à l’assurance, et à toutes les recherches scientifiques. Les statistiques sont en effet enracinées dans notre patrimoine intellectuel. La démarche statistique La statistique et les probabilités sont les deux aspects complémentaires de l’étude des phénomènes aléatoires. Ils sont cependant de natures bien différentes. Les probabilités peuvent être envisagées comme une branche des mathématiques pures, basée sur la théorie de la mesure, abstraite et complètement déconnectée de la réalité. Les probabilités appliquées proposent des modèles probabilistes du déroulement de phénomènes aléatoires concrets. On peut alors, préalablement à toute expérience, faire des prévisions sur ce qui va se produire. Par exemple, il est usuel de modéliser la durée de bon fonctionnement ou durée de vie d’un système, mettons une ampoule électrique, par une variable aléatoire \\(X\\) de loi exponentielle de paramètre \\(\\lambda\\). Ayant adopté ce modèle probabiliste, on peut effectuer tous les calculs que l’on veut. Par exemple : La probabilité que l’ampoule ne soit pas encore tombée en panne à la date \\(t\\) est \\(P(X &gt; t) = e^{−\\lambda t}\\) . La durée de vie moyenne est \\(E(X) = 1/\\lambda\\). Si \\(n\\) ampoules identiques sont mises en fonctionnement en même temps, et qu’elles fonctionnent indépendamment les unes des autres, le nombre \\(N_t\\) d’ampoules qui tomberont en panne avant un instant \\(t\\) est une variable aléatoire de loi binomiale \\(\\mathcal{B}(n,P(X ≤ t)) = \\mathcal{B}(n,1 − e^{-\\lambda t})\\). Donc on s’attend à ce que, en moyenne, \\(E(N_t) = n(1 − e^{-\\lambda t})\\) ampoules tombent en panne entre 0 et \\(t\\) Dans la pratique, l’utilisateur de ces ampoules est très intéressé par ces résultats. Il souhaite évidemment avoir une évaluation de leur durée de vie, de la probabilité qu’elles fonctionnent correctement pendant plus d’un mois, un an, etc… Mais si l’on veut utiliser les résultats théoriques énoncés plus haut, il faut d’une part pouvoir s’assurer qu’on a choisi un bon modèle, c’est-à-dire que la durée de vie de ces ampoules est bien une variable aléatoire de loi exponentielle, et, d’autre part, pouvoir calculer d’une manière ou d’une autre la valeur du paramètre \\(\\lambda\\). C’est la statistique qui va permettre de résoudre ces problèmes. Pour cela, il faut faire une expérimentation, recueillir des données et les analyser. On met donc en place ce qu’on appelle un essai ou une expérience. On fait fonctionner en parallèle et indépendamment les unes des autres \\(n = 10\\) ampoules identiques, dans les mêmes conditions expérimentales, et on relève leurs durées de vie. Admettons que l’on obtienne les durées de vie suivantes, exprimées en heures : 91.6 35.7 251 24.3 5.4 67.3 171 9.5 118 57.1 Notons \\(x_1 ,\\ldots,x_n\\) ces observations. Il est bien évident que la durée de vie des ampoules n’est pas prévisible avec certitude à l’avance. On va donc considérer que \\(x_1 ,\\ldots,x_n\\) sont les réalisations de variables aléatoires \\(X_1 ,\\ldots,X_n\\). Cela signifie qu’avant l’expérience, la durée de vie de la \\(i^{\\text{ème}}\\) ampoule est inconnue et que l’on traduit cette incertitude en modélisant cette durée par une variable aléatoire \\(X_i\\). Mais après l’expérience, la durée de vie a été observée. Il n’y a donc plus d’incertitude, cette durée est égale au réel \\(x_i\\). On dit que \\(x_i\\) est la réalisation de \\(X_i\\) sur l’essai effectué. Puisque les ampoules sont identiques, il est naturel de supposer que les \\(X_i\\) sont de même loi. Cela signifie qu’on observe plusieurs fois le même phénomène aléatoire. Mais le hasard fait que les réalisations de ces variables aléatoires de même loi sont différentes, d’où la variabilité dans les données. Puisque les ampoules ont fonctionné indépendamment les unes des autres, on pourra également supposer que les \\(X_i\\) sont des variables aléatoires indépendantes. On peut alors se poser les questions suivantes : Au vu de ces observations, est-il raisonnable de supposer que la durée de vie d’une ampoule est une variable aléatoire de loi exponentielle? Si non, quelle autre loi serait plus appropriée? C’est un problème de choix de modèle ou de test d’adéquation. Si le modèle de loi exponentielle a été retenu, comment proposer une valeur (ou un ensemble de valeurs) vraisemblable pour le paramètre \\(\\lambda\\)? C’est un problème d’estimation paramétrique. Dans ce cas, peut-on garantir que \\(\\lambda\\) est inférieur à une valeur fixée \\(\\lambda_0\\) ? Cela garantira alors que \\(E(X) = 1/\\lambda \\geq 1/\\lambda_0\\), autrement dit que les ampoules seront suffisamment fiables. C’est un problème de test d’hypothèses paramétriques. Sur un parc de 100 ampoules, à combien de pannes peut-on s’attendre en moins de 50 h? C’est un problème de prévision. Le premier problème central est celui de l’estimation : comment proposer, au vu des observations, une approximation des grandeurs inconnues du problème qui soit la plus proche possible de la réalité? La première question peut se traiter en estimant la fonction de répartition ou la densité de la loi de probabilité sous-jacente, la seconde revient à estimer un paramètre de cette loi, la troisième à estimer un nombre moyen de pannes sur une période donnée. Le second problème central est celui des tests d’hypothèses : il s’agit de se prononcer sur la validité d’une hypothèse liée au problème : la loi est-elle exponentielle? \\(\\lambda\\) est-il inférieur à \\(\\lambda_0\\)? un objectif de fiabilité est-il atteint? En répondant oui ou non à ces questions, il est possible que l’on se trompe. Donc, à toute réponse statistique, il faudra associer le degré de confiance que l’on peut accorder à cette réponse. C’est une caractéristique importante de la statistique par rapport aux mathématiques classiques, pour lesquelles un résultat est soit juste, soit faux. Pour résumer, la démarche probabiliste suppose que la nature du hasard est connue. Cela signifie que l’on adopte un modèle probabiliste particulier (ici la loi exponentielle), qui permettra d’effectuer des prévisions sur les observations futures. Dans la pratique, la nature du hasard est inconnue. La statistique va, au vu des observations, formuler des hypothèses sur la nature du phénomène aléatoire étudié. Maîtriser au mieux cette incertitude permettra de traiter les données disponibles. Probabilités et statistiques agissent donc en aller-retour dans le traitement mathématique des phénomènes aléatoires. L’exemple des ampoules est une illustration du cas le plus fréquent où les données se présentent sous la forme d’une suite de nombres. C’est ce cas que nous traiterons dans ce cours, mais il faut savoir que les données peuvent être beaucoup plus complexes : des fonctions, des images, etc… Les principes et méthodes généraux que nous traiterons dans ce cours seront adaptables à tous les types de données. Objectifs et plan du cours Ce cours a pour but de présenter les principes de base d’une analyse statistique de données (description, estimation, tests), ainsi que les méthodes statistiques les plus usuelles. Ces méthodes seront toujours illustrées par des problèmes concrets. Le cours privilégie l’application à la théorie. Les méthodes présentées seront mises en œuvre à l’aide du logiciel (https://www.r-project.org). Le premier chapitre présente les techniques de base en statistique descriptive, représentations graphiques et indicateurs statistiques. Le chapitre suivant introduit l’échantillonnage et les théorèmes limites avec une étude des statistiques \\(\\overline{X}_n\\) et \\(S^2\\). Le chapitre 3 est consacré aux problèmes d’estima tion paramétrique ponctuelle, le chapitre 4 aux intervalles de confiance et le chapitre 5 aux tests d’hypothèses. Enfin, des annexes donnent quelques rappels de probabilités utiles en statistique et des tables des lois de probabilité usuelles. "],
["statistique-descriptive.html", "Chapitre 1 Statistique descriptive 1.1 Terminologie 1.2 Statistique et Probabilités 1.3 Description d’une série de valeurs 1.4 Représentations graphiques 1.5 Indicateurs statistiques 1.6 Statistique descriptive bidimensionnelle", " Chapitre 1 Statistique descriptive La statistique descriptive a pour but de résumer l’information contenue dans les données de façon à en dégager les caratéristiques essentielles sous une forme simple et intelligible. Les deux principaux outils de la statistique descriptive sont les représentations graphiques et les indicateurs statistiques. Il ne faut pas confondre la statistique qui est la science qui vient d’être définie et une statistique qui est un ensemble de données chiffrées sur un sujet précis. 1.1 Terminologie Les premières statistiques correctement élaborées ont été celles des recensements démographiques. Ainsi le vocabulaire statistique est essentiellement celui de la démographie. Les ensembles étudiés sont appelés population. Les éléments de la population sont appelés individus ou unités statistiques. La population est étudiée selon une ou plusieurs variables (ou caractères). L’ensemble des individus constitue l’échantillon étudié. Exemple: si l’échantillon est un groupe de TD à l’ESILV, Un individu est un étudiant. La population peut être l’ensemble des étudiants de l’ESILV, des élèves ingénieur de France, etc.. Les variables étudiées peuvent être la taille, la filière choisie, la moyenne d’année, la couleur des yeux, etc.. Si l’échantillon est constitué de tous les individus de la population, on dit que l’on fait un recensement. Il est extrêmement rare que l’on se trouve dans cette situation. Quand l’échantillon n’est qu’une partie de la population, on parle de sondage. Le principe des sondages est d’étendre à l’ensemble de la population les enseignements tirés de l’étude de l’échantillon. Pour que les résultats observés lors d’une étude soient généralisables à la population statistique, l’échantillon doit être représentatif de cette dernière, c’est à dire qu’il doit refléter fidèlement sa composition et sa complexité. Seul l’échantillonnage aléatoire assure la représentativité de l’échantillon. Il existe des méthodes pour y parvenir, dont nous ne parlerons pas ici. Un échantillon est qualifié d’aléatoire lorsque chaque individu de la population a une probabilité connue et non nulle d’appartenir à l’échantillon. Le cas particulier le plus connu est celui qui affecte à chaque individu la même probabilité d’appartenir à l’échantillon. Définition 1.1 (Echantillonnage aléatoire simple) L’échantillonnage aléatoire simple est une méthode qui consiste à prélever au hasard et de façon indépendante, \\(n\\) individus ou unités d’échantillonnage d’une population à \\(N\\) individus. Chaque individu possède ainsi la même probabilité de faire partie d’un échantillon de \\(n\\) individus et chacun des échantillons possibles de taille \\(n\\) possède la même probabilité d’être constitué. 1.2 Statistique et Probabilités Les concepts qui viennent d’être présentés sont les homologues de concepts du calcul des probabilités et il est possible de disposer en regard les concepts homologues (voir table ci-dessous). Probabilités Statistique Espace fondamental Population Epreuve Tirage (d’un individu), expérimentation Evènement élémentaire Individu, observation Variable aléatoire Variable (Caractère) Epreuves répétées Echantillonnage Nbre de répétitions d’une épreuve Taille de l’échantillon, effectif total Probabilité Fréquence observée Loi de probabilité Distribution observée ou loi empirique Espérance mathématique Moyenne observée Variance Variance observée Le mot “variable” désigne à la fois la grandeur que l’on veut étudier (variable statistique) et l’objet mathématique qui la représente (variable aléatoire). Ainsi la notion de variable se confond avec celle de variable aléatoire. Une variable statistique peut être discrète ou continue, qualitative ou quantitative. Les méthodes de représentation des données diffèrent suivant la nature des variables étudiées. 1.3 Description d’une série de valeurs On considère ici une série (un ensemble) de valeurs, numériques, ou non, homogènes en ce sens qu’elles se réfèrent à une même variable et qu’elles ne sont pas structurées en sous-ensembles. Chaque valeur est associée à un individu statistique (unité statistique, observation). C’est le cas, par exemple, des notes obtenus par une promotion d’élèves à un examen. Dans cet exemple, lorsqu’il y a plusieurs examens, on peut vouloir considérer ensemble les notes d’un même élève. Les notes sont alors structurées en sous-ensembles et les méthodes à utiliser diffèrent de celles présentées ici. Pour décrire une telle série, l’examen direct des valeurs n’est pas commode dès lors que ces valeurs sont un tant soit peu nombreuses. Pour cela, la statistique propose deux types d’outils : des graphiques et des indicateurs statistiques. 1.4 Représentations graphiques Face à un problème particulier, on peut chercher à construire un graphique ad hoc. Mais, la plupart du temps, on utilise des graphiques standard dont la nature diffère selon le type de la variable étudiée. 1.4.1 Variable qualitative Les valeurs que peut prendre une variable qualitative \\(X\\) (ex: couleur) constituent un ensemble de \\(M\\) modalités: {exemple: 1= bleu; 2=blanc; … ; M=rouge}; une telle série présente une apparence numérique mais on ne peut faire de calcul sur ces nombres (dans l’exemple, blanc n’est pas égale à deux fois bleu). Autres exemples: catégorie socio-professionnelle, genre, région d’appartenance, etc. Données On a “mesuré” une variable qualitative sur \\(n\\) individus. Les données brutes sont constituées par la série des \\(n\\) valeurs \\(\\{x_i; i=1,\\ldots,n\\}\\) avec \\(x_i\\) le numéro de la modalité pour l’individu \\(i\\), \\(x_i \\in 1,\\ldots,M\\). Il est commode d’agréger ces données en comptant le nombre d’individus \\(n_m\\) possédant la modalité \\(m\\). \\(n_m\\) est un effectif, ou une fréquence absolue (par opposition à la fréquence relative \\(n_m/n\\)) Table 1.1: Variable qualitative: données brutes (gauche) et agrégées (droite). \\(n_m\\) nombre d’individus possédant la modalité \\(m\\) n°ind. \\(X\\) \\(1\\) \\(x_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(x_i\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(x_n\\) modalité effectif \\(1\\) \\(n_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(m\\) \\(n_m\\) \\(\\vdots\\) \\(\\vdots\\) \\(M\\) \\(n_M\\) Diagramme en bâtons (barplot) Il représente les données agrégées (groupées). Les modalités figurent en abscisse; la longueur d’un bâton le long de l’ordonnée est proportionnelle à l’effectif (ou à la fréquence). Il est utile de trier les modalités, généralement par fréquence décroissante. Figure 1.1: Diagramme en bâtons L’ordre doit être respecté pour une variable ordinale. Quand il s’agit d’une variable nominale, il est préférable d’ordonner les modalités par effectifs croissants ou décroissant pour rendre le graphique plus lisible. Attention à ne pas confondre cette représentation avec un histogramme (cas de variable continue). Diagrammes circulaires (pie chart) Le fameux “camembert” n’est commode que s’il y a peu de modalités. Ici encore, il est commode de trier les modalités par effectifs décroissants. Figure 1.2: Diagramme circulaire de la variable “couleur des yeux” 1.4.2 Variable quantitative Cas de variable quantitative discrète La distribution de \\(X\\) est fournie par le tableau des fréquences qui fait correspondre aux différentes valeurs (modalités) de la variable. Soit l’exemple suivant de nombre de personnes par ménage pour les 10 ménages suivants: 2 3 3 3 4 5 5 5 5 6 On peut représenter la série avec un diagramme en bâtons (vertical ou horizontal). Figure 1.3: Le nombre de personnes par ménage Cas de variable quantitative continue Pour la visualisation d’une série numérique, l’outil de base est l’histogramme, outil qui ressemble à un diagramme en bâtons mais qui doit en être distingué. Il repose sur une subdivision de la plage de variation de \\(X\\) en intervalles; pour chaque intervalle \\([a,b[\\) on compte le nombre d’individus tels que \\(X \\in [a,b[\\), nombre dit effectif de l’intervalle. Dans l’histogramme, les intervalles sont représentés sur l’axe des abscisses. Au-dessus de chaque intervalle, on dessine un rectangle dont la surface est proportionnelle à son effectif. La base étant la longueur de l’intervalle, la hauteur de chaque rectangle s’interprète comme une densité (effectif par unité de longueur). Table 1.2: Variable quantitative: données brutes (gauche) et agrégées (droite). n°ind. \\(X\\) \\(1\\) \\(x_1\\) \\(\\vdots\\) \\(\\vdots\\) \\(i\\) \\(x_i\\) \\(\\vdots\\) \\(\\vdots\\) \\(n\\) \\(x_n\\) intervalle effectif \\(1\\) \\(n_1\\) \\(\\vdots\\) \\(\\vdots\\) \\([a,b[\\) \\(n_{[a,b[}\\) \\(\\vdots\\) \\(\\vdots\\) \\(M\\) \\(n_M\\) Le nombre de classes doit être choisi de façon à ce que l’effectif moyen par intervalle soit suffisamment grand afin de pas être gêné par des irrégularités locales et, au contraire, afin de mettre en évidence une tendance générale. Soit l’exemple suivant: on s’intéresse à la taille de 237 étudiants1. L’idée de représenter en ordonnée la densité (et non l’effectif) permet de prendre en compte des intervalles de longueurs inégales, ce qui peut être nécessaire pour les intervalles extrêmes. Celà permet aussi de présenter la distribution de l’échantillon. Dans la figure précédente, nous avons utilisé des effectifs (fréquences absolues), on préfère généralement utiliser des fréquences relativespour pouvoir superposer facilement des distributions de référence. Une autre graphique à base des quartiles est la boîte à dispersion ou boîte à moustaches (boxplot). Un rectangle, délimité par les premier et troisième quartiles, représente \\(50\\%\\) de la population. Dans ce rectangle, une barre centrale représente la médiane. De part et d’autre du rectangle, on figure deux segments dont la longueur est environ 1.5 fois l’écart interquartile: “environ” car chaque segment est en fait délimité par une observation réelle incluse dans cet intervalle, celle qui est la plus éloignée de la médiane. Enfin, les observations au-delà de ces limites sont représentées individuellement. Figure 1.4: Boîte à moustaches de Rythme cardiaque de 237 étudiants 1.5 Indicateurs statistiques Les représentations graphiques présentées dans la section précédente ne permettent qu’une analyse visuelle de la répartition des données. Pour des variables quantitatives, il est intéressant de donner des indicateurs numériques permettant de caractériser au mieux ces données. On donne en général deux indicateurs : un indicateur de tendance centrale (ou de localisation) et un indicateur de dispersion: L’ordre de grandeur des observations situées au centre de la distribution: c’est la tendance centrale. La “largeur” de la série, c’est-à-dire la plus ou moins grande fluctuation des observations autour de la tendance centrale : c’est la dispersion. 1.5.1 Tendance centrale Les mesures de tendance centrale permettent d’obtenir une idée juste de l’ordre de grandeur des valeurs ainsi que de la valeur centrale de la caractéristique que l’on désire étudier. Les trois principaux indicateurs de tendance centrale sont: Le Mode. La Moyenne. La Médiane. Le Mode empirique Le mode d’une distribution statistique, noté \\(M_o\\), est la modalité de variable la plus représentée dans la distribution. Également appelé “valeur dominante” de la distribution. Il correspond au sommet de la distribution: le mode est la valeur la plus fréquente La Moyenne empirique La moyenne empirique de l’échantillon est la moyenne arithmétique des observations. Cet indicateur, noté \\(\\overline{x}\\), est le plus utilisé. Le calcul de la moyenne dépends de la représentation des données: Si les données statistiques sont exploitées en série (données individuelles): \\[ \\overline{x}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\] Exemple: Les notes d’un étudiant dans 7 matières sont: 18 16 15 14 12 17 11 La note moyenne est donc \\[ \\overline{x}=\\frac{18+16+15+14+12+17+11}{7}=14.71 \\] Si les données statistiques sont groupées par modalités, \\(n_i\\) étant l’effectif correspondant à la modalité \\(x_i\\) : \\[ \\overline{x}=\\frac{1}{n} \\sum_{i=1}^{p} n_{i} x_{i} \\] Exemple: Soit les notes obtenues par un élève au baccalauréat: \\(x_i\\) \\(n_i\\) 4 2 8 3 16 2 13 3 5 2 Total 12 La moyenne empirique pondérée de cet élève au baccalauréat est: \\[ \\overline{x}=\\frac{(4 \\times 2)+(8 \\times 3)+(16 \\times 2)+(13 \\times 3)+(5 \\times 2)}{12}=\\frac{113}{12}=9.42 \\] La Médiane empirique On appelle “médiane” d’une distribution statistique, notée \\(M_e\\) , la valeur de la variable qui partage en deux groupes d’effectif identique les observations classées par ordre croissant. Il y a 50% des observations qui sont inférieures ou égales à la médiane et 50% des observations qui sont supérieures ou égales à la médiane. L’avantage principal de la médiane, par rapport à la moyenne arithmétique, est qu’elle n’est pas indûment influencée par quelques données extrêmes. Si l’on considère les \\(x_i\\) rangés par ordre croissant la médiane est définie par: \\[\\begin{align} M_e = x_{(n+1)/2} \\quad &amp;\\text{si}\\quad n \\quad\\text{impair}\\\\ M_e = \\frac{x_{n/2}+x_{(n/2)+1}}{2} \\quad &amp;\\text{si}\\quad n \\quad\\text{pair} \\end{align}\\] 1.5.2 Dispersion (variabilité) Après celle relative à la tendance centrale, la deuxième question que l’on se pose à propos d’une série numérique est celle de sa dispersion (souvent sous-entendu “autour de la tendance centrale”). Exemples: Un patient apprend de son médecin que sa pression intra-oculaire est de 19. La pression moyenne pour ceux de son âge et de son sexe est de 17. Que peut-il conclure? Ce n’est pas nécessairement inquiétant: les données d’une population sont presque toutes distinctes de la moyenne. Mais s’écarte-t-il trop de la moyenne? De combien les autres membres de la population s’écartent de la moyenne? La température moyenne à Montréal est de 6.9°C. Ceci n’empêche pas la température de baisser à -35°C en hiver et de monter à +35°C en été. On va définir: L’étendue. La variance et l’écart-type. Etendue (amplitude) empirique C’est la différence entre les valeurs maximum et minimum, soit, les valeurs ayant été classées par ordre croissant : \\(x_n-x_1\\). Historiquement, c’est le critère le plus ancien (employé par les Grecs pour leurs mesure astronomiques). Il est utilisé lorsque l’on ne s’intéresse pas en détail à la dispersion : par exemple, on dira que le temps de trajet en TGV entre Rennes et Paris est compris entre 2h05 (train direct) et 2h25 (quelques arrêts). Variance et Ecart-type empiriques La variance et sa racine l’écart-type sont les indicateurs de dispersion utilisés de manière standard. La variance (noté usuellement \\(s^2\\) pour une série de valeurs observées, \\(\\sigma^2\\) pour une distribution théorique et \\(V(X)\\) pour une variable aléatoire), est la moyenne des carrés des écarts à la moyenne. Soit, pour une série numérique: \\[ s^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2} \\] Par rapport à la variance, l’intérêt de l’écart-type est qu’il s’exprime dans les mêmes unités que les valeurs étudiées. Cependant, la variabilité doit toujours se comparer à la valeur moyenne. Des données présentent une forte variabilité si l’écart-type est grand par rapport à la moyenne. Aussi on définit le coefficient de variation empirique de l’échantillon par \\[ cv_{n}=\\frac{s_{n}}{\\overline{x}_{n}} \\] L’intérêt de cet indicateur est qu’il est sans dimension. Une pratique empirique courante est de considérer que l’échantillon possède une variabilité significative si \\(cv_{n} &gt; 0.15\\). Si \\(cv_{n} \\leq 0.15\\), les données présentent peu de variabilité et on considère que la moyenne empirique à elle seule est un bon résumé de tout l’échantillon. En , la commande var(x) donne \\(s*^2 = \\frac{n}{n-1}s^2\\) au lieu de \\(s^2\\). C’est aussi ce que l’on a sur les calculatrices dotées de fonctionnalités statistiques. On en verra l’explication au chapitre Estimation. De même \\(s*=\\sqrt{s*^2}\\) est donné en par sd(x) (standard deviation). \\(s_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}_{n}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}^{2}-\\overline{x}_{n}^{2}\\) évoque \\(\\operatorname{Var}(X)=E\\left[(X-E(X))^{2}\\right]=E\\left(X^{2}\\right)-[E(X)]^{2}\\). En finance, la variabilité d’une série de données est appelée volatilité. L’étude de la volatilité est fondamentale dans les analyses de risque financier. 1.6 Statistique descriptive bidimensionnelle Dans cette section, on sintérese a l’étude simultanée de deux variables \\(X\\) et \\(Y\\), étudiées sur le même échantillon, toujours noté \\(\\Omega\\). L’objectif essentiel des méthodes présentées est de mettre en évidence une éventuelle variation simultanée des deux variables, que nous appellerons alors liaison. Dans certains cas, cette liaison peut être considérée a priori comme causale, une variable \\(X\\) expliquant l’autre \\(Y\\); dans d’autres, ce n’est pas le cas, et les deux variables jouent des rôles symétriques. Dans la pratique, il conviendra de bien différencier les deux situations et une liaison n’entraîne pas nécessairement une causalité. Les données sont présentées de la façon suivante: on dispose de deux séries \\(X\\) et \\(Y\\) représentant l’observation des variables \\(X\\) et \\(Y\\) sur les mêmes \\(n\\) individus: on a une série bidimensionnelle \\((X,Y)\\) de taille \\(n\\): individu \\(X\\) \\(Y\\) \\(1\\) \\(x_1\\) \\(y_1\\) \\(2\\) \\(x_2\\) \\(y_2\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\ldots\\) \\(i\\) \\(x_i\\) \\(y_i\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\ldots\\) \\(n\\) \\(x_n\\) \\(y_n\\) où \\(x_i\\) est la valeur de \\(X\\) et \\(y_i\\) celle de \\(Y\\) pour l’individu n° \\(i\\) de la série. Deux variables quantitatives Pour étudier la liaison entre deux variables quantitatives (discrètes), on commence par faire un graphique du type nuage de points (scatterplot). La forme générale de ce graphique indique s’ll existe ou non une liaison entre les deux variables. Il s’agit d’un graphique très commode pour représenter les observations simultanées de deux variables quantitatives. Figure 1.5: Exemple de nuage de points représentant la consommation de carburant en fonction du poids de voitures. L’exemple extrait du dataset mtcars dans R Le choix des échelles a retenir pour réaliser un nuage de points peut s’avérer délicat. D’une façon générale, on distinguera le cas de variables homogènes (représentant la même grandeur et exprimées dans la même unité) de celui des variables hétérogenes. Dans le premier cas, on choisira la même échelle sur les deux axes (qui seront donc orthonormés); dans le second cas, il est recommandé soit de représenter les variables centrées et réduites sur des axes orthonormés, soit de choisir des échelles telles que ce soit sensiblement ces variables là que l’on représente (c’est en genéral cette seconde solution qu’utilisent, de façon automatique, les logiciels statistiques). Rappel: variables centrées et réduites Si \\(X\\) est une variable quantitative de moyenne \\(\\overline{x}\\) et d’écart-type \\(\\sigma_{X},\\) on apelle variable centrée associée à \\(X\\) la variable \\(X-\\overline{x}\\) (elle est de moyenne nulle et d’écart-type \\(\\sigma_{X}\\)), et variable centrée et réduite (ou tout simplement variable réduite) associe à \\(X\\) la variable \\(\\frac{X-\\overline{x}}{\\sigma}\\) (elle est de moyene nulle et d’écart-type égal à un). Une variable centrée er réduite s’exprime sans unité. Coefficient de corrélation linéaire On appelle coefficient de corrélation linéaire de \\(X\\) et de \\(Y\\) la valeur définie par \\[\\rho = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] Où \\(Cov(X,Y)\\) est la covariance de \\(X\\) et de \\(Y\\) la valeur si elle existe: \\[Cov(X,Y) = \\frac{1}{N} \\sum_{i=1}^N \\left( X_i - \\bar{X} \\right) \\left( Y_i - \\bar{Y} \\right)\\] On peut montrer que \\(-1 \\leq \\rho(X,Y) \\leq 1\\). Interprétation de \\(\\rho\\) Le coefficient de corrélation est une mesure du degré de linéarité entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proches de \\(1\\) ou \\(-1\\) indiquent une linéarité quasiment rigoureuse entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proche de 0 indiquent une absence de toute relation linéaire. Lorsque \\(\\rho(X,Y)\\) est positif, \\(Y\\) a tendance à augmenter si \\(X\\) en fait autant. Lorsque \\(\\rho(X,Y) &lt; 0\\), \\(Y\\) a tendance à diminuer si \\(X\\) augmente. Si \\(\\rho(X,Y) =0\\), on dit que ces deux statistiques sont non corrélées. Figure 1.6: Illustration de l’effet de la variation of the effect of varying the strength and direction of a correlation La corrélation mesure l’association, pas la causalité. Dans certains cas, une liaison peut être considérée a priori comme causale, une variable expliquant l’autre. Dans d’autres, ce n’est pas le cas et les deux variables jouent alors des rôles symétriques. Sur ce lien on trouve des exemples de variables associées linéairement mais non liées de manière causale. D’autre part, une corrélation nulle ne signifie pas que deux variables sont indépendantes. Voici quelques exemples: Figure 1.7: Exemples où la corrélation est presque nulle mais les variables ne sont pas indépendantes. Une variable quantitative et une qualitative On dispose d’une variable qualitative \\(X\\) à \\(p\\) modalités \\(m_{1}, \\ldots, m_{p}\\) et une variable quantitative \\(Y\\). On a alors \\(p\\) sous-populations déterminées par les \\(p\\) modalités de \\(X\\). L’étude de la liaison entre \\(X\\) et \\(Y\\) consiste en l’étude des différences entre ces sous-populations: il y aura absence de lien si on ne distingue pas de différence notoire dans les caractéristiques de ces différentes sous-populations. Une façon commode de représenter les données dans le cas de l’étude simultanée d’une variable quantitative et d’une variable qualitative consiste à réaliser des boîtes à moustaches parallèles. Les boîtes à moustaches permettent de comparer facilement des groupes d’individus, par exemple ici les garçons et les filles parmi 237 étudiants2: Figure 1.8: Boîtes à moustaches par sexe Deux variables qualitatives On considère dans ce paragraphe deux variables qualitatives observées simultanément sur \\(n\\) individus. On suppose que la première, notée \\(X,\\) possède \\(r\\) modalités notées \\(x_{1}, \\ldots, x_{\\ell}, \\ldots, x_{r},\\) et que la seconde, notée \\(Y,\\) possède \\(c\\) modalités notées \\(y_{1}, \\ldots, y_{h}, \\ldots, y_{c}\\). Ces données sont présentées dans un tableau à double entrée, appelé tables de contingence, dans lequel on dispose les modalités de \\(X\\) en lignes et celles de \\(Y\\) en colonnes. Ce tableau est donc de dimension \\(r \\times c\\) et a pour élément générique le nombre \\(n_{\\ell h}\\) d’observations conjointes des modalités \\(x_{\\ell}\\) de \\(X\\) et \\(y_{h}\\) de \\(Y ;\\) les quantités \\(n_{\\ell h}\\) sont appelées les effectifs conjoints. Une table de contingence se présente donc sous la forme suivante: \\(y_1\\) \\(\\ldots\\) \\(y_h\\) \\(\\ldots\\) \\(y_c\\) sommes \\(x_1\\) \\(n_{11}\\) \\(n_{1h}\\) \\(n_{1c}\\) \\(n_{1.}\\) \\(\\vdots\\) \\(x_{\\ell}\\) \\(n_{\\ell 1}\\) \\(n_{\\ell h}\\) \\(n_{\\ell c}\\) \\(n_{\\ell .}\\) \\(\\vdots\\) \\(x_r\\) \\(n_{r 1}\\) \\(n_{r h}\\) \\(n_{rc}\\) \\(n_{r.}\\) sommes \\(n_{.1}\\) \\(n_{.h}\\) \\(n_{.c}\\) \\(n\\) Les quantités \\(n_{\\ell .}(\\ell=1, \\ldots, r)\\) et \\(n_{.h}(h=1, \\ldots, c)\\) sont appelées les effectifs marginaux; ils sont définis par \\(n_{\\ell .}=\\sum_{h=1}^{c} n_{\\ell h}\\) et \\(n_{. h}=\\sum_{\\ell=1}^{r} n_{\\ell h}\\) et ils vérifient \\(\\sum_{\\ell=1}^{r} n_{\\ell.}=\\sum_{h=1}^{c} n_{.h}=n\\). De façon analogue, on peut définir les notions de fréquences conjointes et de fréquences marginales. On peut représenter graphiquement deux variables qualitatives avec des diagrammes en barre parallèles (mosaïcplots). Indices de liaison Lorsque tous les profis-lignes sont égaux, ce qui est équivalent à ce que tous les profil-colonnes soient égaux et que \\[ \\forall(\\ell, h) \\in\\{1, \\ldots, r\\} \\times\\{1, \\ldots, c\\} : n_{\\ell h}=\\frac{n_{\\ell .} n_{.h}}{n} \\] on dit qu’il n’exist aucune forme de liaison entre les deux variables considérées \\(X\\) et \\(Y .\\) Par suite, la mesure de la laison va se faire en évaluant l’écart entre la situation observée et l’état de non liaison défini ci-dessus. Khi-deux Il est courant en statistique de comparer une table de contingence observée, d’effectif conjoint générique \\(n_{\\ell h},\\) à une table de contingence donnée a priori (et appelée standard), d’effectif conjoint générique \\(s_{\\ell h},\\) en calculant la quantié \\[ \\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-s_{\\ell h}\\right)^{2}}{s_{\\ell h}} \\] De façon naturelle, pour mesurer la liaison sur une table de contingence, on utilise donc l’indice appelé khi-deux (chi-square) et défini comme suit: \\[ \\chi^{2}=\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{\\left(n_{\\ell h}-\\frac{n_{\\ell+} n_{+h}}{n}\\right)^{2}}{\\frac{n_{\\ell+} n_{+h}}{n}}=n\\left[\\sum_{\\ell=1}^{r} \\sum_{h=1}^{c} \\frac{n_{\\ell h}^{2}}{n_{\\ell+} n_{+h}}-1\\right] \\] Le coefficient \\(\\chi^{2}\\) est toujours positif ou nul et il est d’autant plus grand que la liaison entre les deux variables considérés est forte. Un document complet et intéressant sur la visualization des données est sur ce lien Disponibles dans le jeu de données survey de la librairie MASS de ↩ On pourra dire en regardant ces deux figures qu’il n’y a pas de différence de rythme cardiaque entre les garçons et les filles, mais il y probablement une différence de taille. Pour confirmer cette comparaison on effectuera un test d’hypothèse de comparaison de moyennes entre deux échantillons.↩ "],
["exercices.html", "Exercices", " Exercices Exercice 1.1 (Nature des variables et graphique correspondant) Lors d’une enquête, on interroge 1000 individus sur leur âge, leur sexe, leur couleur préférée, leur nombre de frères et soeurs et leur département de naissance. Quelle est la nature de chacune de ces variables? Par quel outil graphique visualiseriez-vous la distribution de chacune des variables élémentaires? Exercice 1.2 (Petite série d’effectifs) On réalise un sondage auprès de 1000 personnes pour évaluer le nombre d’individus par ménage. On obtient la série statistique suivante: Nb de personnes par ménage 1 2 3 4 5 6 Total effectif 107 137 197 302 180 77 1000 Représenter cette série statistique à l’aide d’un graphique. Calculer la moyenne, la variance, l’écart-type et la médiane de cette distribution. Exercice 1.3 (Robustesse de la médiane aux données extrêmes) On a repertorié le temps mis par 20 étudiants pour décrocher un premier emploi (en mois): 0 0 0 0 0 1 1 1 2 2 2 3 3 3 4 4 6 6 8 20 Calculer la moyenne puis la médiane des observations. L’étudiant qui a mis 20 mois à trouver du travail a réalisé un tour du monde avant de chercher un emploi. On considère donc qu’il ne fait pas partie de la population qui nous intéresse et on décide de supprimer cette valeur de l’échantillon. Même question que la question 1 sans la dernière valeur (20). Comparer la stabilité des deux indicateurs de position vis-à-vis de l’élimination de valeurs extrêmes. Exercice 1.4 (Corrélation) Les données suivantes représentent la taille (en cm) et le salaire annuel brut (en k€) de 12 avocats ayant fait la même formation et obtenu presque les même notes. Taille Salaire 162.6 61 165.1 64 167.6 58 170.2 73 175.3 47 177.8 66 Taille Salaire 182.9 75 182.9 58 188.0 92 188.0 72 190.5 60 193.0 84 Représenter les données sur un nuage de points. Calculer le coefficient de corrélation et interpréter. "],
["tp-statistique-descriptive-avec-r.html", "TP Statistique descriptive avec R Qu’est-ce que c’est que ? 1ère partie: Données quantitatives discrètes 2ème partie : Analyse descriptive", " TP Statistique descriptive avec R Qu’est-ce que c’est que ? C’est un langage de programmation et un logiciel gratuit et libre. Il est surtout utilisé pour le développement de programmes statistiques et des analyses de données. Il gagne en popularité depuis quelques années avec l’émergence de la data science et du fait qu’il est gratuit et ouvert (open-source). est née d’un projet de recherche mené par deux chercheurs, Ross Ihaka et Robert Gentleman à l’université d’Auckland (Nouvelle-Zélande) en 1993. En 1997 est mis en place le Comprehension R Archive Network (CRAN) qui centralise les contributions au projet. Depuis le projet connaît une croissance soutenue, grâce à des contributions de la part de milliers de personnes à travers le monde. RStudio C’est une IDE (Integrated Development Environment) ou Environnement Intégré de Développement. Il sert d’interface entre et l’utilisateur, offre à celui diverses commodités d’utilisation Une introduction au RStudio est présentée dans l’annexe A. Vous trouverez une bonne introduction à sur ce lien . 1ère partie: Données quantitatives discrètes Le nombre d’arbres plantés sur les parcelles d’un lotissement a été compté. Les données obtenues sont les suivantes: \\[1,2,4,1,6,3,2,1,2,0,1,2,2,1,3,0,3,2,1,2,2,3,2,3.\\] 1. Quelle est la nature de variable étudiée? 2. Rentrer ces données sous la forme d’un vecteur nommé arbres et affichez ce vecteur. 3. Trier les valeurs de ce vecteur par ordre croissant. 4. Donner la taille de l’échantillon (c’est-à-dire le nombre de composantes de ce vecteur) en la notant n et affichez sa valeur. Effectifs et fréquence 5. Montrer la séquence des modalités et la séquence des effectifs correspondants. 6. Montrer le tableau de fréquences et de pourcentages. 7. Calculer et afficher les effectifs cumulés et les fréquences cumulées. Mesures de tendance centrale 8. Calculer le nombre moyen d’arbres par parcelle. 9. Calculer le nombre maximum et le nombre minimum d’arbres sur une parcelle. 10. Calculer le nombre médian d’arbres par parcelle. 11. Utiliser la fonction summary() pour obtenir un tableau récapitulatif des indicateurs. Indicateurs de dispersion 12. Calculer la variance du nombre d’arbres plantés sur les parcelles. 13. Calculer maintenant l’écart-type et vérifier que l’écart-type est la racine carrée de la variance. 14. Calculer la variance vous-même. La variance obtenue est elle la même que la précédente? Le logiciel utilise \\(n-1\\) pour le dénomiateur dans la définition de la variance, c’est-à-dire \\(\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) (d’écart-type noté \\(\\sigma_{n-1}\\) ou \\(s\\). Cette quantité est souvent préférée dans les applications numériques pour des questions d’estimation). Représentations graphiques 15. La fontion plot() affiche par défaut un nuage de points avec en abscisse le numéro de l’observation (ici de 1 à 24) et en ordonnée le nombre d’arbres. Tester cette fonction. Modifier le titre de la figure, les noms des axes, la couleur et la forme des points affichés. 16. Afficher la courbe des fréquence cumulées. (Indication: Utiliser la fonction ecdf()). 17. Tracer un diagramme en bâtons par la fonction barplot() à partir du tableau des effectifs ou des fréquences. 2ème partie : Analyse descriptive Données utilisées Une enquête a été réalisée sur 237 étudiants. Les données sont les suivantes: Sex: The sex of the student. (Factor with levels “Male” and “Female”.) Wr.Hnd: span (distance from tip of thumb to tip of little finger of spread hand) of writing hand, in centimetres. NW.Hnd: span of non-writing hand. W.Hnd: writing hand of student. (Factor, with levels “Left” and “Right”.) Fold: “Fold your arms! Which is on top” (Factor, with levels “R on L”, “L on R”, “Neither”.) Pulse: pulse rate of student (beats per minute). Clap: ‘Clap your hands! Which hand is on top?’ (Factor, with levels “Right”, “Left”, “Neither”.) Exer: how often the student exercises. (Factor, with levels “Freq” (frequently), “Some”, “None”.) Smoke: how much the student smokes. (Factor, levels “Heavy”, “Regul” (regularly), “Occas” (occasionally), “Never”.) Height: height of the student in centimetres. M.I: whether the student expressed height in imperial (feet/inches) or metric (centimetres/metres) units. (Factor, levels “Metric”, “Imperial”.) Age: age of the student in years. Définition du répertoire de travail Vous avez la possibilité de définir un Répertoire de travail dans lequel vous allez stocker votre script R, vos données etc… Ceci est réalisé par la fonction setwd(\"..Chemin/de/votre/repertoire\"). Cette fonction considère comme seul paramètre le chemin d’accès au répertoire que vous avez choisi. A tout moment, vous pouvez vérifier le répertoire de travail courant en executant l’instruction suivante: getwd() 1. Définisser votre répertoire de travail. Chargement des données Il existe une multitude de fonctions qui permettent de charger un fichier de données. Télécharger le fichier de données en cliquant ici et enregistrer le dans votre répertoire. Ensuite utiliser la fonction read.csv() pour charger les données dans . Cette fonction prend comme principaux paramètres d’entrée le fichier à charger (file=\"data.txt\"), le séparateur de colonnes dans le fichier initial (sep=) et la présence (ou non) des noms de colonnes dans le fichier (header=). Ouvrez toujours le fichier de données dans un éditeur de texte pour connaitre le séparateur de colonnes et voir si les noms de colonnes sont présents. 2. Charger le fichier des données dans l’enquête dans un tableau nommé data. L’instruction de chargement du fichier est la suivante: donnees = read.csv(&quot;enquete.csv&quot;, header = T , sep=&quot;,&quot;) Le fichier de données est chargé dans l’environnement et est affecté à l’objet donnees. C’est cet objet, de type dataframe qui va faire l’objet de manipulations par la suite. 3. Afficher le nombre de d’observations (lignes) et le nombre de variables (colonnes). 4. Utiliser la fonction head() pour afficher les premières lignes (6 par défaut) de données chargées. 5. L’accès à une colonne d’un dataframe se fait par la notation $: nom_du_dataframe$nom_variable. Afficher les valeurs de la variable Age de vos données. Analyse descriptive univariée Indicateurs statistiques pour variables quantitatives 6. Calculer et afficher la moyenne et l’écart-type d’age des élèves qui ont participé à l’enquête. 7. Appliquer la fonction summary() sur la variable Age. Qu’est ce que cette fonction calcule et affiche? Représentations graphiques pour variables quantitatives 8. Tracer l’histogramme de la variable Age. Ecrire un titre correspondante à votre figure, modifier les noms des axes et les couleurs des bâtons. 9. Afficher une la boîte à moustache correspondante à la variable Age. Commenter ce qu’on observe sur cette figure. Indicateurs statistiques et représentations graphiques pour variables qualitatives 10. Choisir une variable qualitative parmi les variables de cette enquête. Justifier votre choix. Calculer et afficher les effectifs et les fréquences de cette variable. Pour voir les variables dans votre dataframe, vous pouvez utiliser la fonction names() pour afficher les noms des variables, ou str() (structure) pour voir toutes les colonnes, leur types, les quelques premières valeurs, etc.. Ou simplement dans Rstudio on peut voir la structure du dataframe dans la fenêtre “Environment”. 11. Afficher un diagramme circulaire (en utilisant la fonction pie()) pour la variable qualitative choisie. Analyse descriptive bivariée Indicateurs pour le croisement de deux variables qualitatives Le tableau de contingence est un moyen particulier de représenter simultanément deux caractères observés sur une même population, s’ils sont discrets ou bien continus et regroupés en classes. 12. Un tableau de contingence des effectifs joints croisant deux variables qualitatives est réalisé par la fonction table(). Effectuer et afficher le croisement de deux variables (Sex) et (Smoke). 13. Utiliser la fonction addmargins() pour ajouter au tableau les effectifs marginaux. Représentations graphiques pour le croisement de deux variables qualitatives On peut représenter le croisement de deux variables qualitatives avec un diagramme en bâtons. Dans le cas de deux variables qualitatives, la fonction barplot() prend comme premier paramètre le tableau de contingence. 14. Afficher un diagramme en bâtons représentant la distribution du tabagisme (variable Smoke) en fontion du sexe des étudiants. La figure souhaitée est la suivante: On remarque qu’il y a plus d’hommes que des femmes qui fument régulièrement. (selon l’enquête réalisée) Il est plus convenable dans ce cas de représenter le croisement de ces deux variables qualitatives à l’aide d’un mosaicplot. Indicateurs pour le croisement d’une variable qualitative et d’une variable quantitative Disons qu’on souhaite calculer la moyenne de fréquence cardiaque chez les hommes ayant répondu au questionnaire de l’enquête. On a besoin de filtrer le dataframe de la façon suivante: # On utilise la fontion subset pour créer un sous ensemble de nos données # Remarquer qu&#39;on utilise == pour comparer pulse_hommes = subset(donnees, donnees$Sex==&quot;Male&quot;) 15. Vérifier que le sous ensemble créé pulse_homme ne contient que des hommes. 16. Ensuite calculer les indicateurs statistiques de la fréquence cardiaque chez les hommes (vous pouvez utiliser la fonction summary()). 17. Faire la même chose mais pour les femmes. Représentation graphique pour le croisement d’une variable qualitative et d’une variable quantitative On peut réaliser une boîte à moustache des valeurs de la variable quantitative en fonction des modalités de la variables qualitative, pour cela on peut utliser la fonction boxplot(). Plus précisémeent, on utilise le paramètre formula qui permet de spécifier que nous voulons une boîte à moustache de la variable quantitative en fonction (caractère ~ ) de la variable quantitative. 18. Afficher sur la même figure la fréquence cardiaque en fontion du sexe. Interpréter la figure. La figure souhaitée est la suivante: 19. Afficher sur la même figure la taille (variable Height) en fontion du sexe. Interpréter la figure. Représentation graphique pour le croisement de deux variables quantitatives Un nuage de points entre les deux variables quantitatives est réalisé par la fonction plot(). Le premier paramètre correspond à la variable en abscisse et le deuxième à la variable en ordonnées. 20. Afficher la fréquence cardiaque en fonction de l’age. Modifier les paramètres de la figure (titre, noms des axes, couleurs des points, tailles, formes, etc..) ◼ "],
["tp-statistique-descriptive-avec-r-avec-corrections.html", "TP Statistique descriptive avec R (Avec corrections) Qu’est-ce que c’est que ? 1ère partie: Données quantitatives discrètes 2ème partie : Analyse descriptive", " TP Statistique descriptive avec R (Avec corrections) Qu’est-ce que c’est que ? C’est un langage de programmation et un logiciel gratuit et libre. Il est surtout utilisé pour le développement de programmes statistiques et des analyses de données. Il gagne en popularité depuis quelques années avec l’émergence de la data science et du fait qu’il est gratuit et ouvert (open-source). est née d’un projet de recherche mené par deux chercheurs, Ross Ihaka et Robert Gentleman à l’université d’Auckland (Nouvelle-Zélande) en 1993. En 1997 est mis en place le Comprehension R Archive Network (CRAN) qui centralise les contributions au projet. Depuis le projet connaît une croissance soutenue, grâce à des contributions de la part de milliers de personnes à travers le monde. RStudio C’est une IDE (Integrated Development Environment) ou Environnement Intégré de Développement. Il sert d’interface entre et l’utilisateur, offre à celui diverses commodités d’utilisation Une introduction au RStudio est présentée dans l’annexe A. Vous trouverez une bonne introduction à sur ce lien . 1ère partie: Données quantitatives discrètes Le nombre d’arbres plantés sur les parcelles d’un lotissement a été compté. Les données obtenues sont les suivantes: \\[1,2,4,1,6,3,2,1,2,0,1,2,2,1,3,0,3,2,1,2,2,3,2,3.\\] 1. Quelle est la nature de variable étudiée? 2. Rentrer ces données sous la forme d’un vecteur nommé arbres et affichez ce vecteur. arbres = c(1,2,4,1,6,3,2,1,2,0,1,2,2,1,3,0,3,2,1,2,2,3,2,3) arbres #ans&gt; [1] 1 2 4 1 6 3 2 1 2 0 1 2 2 1 3 0 3 2 1 2 2 3 2 3 3. Trier les valeurs de ce vecteur par ordre croissant. sort(arbres) # en ordre croissant #ans&gt; [1] 0 0 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 4 6 sort(arbres,decreasing = T) # en ordre décroissant #ans&gt; [1] 6 4 3 3 3 3 3 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 0 0 4. Donner la taille de l’échantillon (c’est-à-dire le nombre de composantes de ce vecteur) en la notant n et affichez sa valeur. n = length(arbres) n #ans&gt; [1] 24 Effectifs et fréquence 5. Montrer la séquence des modalités et la séquence des effectifs correspondants. sort(unique(arbres)) # les modalités #ans&gt; [1] 0 1 2 3 4 6 effectifs = table(arbres) # pour calculer les effectifs effectifs #ans&gt; arbres #ans&gt; 0 1 2 3 4 6 #ans&gt; 2 6 9 5 1 1 6. Montrer le tableau de fréquences et de pourcentages. frequences = effectifs/n frequences #ans&gt; arbres #ans&gt; 0 1 2 3 4 6 #ans&gt; 0.0833 0.2500 0.3750 0.2083 0.0417 0.0417 pourcentages = frequences * 100 # ou (effectifs*100)/n pourcentages #ans&gt; arbres #ans&gt; 0 1 2 3 4 6 #ans&gt; 8.33 25.00 37.50 20.83 4.17 4.17 7. Calculer et afficher les effectifs cumulés et les fréquences cumulées. effectifs_cumules = cumsum(effectifs) effectifs_cumules #ans&gt; 0 1 2 3 4 6 #ans&gt; 2 8 17 22 23 24 frequences_cumulees = cumsum(frequences) frequences_cumulees #ans&gt; 0 1 2 3 4 6 #ans&gt; 0.0833 0.3333 0.7083 0.9167 0.9583 1.0000 Mesures de tendance centrale 8. Calculer le nombre moyen d’arbres par parcelle. moyenne = mean(arbres) moyenne #ans&gt; [1] 2.04 9. Calculer le nombre maximum et le nombre minimum d’arbres sur une parcelle. max(arbres) #ans&gt; [1] 6 min(arbres) #ans&gt; [1] 0 10. Calculer le nombre médian d’arbres par parcelle. mediane = median(arbres) mediane #ans&gt; [1] 2 11. Utiliser la fonction summary() pour obtenir un tableau récapitulatif des indicateurs. summary(arbres) #ans&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #ans&gt; 0.00 1.00 2.00 2.04 3.00 6.00 Indicateurs de dispersion 12. Calculer la variance du nombre d’arbres plantés sur les parcelles. var(arbres) #ans&gt; [1] 1.69 13. Calculer maintenant l’écart-type et vérifier que l’écart-type est la racine carrée de la variance. ecart_type = sd(arbres) ecart_type #ans&gt; [1] 1.3 ecart_type^2 #ans&gt; [1] 1.69 14. Calculer la variance vous-même. La variance obtenue est elle la même que la précédente? # la fonction var() calcule la variance corrigée variance = sum((arbres-mean(arbres))^2)/length(arbres) variance #ans&gt; [1] 1.62 Le logiciel utilise \\(n-1\\) pour le dénomiateur dans la définition de la variance, c’est-à-dire \\(\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) (d’écart-type noté \\(\\sigma_{n-1}\\) ou \\(s\\). Cette quantité est souvent préférée dans les applications numériques pour des questions d’estimation). Représentations graphiques 15. La fontion plot() affiche par défaut un nuage de points avec en abscisse le numéro de l’observation (ici de 1 à 24) et en ordonnée le nombre d’arbres. Tester cette fonction. Modifier le titre de la figure, les noms des axes, la couleur et la forme des points affichés. plot(arbres) 16. Afficher la courbe des fréquence cumulées. (Indication: Utiliser la fonction ecdf()). plot(ecdf(arbres)) 17. Tracer un diagramme en bâtons par la fonction barplot() à partir du tableau des effectifs ou des fréquences. barplot(effectifs, xlab=&quot;nombre d&#39;arbres&quot;, ylab=&quot;effectifs&quot;) barplot(frequences, xlab=&quot;nombre d&#39;arbres&quot;, ylab=&quot;fréquence&quot;) 2ème partie : Analyse descriptive Données utilisées Une enquête a été réalisée sur 237 étudiants. Les données sont les suivantes: Sex: The sex of the student. (Factor with levels “Male” and “Female”.) Wr.Hnd: span (distance from tip of thumb to tip of little finger of spread hand) of writing hand, in centimetres. NW.Hnd: span of non-writing hand. W.Hnd: writing hand of student. (Factor, with levels “Left” and “Right”.) Fold: “Fold your arms! Which is on top” (Factor, with levels “R on L”, “L on R”, “Neither”.) Pulse: pulse rate of student (beats per minute). Clap: ‘Clap your hands! Which hand is on top?’ (Factor, with levels “Right”, “Left”, “Neither”.) Exer: how often the student exercises. (Factor, with levels “Freq” (frequently), “Some”, “None”.) Smoke: how much the student smokes. (Factor, levels “Heavy”, “Regul” (regularly), “Occas” (occasionally), “Never”.) Height: height of the student in centimetres. M.I: whether the student expressed height in imperial (feet/inches) or metric (centimetres/metres) units. (Factor, levels “Metric”, “Imperial”.) Age: age of the student in years. Définition du répertoire de travail Vous avez la possibilité de définir un Répertoire de travail dans lequel vous allez stocker votre script R, vos données etc… Ceci est réalisé par la fonction setwd(\"..Chemin/de/votre/repertoire\"). Cette fonction considère comme seul paramètre le chemin d’accès au répertoire que vous avez choisi. A tout moment, vous pouvez vérifier le répertoire de travail courant en executant l’instruction suivante: getwd() 1. Définisser votre répertoire de travail. # avec un setwd() ou bien depuis la fenetre des fichiers Chargement des données Il existe une multitude de fonctions qui permettent de charger un fichier de données. Télécharger le fichier de données en cliquant ici et enregistrer le dans votre répertoire. Ensuite utiliser la fonction read.csv() pour charger les données dans . Cette fonction prend comme principaux paramètres d’entrée le fichier à charger (file=\"data.txt\"), le séparateur de colonnes dans le fichier initial (sep=) et la présence (ou non) des noms de colonnes dans le fichier (header=). Ouvrez toujours le fichier de données dans un éditeur de texte pour connaitre le séparateur de colonnes et voir si les noms de colonnes sont présents. 2. Charger le fichier des données dans l’enquête dans un tableau nommé data. L’instruction de chargement du fichier est la suivante: donnees = read.csv(&quot;enquete.csv&quot;, header = T , sep=&quot;,&quot;) Le fichier de données est chargé dans l’environnement et est affecté à l’objet donnees. C’est cet objet, de type dataframe qui va faire l’objet de manipulations par la suite. 3. Afficher le nombre de d’observations (lignes) et le nombre de variables (colonnes). dim(donnees) # pour afficher les deux #ans&gt; [1] 168 12 nrow(donnees) # nb de lignes #ans&gt; [1] 168 ncol(donnees) # nb de colonnes #ans&gt; [1] 12 4. Utiliser la fonction head() pour afficher les premières lignes (6 par défaut) de données chargées. head(donnees) #ans&gt; Sex Wr.Hnd NW.Hnd W.Hnd Fold Pulse Clap Exer Smoke Height #ans&gt; 1 Female 18.5 18.0 Right R on L 92 Left Some Never 173 #ans&gt; 2 Male 19.5 20.5 Left R on L 104 Left None Regul 178 #ans&gt; 3 Male 20.0 20.0 Right Neither 35 Right Some Never 165 #ans&gt; 4 Female 18.0 17.7 Right L on R 64 Right Some Never 173 #ans&gt; 5 Male 17.7 17.7 Right L on R 83 Right Freq Never 183 #ans&gt; 6 Female 17.0 17.3 Right R on L 74 Right Freq Never 157 #ans&gt; M.I Age #ans&gt; 1 Metric 18.2 #ans&gt; 2 Imperial 17.6 #ans&gt; 3 Metric 23.7 #ans&gt; 4 Imperial 21.0 #ans&gt; 5 Imperial 18.8 #ans&gt; 6 Metric 35.8 5. L’accès à une colonne d’un dataframe se fait par la notation $: nom_du_dataframe$nom_variable. Afficher les valeurs de la variable Age de vos données. donnees$Age #ans&gt; [1] 18.2 17.6 23.7 21.0 18.8 35.8 19.0 22.3 28.5 17.5 19.3 18.3 17.9 17.9 #ans&gt; [15] 18.2 17.8 18.2 17.5 18.1 19.2 17.5 39.8 17.2 18.0 17.9 35.5 17.1 17.5 #ans&gt; [29] 18.9 19.4 18.4 30.8 18.5 17.5 18.3 17.4 20.0 17.2 17.7 20.3 17.3 17.5 #ans&gt; [43] 18.6 17.6 17.7 17.4 17.8 20.7 23.6 17.1 20.2 17.2 17.2 18.0 18.8 21.6 #ans&gt; [57] 19.7 19.7 22.8 19.4 23.2 19.1 17.8 20.2 17.7 18.2 18.6 17.8 24.2 18.2 #ans&gt; [71] 21.2 17.9 17.4 20.5 22.9 18.9 18.9 20.1 18.2 17.5 17.4 21.0 17.7 18.1 #ans&gt; [85] 18.0 18.3 20.0 18.8 18.5 18.4 19.2 19.3 18.7 17.5 17.2 19.0 19.2 19.0 #ans&gt; [99] 23.0 32.7 20.0 20.2 25.5 18.2 23.5 70.4 43.8 23.6 44.2 17.9 18.4 17.5 #ans&gt; [113] 29.1 18.5 18.2 32.8 17.3 18.7 18.7 17.8 17.2 36.6 23.1 17.2 23.4 17.1 #ans&gt; [127] 17.2 23.8 18.8 21.2 24.7 18.5 20.3 20.1 18.9 27.3 18.9 17.2 26.5 17.0 #ans&gt; [141] 17.2 19.2 17.5 19.2 21.3 20.2 18.7 17.1 17.4 18.6 19.5 17.2 17.2 20.4 #ans&gt; [155] 17.1 19.3 18.9 17.3 18.2 18.4 17.4 20.3 19.3 18.2 17.7 16.9 17.2 17.8 Analyse descriptive univariée Indicateurs statistiques pour variables quantitatives 6. Calculer et afficher la moyenne et l’écart-type d’age des élèves qui ont participé à l’enquête. moyenne_age = mean(donnees$Age) moyenne_age #ans&gt; [1] 20.4 ecarttype_age= sd(donnees$Age) ecarttype_age #ans&gt; [1] 6.1 7. Appliquer la fonction summary() sur la variable Age. Qu’est ce que cette fonction calcule et affiche? summary(donnees$Age) #ans&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #ans&gt; 16.9 17.7 18.6 20.4 20.2 70.4 Représentations graphiques pour variables quantitatives 8. Tracer l’histogramme de la variable Age. Ecrire un titre correspondante à votre figure, modifier les noms des axes et les couleurs des bâtons. hist(donnees$Age, main=&quot;histogrammes des ages&quot;, xlab=&quot;Age&quot;, ylab=&quot;effectifs&quot;, col = &quot;steelblue&quot;) 9. Afficher une la boîte à moustache correspondante à la variable Age. Commenter ce qu’on observe sur cette figure. boxplot(donnees$Age) boxplot(donnees$Age, horizontal = T) # plus lisible Indicateurs statistiques et représentations graphiques pour variables qualitatives 10. Choisir une variable qualitative parmi les variables de cette enquête. Justifier votre choix. Calculer et afficher les effectifs et les fréquences de cette variable. Pour voir les variables dans votre dataframe, vous pouvez utiliser la fonction names() pour afficher les noms des variables, ou str() (structure) pour voir toutes les colonnes, leur types, les quelques premières valeurs, etc.. Ou simplement dans Rstudio on peut voir la structure du dataframe dans la fenêtre “Environment”. str(donnees) # str pour structure #ans&gt; &#39;data.frame&#39;: 168 obs. of 12 variables: #ans&gt; $ Sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 1 2 1 2 2 1 1 ... #ans&gt; $ Wr.Hnd: num 18.5 19.5 20 18 17.7 17 20 18.5 17 19.5 ... #ans&gt; $ NW.Hnd: num 18 20.5 20 17.7 17.7 17.3 19.5 18.5 17.2 20.2 ... #ans&gt; $ W.Hnd : Factor w/ 2 levels &quot;Left&quot;,&quot;Right&quot;: 2 1 2 2 2 2 2 2 2 2 ... #ans&gt; $ Fold : Factor w/ 3 levels &quot;L on R&quot;,&quot;Neither&quot;,..: 3 3 2 1 1 3 3 3 1 1 ... #ans&gt; $ Pulse : int 92 104 35 64 83 74 72 90 80 66 ... #ans&gt; $ Clap : Factor w/ 3 levels &quot;Left&quot;,&quot;Neither&quot;,..: 1 1 3 3 3 3 3 3 3 2 ... #ans&gt; $ Exer : Factor w/ 3 levels &quot;Freq&quot;,&quot;None&quot;,..: 3 2 3 3 1 1 3 3 1 3 ... #ans&gt; $ Smoke : Factor w/ 4 levels &quot;Heavy&quot;,&quot;Never&quot;,..: 2 4 2 2 2 2 2 2 2 2 ... #ans&gt; $ Height: num 173 178 165 173 183 ... #ans&gt; $ M.I : Factor w/ 2 levels &quot;Imperial&quot;,&quot;Metric&quot;: 2 1 2 1 1 2 2 2 1 2 ... #ans&gt; $ Age : num 18.2 17.6 23.7 21 18.8 ... # prenant Smoke ou Sex table(donnees$Smoke) #ans&gt; #ans&gt; Heavy Never Occas Regul #ans&gt; 7 134 13 14 11. Afficher un diagramme circulaire (en utilisant la fonction pie()) pour la variable qualitative choisie. pie(table(donnees$Smoke)) ### Analyse descriptive bivariée {-} Indicateurs pour le croisement de deux variables qualitatives Le tableau de contingence est un moyen particulier de représenter simultanément deux caractères observés sur une même population, s’ils sont discrets ou bien continus et regroupés en classes. 12. Un tableau de contingence des effectifs joints croisant deux variables qualitatives est réalisé par la fonction table(). Effectuer et afficher le croisement de deux variables (Sex) et (Smoke). table(donnees$Sex,donnees$Smoke) #ans&gt; #ans&gt; Heavy Never Occas Regul #ans&gt; Female 4 70 7 3 #ans&gt; Male 3 64 6 11 13. Utiliser la fonction addmargins() pour ajouter au tableau les effectifs marginaux. addmargins(table(donnees$Sex,donnees$Smoke)) #ans&gt; #ans&gt; Heavy Never Occas Regul Sum #ans&gt; Female 4 70 7 3 84 #ans&gt; Male 3 64 6 11 84 #ans&gt; Sum 7 134 13 14 168 Représentations graphiques pour le croisement de deux variables qualitatives On peut représenter le croisement de deux variables qualitatives avec un diagramme en bâtons. Dans le cas de deux variables qualitatives, la fonction barplot() prend comme premier paramètre le tableau de contingence. colors&lt;-c(&quot;darkred&quot;,&quot;darkblue&quot;) barplot(table(donnees$Sex,donnees$Smoke),col=colors, main=&quot;Distribution du tabagisme en fonction du sexe&quot;, xlab=&quot;&quot;,ylab=&quot;Effectifs&quot;) legend(&quot;topright&quot;, xpd = TRUE, legend = c(&quot;Femme&quot;,&quot;Homme&quot;),fill=colors) On remarque qu’il y a plus d’hommes que des femmes qui fument régulièrement. (selon l’enquête réalisée) Il est plus convenable dans ce cas de représenter le croisement de ces deux variables qualitatives à l’aide d’un mosaicplot. mosaicplot(Smoke~Sex, data=donnees, color=colors, main = &quot;Distribution du tabagisme en fonction du sexe&quot;) Indicateurs pour le croisement d’une variable qualitative et d’une variable quantitative Disons qu’on souhaite calculer la moyenne de fréquence cardiaque chez les hommes ayant répondu au questionnaire de l’enquête. On a besoin de filtrer le dataframe de la façon suivante: # On utilise la fontion subset pour créer un sous ensemble de nos données # Remarquer qu&#39;on utilise == pour comparer pulse_hommes = subset(donnees, donnees$Sex==&quot;Male&quot;) 15. Vérifier que le sous ensemble créé pulse_homme ne contient que des hommes. table(pulse_hommes$Sex) # ou summary(pulse_hommes$Sex) #ans&gt; Error in table(pulse_hommes$Sex): object &#39;pulse_hommes&#39; not found 16. Ensuite calculer les indicateurs statistiques de la fréquence cardiaque chez les hommes (vous pouvez utiliser la fonction summary()). summary(pulse_hommes$Pulse) #ans&gt; Error in summary(pulse_hommes$Pulse): object &#39;pulse_hommes&#39; not found 17. Faire la même chose mais pour les femmes. pulse_femmes = subset(donnees, donnees$Sex==&quot;Female&quot;) summary(pulse_femmes$Pulse) #ans&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #ans&gt; 40.0 68.0 74.0 74.8 80.0 104.0 Représentation graphique pour le croisement d’une variable qualitative et d’une variable quantitative On peut réaliser une boîte à moustache des valeurs de la variable quantitative en fonction des modalités de la variables qualitative, pour cela on peut utliser la fonction boxplot(). Plus précisémeent, on utilise le paramètre formula qui permet de spécifier que nous voulons une boîte à moustache de la variable quantitative en fonction (caractère ~ ) de la variable quantitative. 18. Afficher sur la même figure la fréquence cardiaque en fontion du sexe. Interpréter la figure. La figure souhaitée est la suivante: boxplot(formula=donnees$Pulse~donnees$Sex,col=&quot;darkred&quot;, main=&quot;Fréquence cardiaque en fonction du sexe&quot;, xlab=&quot;Sexe&quot;,ylab=&quot;Fréquence cardiaque&quot;) 19. Afficher sur la même figure la taille (variable Height) en fontion du sexe. Interpréter la figure. boxplot(formula=donnees$Height~donnees$Sex,col=&quot;darkred&quot;, main=&quot;Taille en fonction du sexe&quot;, xlab=&quot;Sexe&quot;,ylab=&quot;Taille&quot;) Représentation graphique pour le croisement de deux variables quantitatives Un nuage de points entre les deux variables quantitatives est réalisé par la fonction plot(). Le premier paramètre correspond à la variable en abscisse et le deuxième à la variable en ordonnées. 20. Afficher la fréquence cardiaque en fonction de l’age. Modifier les paramètres de la figure (titre, noms des axes, couleurs des points, tailles, formes, etc..) plot(donnees$Age, donnees$Pulse, pch=21, bg=&quot;darkred&quot;, main=&quot;Fréquence cardiaque en fonction de l&#39;age&quot;, xlab=&quot;Age&quot;,ylab=&quot;Fréquence cardiaque&quot;) ◼ "],
["echantillonnage-et-theoremes-limites.html", "Chapitre 2 Échantillonnage et Théorèmes limites 2.1 Échantillonnage 2.2 La statistique \\(\\overline{X}_n\\) 2.3 Théorèmes limites 2.4 Loi d’un pourcentage 2.5 Etude de la statistique \\(S^2\\) 2.6 Introduction à l’estimation", " Chapitre 2 Échantillonnage et Théorèmes limites 2.1 Échantillonnage L’étude d’une caractéristique d’une pièce fabriquée en grand nombre (telle que la luminosité d’une ampoule, sa durée de vie ou encore le diamètre d’une pièce mécanique) relève, elle aussi, de la statistique descriptive. Il n’est toutefois pas possible de mesurer cette caractéristique sur toutes les pièces produites. Il est alors nécessaire de se limiter à l’étude des éléments d’un échantillon. Cet échantillon devra répondre à des critères particuliers pour pouvoir représenter la population toute entière dans l’étude statistique. La démarche statistique présente plusieurs étapes : Prélèvement d’un échantillon représentatif de la population ou échantillon aléatoire par des techniques appropriées. Cela relève de la théorie de l’échantillonnage. Étude des caractéristiques de cet échantillon, issu d’une population dont on connaît la loi de probabilité. On s’intéresse principalement à ceux issus d’une population gaussienne. Définition 2.1 (Echantillon) Un échantillon aléatoire est un \\(n\\)-uplet \\((X_1,\\ldots,X_n)\\) de \\(n\\) variables aléatoires indépendantes suivant la même loi qu’une variable \\(X\\) appelée variable aléatoire parente. Une réalisation de l’échantillon sera notée \\((x_1,\\ldots,x_n)\\). Définition 2.2 (Une statistique) Soit \\(X\\) une variable aléatoire. Considérons un \\(n\\)-échantillon \\((X_1,\\ldots,X_n)\\) de \\(X\\). Une Statistique \\(T\\) est une variable aléatoire fonction mesurable de \\((X_1,\\ldots,X_n)\\). \\[ T(X)=T(X_1,\\ldots,X_n) \\] A un échantillon, on peut associer plusieurs statistiques. 2.2 La statistique \\(\\overline{X}_n\\) Définition 2.3 (Moyenne empirique) La statistique \\(\\overline{X}_n\\) ou moyenne empirique d’un échantillon est une statistique définie par \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\] Espérance et variance de \\(\\overline{X}_n\\): Soit \\(m\\) l’espérance et \\(\\sigma^2\\) la variance de la variable parente \\(X\\) (e.g. l’espérance et la variance de la population). L’espérance et la variance de la statistique \\(\\overline{X}_n\\) sont: \\[E(\\overline{X}_n) = m\\] \\[V(\\overline{X}_n) = \\frac{\\sigma^2}{n}\\] Démonstration: \\(E(\\overline{X}_n) = \\frac{1}{n} \\sum_{i=1}^n E(X_i) = \\frac{1}{n} nm = m\\) \\(V(\\overline{X}_n) =V(\\frac{1}{n} \\sum_{i=1}^n X_i) = \\frac{1}{n^2} V(\\sum_{i=1}^n X_i) = \\frac{1}{n^2} \\sum_{i=1}^n V(X_i) = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma^2 = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\quad \\quad\\) (Les \\(X_i\\) étant supposées indépendantes) 2.3 Théorèmes limites Cette section introduit trois résultats importants de la théorie asymptotique des probabilités: la loi faible des grands nombres, la loi forte des grands nombres et le théorème central limite, dans sa version pour variables aléatoires indépendantes et identiquement distribuées (\\(X_i\\) suivent la même loi pour \\(i=1,\\ldots,n\\)). Ce sont des résultats qui traitent les propriétés de la distribution de la statistique \\(\\overline{X}_n\\). Des variables aléatoires indépendantes et identiquement distribuées (i.i.d) sont des variables aléatoires indépendantes qui suivent la même loi de probabilité, donc ont la même espérance et la même variance. Les deux lois des grands nombres énoncent les conditions sous lesquelles la moyenne d’une suite de variables aléatoires converge vers leur espérance commune et expriment l’idée que lorsque le nombre d’observations augmente, la différence entre la valeur attendue (\\(m = E(X)\\)) et la valeur observée (\\(\\overline{X}_n\\)) tend vers zéro. De son côté, le théorème central limite établit que la distribution standardisée d’une moyenne tend asymptotiquement vers une loi normale, et cela même si la distribution des variables sous-jacentes est non normale. Ce résultat est central en probabilités et statistique et peut être facilement illustré (cf. figure 2.1). Indépendamment de la distribution sous-jacente des observations (ici une loi uniforme), lorsque \\(n\\) croît, la distribution de \\(\\overline{X}_n\\) tend vers une loi normale: on observe dans l’illustration la forme de plus en plus symétrique de la distribution ainsi que la concentration autour de l’espérance (ici \\(m = 0.5\\)) et la réduction de la variance. Figure 2.1: Illustration du théorème central limite: histogramme de la moyenne de 200 échantillons issus d’une loi uniforme sur l’intervalle (0,1) en fonction de la taille \\(n\\) de l’échantillon. Les retombées pratiques de ces résultats sont importantes. En effet, la moyenne de variables aléatoires est une quantité qui intervient dans plusieurs procédures statistiques. Aussi, le résultat du théorème central limite permet l’approximation des probabilités liées à des sommes de variables aléatoires (e.g. méthode de Monte-Carlo). De plus, lorsque l’on considère des modèles statistiques, le terme d’erreur représente la somme de beaucoup d’erreurs (erreurs de mesure, variables non considérées, etc.). En prenant comme justification le théorème central limite, ce terme d’erreur est souvent supposé se comporter comme une loi normale. 2.3.1 Loi faible des grands nombres Théorème 2.1 (Loi faible des grands nombres) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées. On suppose que \\(E(|X_i|) &lt; \\infty\\) et que tous les \\(X_i\\) admettent la même espérance \\(E(X_i)=m\\). Alors: \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\rightarrow m \\quad \\text{au sens de la convergence en probabilités}\\] càd \\(\\forall \\varepsilon &gt; 0, \\lim_{n \\to \\infty} P(|\\overline{X}_n - m|&gt; \\varepsilon) = 0\\). \\(\\overline{X}_n \\text{ converge en probabilité vers } m \\text{ quand } n \\rightarrow +\\infty\\). 2.3.2 Loi forte des grands nombres Théorème 2.2 (Loi forte des grands nombres) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées. On suppose que \\(E(|X_i|) &lt; \\infty\\) et que tous les \\(X_i\\) admettent la même espérance \\(E(X_i)=m\\). Alors: \\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\rightarrow m \\quad \\text{presque sûrement}\\] càd \\(P\\big( \\lim_{n \\to \\infty} \\overline{X}_n = m\\big) = 1\\). \\(\\overline{X}_n \\text{ converge presque sûrement vers } m\\) Illustration de la loi des grands nombres Prenons l’exemple d’un lancer de dé équilibré. On lance un dé et on note \\(X\\) le résultat obtenu. La loi de \\(X\\) est la suivante: \\(x_i\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(P(X = x_i)\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) Et donc l’espérance de \\(X\\) est \\(E(X)=\\sum_i p_i x_i = 3.5\\). Pour illustrer le théorème on va procéder à un échantillonnage. On répète le lancement du dé \\(n\\) fois. A chaque \\(n\\) on va calculer la moyenne empirique des résultats obtenus, qu’on va noter \\(\\overline{X}_n\\). Selon la loi de grands nombre cette moyenne va converger vers l’espérance théorique: \\[\\overline{X}_n \\rightarrow E(X)=m=3.5 \\quad \\text{quand} \\quad n \\rightarrow \\infty\\] Traçons l’évolution de la moyenne empirique en fonction de la taille \\(n\\) de l’échantillon, dans deux différents échantillonnages aléatoires: Figure 2.2: Convergence de \\(\\overline{X}_n\\) vers \\(m=3.5\\) pour deux différents échantillonnages 2.3.3 Théorème central limite Théorème 2.3 (Théorème central limite) Soit \\(X_1,\\ldots,X_n\\) une suite de variables aléatoires indépendantes et identiquement distribuées, d’espérance \\(m\\) et variance \\(\\sigma^2\\) finie. Alors \\[\\begin{equation} \\frac{\\overline{X}_n - m}{ \\sigma/\\sqrt{n}} \\xrightarrow{n \\to \\infty} \\mathcal{N}(0,1) \\quad \\text{en distribution} \\tag{2.1} \\end{equation}\\] On voit bien que, afin que la convergence se fasse, une standardisation est nécessaire: en effet, on peut voir le rapport dans (2.1) comme \\[\\frac{\\overline{X}_n - m}{ \\sigma/\\sqrt{n}} = \\frac{\\overline{X}_n - E(\\overline{X}_n)}{ \\sqrt{var(\\overline{X}_n)}}\\] Notes Historiques La loi faible des grands nombres a été établie la première fois par J. Bernoulli pour le cas particulier d’une variable aléatoire binaire ne prenant que les valeurs 0 ou 1. Le résultat a été publié en 1713. La loi forte des grands nombres est due au mathématicien E. Borel (1871- 1956), d’où parfois son autre appellation: théorème de Borel. Le théorème central limite a été formulé pour la première fois par A. de Moivre en 1733 pour approximer le nombre de “piles” dans le jet d’une pièce de monnaie équilibrée. Ce travail a été un peu oublié jusqu’à ce que P.S. Laplace ne l’étende à l’approximation d’une loi binomiale par la loi normale dans son ouvrage Théorie analytique des probabilités en 1812. C’est dans les premières années du \\(XX^e\\) siècle que A. Lyapounov l’a redéfini en termes généraux et prouvé avec rigueur. Application 1: Pour une taille d’échantillon \\(n\\) suffisamment grande, on peut considérer que \\(\\overline{X}_n\\) a pour loi: \\[ \\overline{X}_n \\thicksim \\mathcal{N}\\big(m, \\frac{\\sigma^2}{n}\\big)\\] Dans la notation de la loi normale ci dessus \\(\\frac{\\sigma^2}{n}\\) est la variance. \\(\\frac{\\sigma}{\\sqrt{n}}\\) est l’écart-type. Application 2: la loi d’un pourcentage, étudiée dans la section suivante. 2.4 Loi d’un pourcentage Soit \\(X\\) la variable aléatoire représentant le nombre de succès au cours d’une suite de \\(n\\) répétitions indépendantes d’une même épreuve dont la probabilité de succès est \\(p\\). La loi de \\(X\\) est la loi binomiale de paramètres \\(n\\) et \\(p\\) notée \\(\\mathcal{B}(n, p)\\). \\(X\\) est la somme de \\(n\\) variables indépendantes de Bernoulli de paramètre \\(p\\). Notons \\(P_n\\) la fréquence empirique du nombre de succès parmi les \\(n\\) épreuves: \\[P_n= \\frac{X}{n}\\] \\(P_n = \\overline{X}_n\\) car \\(X\\) est la somme de \\(n\\) variables indépendantes de Bernoulli de paramètre \\(p\\). \\(P_n\\) a pour espérance et pour variance: \\[E(P_n) = p \\quad \\quad \\text{et} \\quad \\quad V(P_n) = \\frac{p(1-p)}{n}\\] En appliquant le théorème central limite à \\(X\\) somme des variables de Bernoulli: \\[ \\text{Pour } n \\text{ suffisamment grand, on peut être considérer que } P_n \\text{ suit la loi normale :}\\] \\[P_n \\thicksim \\mathcal{N}\\bigg(p, {\\frac{p(1-p)}{n}}\\bigg)\\] Ce résultat est une autre formulation du théorème de “De Moivre-Laplace” (lien). 2.5 Etude de la statistique \\(S^2\\) Définition 2.4 (Variance empirique) La statistique \\(S_n^2\\) ou variance empirique d’échantillon est définie par: \\[S_n^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_n\\right)^{2}\\] Propriétés: \\(S_n^{2}=\\frac{1}{n} \\big(\\sum_{i=1}^{n}X_{i}^2\\big)-\\big(\\overline{X}_n\\big)^{2}\\). \\(S_n^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\big(X_{i}-m\\big)^2-\\big(\\overline{X}_n-m\\big)^{2}\\). \\(S_n^{2} \\text{ converge presque sûrement vers } \\sigma^2\\). Espérance de \\(S_n^{2}\\): L’espérance de \\(S_n^{2}\\) est: \\[E(S_n^{2}) = \\frac{n-1}{n}\\sigma^2\\] démonstration: \\[\\begin{align} E(S_n^{2}) &amp; =\\frac{1}{n} \\sum_{i=1}^{n}E\\big(X_{i}-m\\big)^2-E\\big(\\overline{X}_n-m\\big)^{2} \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n}V(X_i)-V(\\overline{X}_n) \\\\ &amp; = \\frac{1}{n} \\sum_{i=1}^{n}\\sigma^2- \\frac{\\sigma^2}{n} = \\sigma^2- \\frac{\\sigma^2}{n} = \\frac{n-1}{n} \\sigma^2 \\end{align}\\] On peut remarquer que si on pose: \\({S_n^{*}}^2 = \\frac{n}{n-1}S_n^2\\) alors \\(E({S_n^{*}}^2) = \\sigma^2\\). 2.6 Introduction à l’estimation On appelle estimation, la procédure d’utilisation des informations obtenues à partir d’un échantillon qui permet de déduire des résultats concernant l’ensemble de la population. Dans ce cours, les estimations sont calculées à partir d’un échantillonnage aléatoire simple et avec remise, càd tous les individus de la population ont une probabilité égale de faire partie de l’échantillon et qu’un individu peut être choisi plus d’une fois. La statistique inconnue d’une population, à estimer à partir d’un échantillon, est appelée un paramètre. Souvent le paramètre à estimer est une moyenne, un total, une proportion, un écart-type ou une variance. Le paramètre de la population est estimé à partir d’une estimation calculée à partir des données d’un échantillon. Le tableau ci dessous illustre les différents symboles souvent utilisés. Paramètres population Estimations calculées sur un échantillon de taille \\(n\\) Moyenne \\(m\\) \\(\\overline{X}_n=\\hat{m}\\) Ecart-type \\(\\sigma\\) \\(S_n=\\hat{\\sigma}\\) Variance \\(\\sigma^2\\) \\(S_n^2=\\hat{\\sigma}^2\\) Proportion \\(p\\) \\(P_n=\\hat{p}\\) On dit que \\(\\overline{X}_n\\) est un estimateur de \\(m\\). La valeur obtenue est une estimation de \\(m\\), qu’on appelle \\(\\hat{m}\\). Les estimateurs sont des variables aléatoires et les estimations sont les valeurs observées des estimateurs (i.e des variables aléatoires). Dans ce chapitre nous avons introduit des estimateurs de la moyenne, la variance et la proportion. Nous avons aussi étudié les lois de ces estimateurs. Dans le chapitre suivant, nous allons étudier la théorie de l’estimation ponctuelle et la recherche d’un estimateur. "],
["exercices-1.html", "Exercices", " Exercices Exercice 2.1 (Loi de \\(\\overline{X}_n\\)) Une population est composée de 3 salariés A, B et C âgés respectivement de 23, 37 et 45 ans. On choisit au hasard un salarié. Définir l’expérience aléatoire \\(\\varepsilon\\), la population \\(\\Omega\\), la probabilité \\(P\\) et la variable aléatoire \\(X\\) étudiée. Calculer \\(E(X)=m\\) et \\(V(X)=\\sigma^2\\). Que représentent \\(E(X)\\) et \\(V(X)\\)? On choisit maintenant au hasard un échantillon de 2 salariés. Définir la nouvelle expérience aléatoire \\(\\varepsilon_n\\), l’ensemble des échantillons \\(E_n\\) et les variables alétoires \\(X_i, i=1,\\ldots,n\\). Définir la variable alétoire \\(\\overline{X}_n\\) et déterminer sa loi. Calculer \\(E(\\overline{X}_n)\\) et \\(V(\\overline{X}_n)\\). Retrouver les formules du cours. Exercice 2.2 (Loi de \\(P_n\\)) Une population est composée de 3 individus A, B et C dont les résultats de vote pour un certain candidat sont respectivement les suivants: Non, Non et Oui. On choisit au hasard un individu. Définir l’expérience aléatoire \\(\\varepsilon\\), la population \\(\\Omega\\), la probabilité \\(P\\) et la variable aléatoire \\(X\\) étudiée. Calculer \\(E(X)\\) et \\(V(X)\\). Que représente \\(E(X)\\)? On choisit maintenant au hasard un échantillon de 2 individus. Définir la nouvelle expérience aléatoire \\(\\varepsilon_n\\), l’ensemble des échantillons \\(E_n\\) et les variables aléatoires \\(X_i, i=1,\\ldots,n\\). Définir la variable aléatoire \\(P_n\\) et déterminer sa loi. Calculer \\(E(P_n)\\) et \\(V(P_n)\\). Retrouver les formules du cours. Exercice 2.3 Est ce qu’il s’agit d’une variable aléatoire? Moyenne de la population. Taille de la population. Taille de l’échantillon. Moyenne de l’échantillon. Variance de la moyenne de l’échantillon. Plus grande valeur de l’échantillon. Variance de la population. Variance estimée de la moyenne de l’échantillon. Exercice 2.4 (Théorème Central Limite) Devant l’augmentation des problèmes de poids dans la population européenne, une nouvelle étude est mandatée pour mesurer la relation entre celui-ci et la quantité de calories ingérées par habitant. Des études antérieures montrent qu’un Européen consomme en moyenne \\(2700\\) calories par jour avec un écart-type de \\(800\\). On dispose dans cette étude d’un échantillon de \\(500\\) Européens. Définir les variables aléatoires étudiées dans l’énoncé. On les nomme \\(X_i\\) et \\(\\overline{X}_n\\). Utiliser le TCL pour donner la distribution de la v.a. \\(\\overline{X}_n\\). Calculer la probabilité que la moyenne des calories consommées par jour par les Européens, qu’on a nommé \\(\\overline{X}_n\\), dans l’échantillon soit supérieure à \\(2750\\). Exercice 2.5 Afin d’estimer leur espérance respective, on échantillonne 2 populations. On utilise un échantillon de taille \\(n_{1}\\) pour la population 1, qui présente un écart-type égal à \\(\\sigma_{1}\\). Pour la population 2, dont l’écart-type vaut \\(\\sigma_{2}=2\\sigma_{1}\\), on prend un échantillon de taille \\(n_{2}=2 n_{1}\\). Pour lequel des 2 échantillons est-ce que l’estimation de la moyenne de la population est la plus précise? Exercice 2.6 Dans une certaine commune française, on veut estimer la proportion de familles vivant en dessous du seuil de pauvreté. Si cette proportion est environ \\(0.15\\), quelle est la taille de l’échantillon nécessaire pour que l’écart-type de l’estimateur soit égal à \\(0.02\\)? Extra Exercice 2.7 (Théorème Central Limite) Rémy et ses 9 amis voudraient jouer au bowling. Ils décident de rassembler leur argent de poche et espèrent obtenir la somme totale nécessaire. On peut supposer que l’argent de poche de chacun est une variable aléatoire \\(X_i\\) qui suit la loi exponentielle de paramètre \\(\\lambda=0.06\\). Sa densité est donc \\[ f(x)=0.06 e^{-0.06 x} \\times 1_{\\mathbb{R}^+}(x) \\] De plus, on admet que les \\(X_i\\) sont indépendantes. Démontrer que la loi exponentielle \\(\\mathcal{E}(\\lambda)\\) est un cas exceptionnel de la loi Gamma en donnant les paramètres de celle-ci. (un rappel de la définition de la loi Gamma est donnée ci dessous). Soit \\(S_{10}=\\sum_{i=1}^{10} X_{i}\\) Quelle est la loi de \\(S_{10}\\)? Sachant qu’une partie de bowling coûte \\(15 €\\), quelle est la probabilité que Rémy et ses amis puissent jouer une partie? (Indication: Appliquer le TCL pour \\(S_{10}= \\sum_{i=1}^n X_i = n \\times \\overline{X}_n\\)) Comment faut-il choisir \\(z&gt;0\\) pour que la probabilité que la somme totale d’argent du groupe soit supérieure à \\(z\\) soit égale à \\(5 \\%\\)? Loi Gamma \\(\\Gamma(a,\\lambda)\\) On dit que la variable aléatoire \\(X\\) suit une loi Gamma de paramètres \\(a&gt;0\\) et \\(\\lambda&gt;0\\), \\(X \\thicksim \\Gamma(a,\\lambda)\\) si \\(X\\) a la densité: \\[\\forall \\, x \\in \\mathbb{R}, \\quad f_{a,\\lambda} (x)= \\frac{\\lambda^a}{\\Gamma(a)} x^{a-1} e^{-\\lambda x} \\times {1}_{\\mathbb{R}_{+}}(x)\\] La fonction Gamma est définie sur \\(\\mathbb{R}_{+}^*\\) par: \\(\\Gamma(x) = \\int_0^{+\\infty} t^{x-1}e^{-t} dt\\). Propriétés de la fonction Gamma: \\(\\Gamma(x+1)=x\\Gamma(x) \\quad \\forall \\, x &gt;0\\) \\(\\Gamma(n+1) = n! \\quad \\forall n \\in \\mathbb{N}\\) \\(\\Gamma(1) = 1\\) et \\(\\Gamma(\\frac{1}{2})=\\sqrt{\\pi}\\) Propriétés de la loi Gamma: \\(E(X)= \\frac{a}{\\lambda}\\) et \\(V(X)= \\frac{a}{\\lambda^2}\\). Si \\((X_n)_{n \\in \\mathbb{N}}\\) est une suite de variables aléatoires indépendantes de lois \\(\\Gamma(a_n,\\lambda)\\) alors la variable aléatoire somme \\(\\sum_{n=1}^N X_n\\) suit également une loi gamma \\(\\Gamma(\\sum_{n=1}^N a_n, \\lambda)\\). "],
["tp-illustration-numerique-des-theoremes-limites-avec-r.html", "TP illustration numérique des théorèmes limites avec R Illustrations de théorèmes limites Bonus", " TP illustration numérique des théorèmes limites avec R Réalisations de variables aléatoires Pour générer (simuler) des réalisations de variables aléatoires on utilise la fonction qui commence par la lettre r (pour random) succédée par le nom de la loi que l’on souhaite simuler. Par exemple rnorm pour simuler des réalisations de la variable alétoire de loi normale. Tapez ?rnorm pour voir la liste des paramètres (arguments) de cette fonction. rnorm(10) # génére 10 réalisations alétoires de la loi normale centrée réduite. #ans&gt; [1] 1.7111 0.1184 0.2113 0.8255 0.4184 1.8033 0.3251 0.3784 0.0252 0.0345 rnorm(10, mean = 1, sd = 3) # génére 10 réalisations alétoires de la loi normale d&#39;espérance 1 et écart-type 3 #ans&gt; [1] 4.7308 5.9451 -3.1275 0.5337 -3.8268 3.9036 1.9544 0.0549 #ans&gt; [9] -0.6323 4.6497 1. Simuler un vecteur de 1000 réalisations indépendantes de loi uniforme sur l’intervalle \\([0,1]\\). Simuler ensuite 1000 réalisations de la loi exponentielle de paramètre 2. Extra: Afin de vérifier les simulations on peut afficher l’histogramme des réalisations et superposer avec la densité de la loi correspondante. Fonction de densité Pour calculer la densité d’une certaine loi, il suffit d’utiliser comme fonction le nom de la loi dans avec le préfixe d pour une densité. Taper ?dnorm pour comprendre le fonctionnement de cette commande. 2. Que vaut la densité de la loi normale centrée réduite en 0? Essayer la même chose pour la loi binomiale de paramètres \\(n=10\\) et \\(p=0.5\\). Cette commande permet de tracer facilement des fonctions de densité. 3. Que se passe-t-il si l’on demande la fonction de densité de la loi binomiale en 0.3? Pourquoi? 4. Tracer la fonction de densité de la loi normale centrée réduite. Commencer par créer un vecteur d’abscisses à l’aide de la fonction seq(). Tracer ensuite la fonction de densité en ces points. Il existe plusieurs paramètres réglables pour avoir des courbes de différents design. 5. Tracer un histogramme d’un vecteur de 10000 réalisations indépendantes d’une loi normale centrée réduite. Superposer l’histogramme avec la densité réelle de cette loi. Fonction de répartition Pour calculer la fonction de répartition d’une certaine loi on utilise le nom de la loi dans avec le préfixe p. 6. Que vaut la fonction de répartition de la loi normale centrée réduite en 0? 7. Tracer la fonction de répartition de la loi normale centrée réduite. On peut également tracer des fonctions de répartitions empiriques (calculées sur un échantillon). 8. Créer un vecteur de 100 réalisations indépendantes de la loi de votre choix. Utiliser la fonction ecdf() pour construire la fonction de répartition empirique de votre vecteur. Puis superposer avec la fonction de répartition théorique. Illustrations de théorèmes limites Loi forte de grands nombres On considère une variable \\(X\\) à valeurs dans \\(\\{0,1,3\\}\\), distribuée comme suit: \\(P(X=0)=0.5, P(X=1)=0.25, P(X=3)=0.25\\). 9. Proposer une façon de simuler \\(X\\). (Suggestion: utiliser la fonction sample()). 10. On considère \\(X_1,X_2,\\ldots\\) une suite infinie de variables indépendantes de même loi que \\(X\\). Soit \\(\\overline{X}_n\\) la moyenne empirique des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,n\\}\\). 10.1 Simuler la suite des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,10000\\}\\). 10.2 Produire un graphique représentant l’évolution de \\(\\overline{X}_n\\) pour \\(n\\) variant de 1 à 10000. (Indication: penser à utiliser la fonction cumsum()). 10.3 Que constate-on? Pouvait-on s’y attendre? Théorème central limite On désire maintenant approfondir comment \\(\\overline{X}_{500}\\) varie autour de sa valeur moyenne. 11. Proposer une transformation affine de \\(\\overline{X}_{500}\\), de la forme \\(Y=a \\times (\\overline{X}_{500} + b)\\), qui suive approximativement la loi \\(\\mathcal{N}(0,1)\\). 11.1 Simuler avec \\(10000\\) réalisations indépendantes de la variable \\(Y\\). Nous les noterons \\(Y_1, \\ldots, Y_{10000}\\). 11.2 Confirmer l’approximation gaussienne en réalisant un histogramme des valeurs prises par les \\(Y_j\\) pour \\(j \\in \\{1,\\ldots,10000\\}\\), sans oublier de tracer la densité gaussienne correctement renormalisée. Commenter l’écart entre l’histogramme et la densité gaussienne. Bonus Illustration du théorème de Moivre Laplace Pour \\((n=10, p=0.5),\\) puis \\((n=50, p=0.9),\\) puis \\((n=100, p=0.1)\\) Simuler un échantillon \\(X\\) de la loi binomiale de paramètres \\(n\\) et \\(p .\\) Calculer l’échantillon centré-réduit \\(Xcr=\\frac{x-n p}{\\sqrt{n p(1-p)}}\\). Représenter un histogramme des valeurs de \\(X c r .\\) Superposer sur le même graphique la densité de la loi normale \\(\\mathcal{N}(0,1) .\\) Représenter la fonction de répartition empirique de \\(X c r .\\) Superposer sur le même graphique la fonction de répartition de la loi normale \\(N(0,1)\\). "],
["tp-illustration-numerique-des-theoremes-limites-avec-r-avec-corrections.html", "TP illustration numérique des théorèmes limites avec R (Avec corrections) Illustrations de théorèmes limites Bonus", " TP illustration numérique des théorèmes limites avec R (Avec corrections) Réalisations de variables aléatoires Pour générer (simuler) des réalisations de variables aléatoires on utilise la fonction qui commence par la lettre r (pour random) succédée par le nom de la loi que l’on souhaite simuler. Par exemple rnorm pour simuler des réalisations de la variable alétoire de loi normale. Tapez ?rnorm pour voir la liste des paramètres (arguments) de cette fonction. rnorm(10) # génére 10 réalisations alétoires de la loi normale centrée réduite. #ans&gt; [1] -1.26e-01 -1.10e+00 -1.06e+00 3.22e-01 3.58e-01 -6.71e-05 -1.07e+00 #ans&gt; [8] 1.14e+00 2.65e-01 8.07e-01 rnorm(10, mean = 1, sd = 3) # génére 10 réalisations alétoires de la loi normale d&#39;espérance 1 et écart-type 3 #ans&gt; [1] 1.2251 -0.3493 0.8204 -2.7993 -1.5955 -0.3777 -3.7275 1.3010 #ans&gt; [9] 0.0191 -1.5619 1. Simuler un vecteur de 1000 réalisations indépendantes de loi uniforme sur l’intervalle \\([0,1]\\). Simuler ensuite 1000 réalisations de la loi exponentielle de paramètre 2. u = runif(1000,0,1) e = rexp(1000,2) Extra: Afin de vérifier les simulations on peut afficher l’histogramme des réalisations et superposer avec la densité de la loi correspondante. set.seed(1812) u = runif(1000,0,1) e = rexp(1000,2) par(mfrow=c(1,2)) hist(u,col=mycolor, prob=T, xlab=&quot;&quot;,main= &quot;1000 réalisations de la loi U(0,1)&quot;, cex.main=1) curve(dunif(x,0,1),xlim=c(0,1),add=T, col=&quot;red&quot;,lwd=3) # d&#39;autres moyens sont possibles hist(e,col=mycolor, prob=T, xlab=&quot;&quot;,main= &quot;1000 réalisations de la loi Exp(2)&quot;, cex.main=1) curve(dexp(x,2),add=T, col=&quot;red&quot;,lwd=3) # d&#39;autres moyens sont possibles Fonction de densité Pour calculer la densité d’une certaine loi, il suffit d’utiliser comme fonction le nom de la loi dans avec le préfixe d pour une densité. Taper ?dnorm pour comprendre le fonctionnement de cette commande. 2. Que vaut la densité de la loi normale centrée réduite en 0? Essayer la même chose pour la loi binomiale de paramètres \\(n=10\\) et \\(p=0.5\\). dnorm(0) # 1/sqrt(2*pi) #ans&gt; [1] 0.399 dbinom(0,10,0.5) # faire calcul manuel au tableau #ans&gt; [1] 0.000977 Cette commande permet de tracer facilement des fonctions de densité. 3. Que se passe-t-il si l’on demande la fonction de densité de la loi binomiale en 0.3? Pourquoi? dbinom(0.3, 10,0.5) # erreur car 0.3 n&#39;est pas une possibilité #ans&gt; Warning in dbinom(0.3, 10, 0.5): non-integer x = 0.300000 #ans&gt; [1] 0 4. Tracer la fonction de densité de la loi normale centrée réduite. Commencer par créer un vecteur d’abscisses à l’aide de la fonction seq(). Tracer ensuite la fonction de densité en ces points. x = seq(-5,5,0.01) plot(x,dnorm(x), type=&quot;l&quot;,col=&quot;red&quot;, lwd=2, main=&quot;densité de N(0,1)&quot;) # curve(dnorm(x), -5,5) # perso je préfère la fonction curve Il existe plusieurs paramètres réglables pour avoir des courbes de différents design. 5. Tracer un histogramme d’un vecteur de 10000 réalisations indépendantes d’une loi normale centrée réduite. Superposer l’histogramme avec la densité réelle de cette loi. hist(rnorm(10000),col=mycolor, prob=T, xlab=&quot;&quot;,main= &quot;1000 réalisations de la loi N(0,1)&quot;) curve(dnorm(x),xlim=c(-5,5),add=T, col=&quot;red&quot;,lwd=3) # ou plot comme la question précédente Fonction de répartition Pour calculer la fonction de répartition d’une certaine loi on utilise le nom de la loi dans avec le préfixe p. 6. Que vaut la fonction de répartition de la loi normale centrée réduite en 0? pnorm(0) #ans&gt; [1] 0.5 7. Tracer la fonction de répartition de la loi normale centrée réduite. x = seq(-5,5,0.01) plot(x,pnorm(x), type=&quot;l&quot;,col=&quot;red&quot;, lwd=2, main=&quot;Fonction de répartition de N(0,1)&quot;) # curve(pnorm(x), -5,5) On peut également tracer des fonctions de répartitions empiriques (calculées sur un échantillon). 8. Créer un vecteur de 100 réalisations indépendates de la loi de votre choix. Utiliser la fonction ecdf() pour construire la fonction de répartition empirique de votre vecteur. Puis superposer avec la fonction de répartition théorique. Illustrations de théorèmes limites Loi forte de grands nombres On considère une variable \\(X\\) à valeurs dans \\(\\{0,1,3\\}\\), distribuée comme suit: \\(P(X=0)=0.5, P(X=1)=0.25, P(X=3)=0.25\\). 9. Proposer une façon de simuler \\(X\\). (Suggestion: utiliser la fonction sample()). # réaliser un tirage de cette loi sample(c(0,1,3), size=1 , prob=c(0.5,0.25,0.25)) #ans&gt; [1] 3 10. On considère \\(X_1,X_2,\\ldots\\) une suite infinie de variables indépendantes de même loi que \\(X\\). Soit \\(\\overline{X}_n\\) la moyenne empirique des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,n\\}\\). 10.1 Simuler la suite des \\(X_i\\) pour \\(i \\in \\{1,\\ldots,10000\\}\\). # simulations simulations = sample(c(0,1,3), size=10000 , replace=T, prob=c(0.5,0.25,0.25)) 10.2 Produire un graphique représentant l’évolution de \\(\\overline{X}_n\\) pour \\(n\\) variant de 1 à 10000. (Indication: penser à utiliser la fonction cumsum()). x_n_cum= cumsum(simulations)/(1:10000) plot(1:10000, x_n_cum, ylim=c(0,3),type=&quot;l&quot;, lwd=1.5, col=mycolor, ylab=expression(bar(X)[n]), main=expression(paste(&quot;Evolution de &quot;,bar(X)[n]))) 10.3 Que constate-on? Pouvait-on s’y attendre? # Convergence vers la moyenne théorique qui est 1. Illustration de loi de Grands Nombres Théorème central limite On désire maintenant approfondir comment \\(\\overline{X}_{500}\\) varie autour de sa valeur moyenne. 11. Proposer une transformation affine de \\(\\overline{X}_{500}\\), de la forme \\(Y=a \\times (\\overline{X}_{500} + b)\\), qui suive approximativement la loi \\(\\mathcal{N}(0,1)\\). Correction: \\(\\overline{X}_{500} \\thicksim \\mathcal{N}(m, \\frac{\\sigma^2}{n}) = \\mathcal{N}(1, \\frac{1.5}{500})\\) Donc \\(Y = \\sqrt{\\frac{500}{1.5}}(\\overline{X}_{500} -1) \\thicksim \\mathcal{N}(0,1)\\) 11.1 Simuler avec \\(10000\\) réalisations indépendantes de la variable \\(Y\\). Nous les noterons \\(Y_1, \\ldots, Y_{10000}\\). y=rep(0,10000) for(i in 1:10000){ x = sample(c(0,1,3), size= 500, prob=c(0.5,0.25,0.25), replace = T) y[i] = sqrt(500/1.5) * ( mean(x) - 1 ) } 11.2 Confirmer l’approximation gaussienne en réalisant un histogramme des valeurs prises par les \\(Y_j\\) pour \\(j \\in \\{1,\\ldots,10000\\}\\), sans oublier de tracer la densité gaussienne correctement renormalisée. Commenter l’écart entre l’histogramme et la densité gaussienne. hist(y, col=mycolor, prob=T) curve(dnorm(x),xlim=c(-5,5),add=T, col=&quot;red&quot;,lwd=3) Bonus Illustration du théorème de Moivre Laplace Pour \\((n=10, p=0.5),\\) puis \\((n=50, p=0.9),\\) puis \\((n=100, p=0.1)\\) Simuler un échantillon \\(X\\) de la loi binomiale de paramètres \\(n\\) et \\(p .\\) Calculer l’échantillon centré-réduit \\(Xcr=\\frac{x-n p}{\\sqrt{n p(1-p)}}\\). Représenter un histogramme des valeurs de \\(X c r .\\) Superposer sur le même graphique la densité de la loi normale \\(\\mathcal{N}(0,1) .\\) Représenter la fonction de répartition empirique de \\(X c r .\\) Superposer sur le même graphique la fonction de répartition de la loi normale \\(N(0,1)\\). "],
["estimation-ponctuelle.html", "Chapitre 3 Estimation ponctuelle 3.1 Introduction 3.2 Méthodes d’estimation 3.3 La méthode des moments 3.4 La méthode du maximum de vraisemblance 3.5 Qualité d’un estimateur 3.6 Propriétés des estimateurs des moments (EMM) 3.7 Propriétés des estimateurs de maximum de vraisemblance (EMV)", " Chapitre 3 Estimation ponctuelle 3.1 Introduction Dans ce chapitre, on suppose que les données \\(x_1,\\ldots,x_n\\) sont \\(n\\) réalisations indépendantes d’une même variable aléatoire sous-jacente \\(X\\) (variable parente). Il est équivalent de supposer que \\(x_1,\\ldots,x_n\\) sont les réalisations de variables aléatoires \\(X_1,\\ldots,X_n\\) indépendantes et de même loi (i.i.d). Nous adopterons ici la seconde formulation, qui est plus pratique à manipuler. Les techniques de statistique descriptive, comme l’histogramme ou le graphe de probabilités, permettent de faire des hypothèses sur la nature de la loi de probabilité des \\(X_i\\). Des techniques statistiques plus sophistiquées, les tests d’adéquation, permettent de valider ou pas ces hypothèses. On supposera ici que ces techniques ont permis d’adopter une famille de lois de probabilité bien précise (par exemple, loi normale, loi de Poisson, etc.) pour la loi des \\(X_i\\), mais que la valeur du ou des paramètres de cette loi est inconnue. On notera \\(\\theta\\) le paramètre inconnu. Le problème traité dans ce chapitre est celui de l’estimation du paramètre \\(\\theta\\). Comme on l’a déjà dit, il s’agit de donner, au vu des observations \\(x_1,\\ldots,x_n\\), une approximation ou une évaluation de \\(\\theta\\) que l’on espère la plus proche possible de la vraie valeur inconnue. On pourra proposer une unique valeur vraisemblable pour \\(\\theta\\) (estimation ponctuelle, dans ce chapitre) ou un ensemble de valeurs vraisemblables (estimation ensembliste ou région (intervalle) de confiance, dans le chapitre suivant). On notera \\(F(x;\\theta)\\) la fonction de répartition des \\(X_i\\). Pour les variables aléatoires discrètes on notera \\(P(X = x;\\theta)\\) les probabilités élémentaires, et pour les variables aléatoires continues on notera \\(f(x;\\theta)\\) la densité. Par exemple, quand \\(X\\) est de loi exponentielle \\(\\mathcal{E}(\\lambda)\\), on aura \\(F(x;\\lambda) = 1 − e^{−\\lambda x}\\) et \\(f(x;\\lambda) = \\lambda e^{−\\lambda x}\\). L’estimation du paramètre \\(\\theta\\) s’agit de donner, au vu des observations \\(x_1,\\ldots,x_n\\), une approximation ou une évaluation de \\(\\theta\\) que l’on espère la plus proche possible de la vraie valeur inconnue. 3.2 Méthodes d’estimation Il existe de nombreuses méthodes pour estimer un paramètre \\(\\theta\\). Dans cette section, nous ne nous intéressons qu’aux deux méthodes d’estimation les plus usuelles, la méthode des moments et la méthode du maximum de vraisemblance. Mais il faut d’abord définir précisément ce que sont une estimation et surtout un estimateur. Pour estimer \\(\\theta\\) on ne dispose que des données \\(x_1,\\ldots,x_n\\), donc une estimation de \\(\\theta\\) sera une fonction de ces observations. Définition 3.1 (Définition d’une statistique) Une statistique \\(t\\) est une fonction des observations \\(x_1,\\ldots,x_n\\) : \\[\\begin{align} t: \\, &amp; \\mathbb{R}^n \\rightarrow \\mathbb{R}^m \\\\ &amp; (x_1,\\ldots,x_n) \\rightarrow t(x_1,\\ldots,x_n) \\end{align}\\] Par exemple, \\(\\overline{x}_n = \\frac{1}{n} \\sum_{i=1}^n x_i, \\,\\, x_1^2 \\,\\, \\text{ou} \\,\\, (x_1,x_3+x_4,2 \\ln x_6)\\) sont des statistiques. Puisque les observations \\(x_1,\\ldots,x_n\\) sont des réalisations des variables aléatoires \\(X_1,\\ldots,X_n\\), la quantité calculable à partir des observations \\(t(x_1,\\ldots,x_n)\\) est une réalisation de la variable aléatoire \\(t(X_1,\\ldots,X_n)\\). Et on retrouve par exemple le fait que \\(\\overline{x}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\) est une réalisation de \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Pour simplifier les écritures, on note souvent \\(t_n = t(x_1,\\ldots,x_n)\\) et \\(T_n = t(X_1,\\ldots,X_n)\\). Par abus, on donne le même nom de statistique aux deux quantités, mais dans une perspective d’estimation, on va nommer différemment \\(t_n\\) et \\(T_n\\). Définition 3.2 (Définition d’un estimateur) Un estimateur d’une grandeur \\(\\theta\\) est une statistique \\(T_n\\) à valeurs dans l’ensemble des valeurs possibles de \\(\\theta\\). Une estimation de \\(\\theta\\) est une réalisation \\(t_n\\) de l’estimateur \\(T_n\\). Un estimateur est donc une variable aléatoire, alors qu’une estimation est une valeur déterministe. Dans l’exemple des ampoules dans l’introduction (ici), l’estimateur de \\(\\lambda\\) est \\(1/\\overline{X}_n\\) et l’estimation de \\(\\lambda\\) est \\(0.012\\).3 Un estimateur est une variable aléatoire, alors qu’une estimation est une valeur déterministe. 3.3 La méthode des moments 3.3.1 L’estimateur des moments (EMM) C’est la méthode la plus naturelle. L’idée de base est d’estimer une espérance mathématique par une moyenne empirique, une variance par une variance empirique, etc… Si le paramètre à estimer est l’espérance de la loi des \\(X_i\\), alors on peut l’estimer par la moyenne empirique de l’échantillon. Autrement dit, si \\(\\theta = E(X)\\), alors l’estimateur de \\(\\theta\\) par la méthode des moments (EMM) est \\(\\hat{\\theta}_n=\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Plus généralement, pour \\(\\theta \\in \\mathbb{R}\\), si \\(E(X) = \\phi(\\theta)\\), où \\(\\phi\\) est une fonction inversible, alors l’estimateur de \\(\\theta\\) par la méthode des moments est \\(\\hat{\\theta}_n = \\phi^{-1} (\\overline{X}_n)\\). De la même manière, on estime la variance de la loi des \\(X_i\\) par la variance empirique de l’échantillon \\(S_n^2= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2\\). 3.3.2 Exemples 3.3.2.1 Exemple 1: loi de Bernoulli Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi de Bernoulli \\(\\mathcal{B}(p)\\), \\(E(X) = p\\). Donc l’estimateur de \\(p\\) par la méthode des moments est \\(\\hat{p}_n = \\overline{X}_n\\). Cet estimateur n’est autre que la proportion de 1 dans l’échantillon. On retrouve donc le principe d’estimation d’une probabilité par une proportion (voir 2.4 et 2.6). 3.3.2.2 Exemple 2: loi exponentielle Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi exponentielle \\(\\mathcal{E}(\\lambda)\\), \\(E(X) = 1/\\lambda\\). Donc l’estimateur de \\(\\lambda\\) par la méthode des moments est \\(\\hat{\\lambda}_n = 1/\\overline{X}_n\\). 3.3.2.3 Exemple 3: loi normale Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi normale \\(\\mathcal{N}(m,\\sigma^2 )\\), \\(E(X) = m\\) et \\(V(X) = \\sigma^2\\), donc les estimateurs de \\(m\\) et \\(\\sigma^2\\) par la méthode des moments sont \\(\\hat{m} = \\overline{X}_n\\) et \\(\\hat{\\sigma}^2=S_n^2\\).4 3.3.2.4 Exemple 4: loi gamma Si \\(X_1,\\ldots,X_n\\) sont indépendantes et de même loi gamma \\(\\Gamma(a,\\lambda)\\), \\(E(X) = a/\\lambda\\) et \\(V(X) = a/\\lambda^2\\). On en déduit facilement que : \\[ \\lambda = \\frac{E(X)}{V(X)} \\quad \\text{et} \\quad a = \\frac{[E(X)]^2}{V(X)}\\] Donc les EMM de \\(a\\) et \\(\\lambda\\) sont: \\[ \\hat{\\lambda} = \\frac{\\overline{X}_n}{S_n^2} \\quad \\text{et} \\quad a = \\frac{\\overline{X}_n^2}{S_n^2}\\] :::rmdinsight L’idée de base de l’estimateur par la méthode des moments est d’estimer une espérance mathématique par une moyenne empirique, une variance par une variance empirique, etc… ::: 3.4 La méthode du maximum de vraisemblance 3.4.1 La fonction de vraisemblance Définition 3.3 Quand les observations sont toutes discrètes ou toutes continues, on appelle fonction de vraisemblance (ou plus simplement vraisemblance) pour l’échantillon \\(x_1,\\ldots,x_n\\), la fonction du paramètre \\(\\theta\\) : \\[\\begin{equation*} \\mathcal{L}(\\theta; x_1,\\ldots,x_n) = \\left\\lbrace \\begin{array}{ll} P(X_1=x_1,\\ldots,X_n=x_n; \\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont discrètes}\\\\ f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n;\\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont continues} \\end{array} \\right. \\end{equation*}\\] Dans tous les exemples que nous traiterons ici, les \\(X_i\\) sont indépendantes et de même loi. Dans ce cas, la fonction de vraisemblance s’écrit: \\[\\begin{equation*} \\mathcal{L}(\\theta; x_1,\\ldots,x_n) = \\left\\lbrace \\begin{array}{ll} \\displaystyle \\prod_{i=1}^n P(X_i=x_i; \\theta) = \\prod_{i=1}^n P(X=x_i; \\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont discrètes}\\\\ \\displaystyle \\prod_{i=1}^n f_{X_i}(x_i;\\theta) = \\prod_{i=1}^n f(x_i;\\theta) &amp; \\text{si les} \\, X_i \\, \\text{sont continues} \\end{array} \\right. \\end{equation*}\\] Remarque: La probabilité et la densité utilisées dans cette définition sont des fonctions des observations \\(x_1,\\ldots,x_n\\), dépendant du paramètre \\(\\theta\\). A l’inverse, la fonction de vraisemblance est considérée comme une fonction de \\(\\theta\\) dépendant des observations \\(x_1,\\ldots,x_n\\), ce qui permet, par exemple, de dériver cette fonction par rapport à \\(\\theta\\). 3.4.1.1 Exemple introductif Dans cet exemple, \\(n = 1\\). On considère que l’on sait que \\(X_1\\) est de loi binomiale \\(\\mathcal{B}(15,p)\\), avec \\(p\\) inconnu. On observe \\(x_1 = 5\\) et on cherche à estimer \\(p\\). La fonction de vraisemblance est : \\[\\mathcal{L}(p;5) = P(X_1 = 5;p) = C_{15}^5 p^5 (1-p)^{15-5}\\] C’est la probabilité d’avoir observé un 5 quand la valeur du paramètre est \\(p\\). Calculons-là pour quelques valeurs de \\(p\\). \\(p\\) \\(0.1\\) \\(0.2\\) \\(0.3\\) \\(0.4\\) \\(0.5\\) \\(0.6\\) \\(0.7\\) \\(0.8\\) \\(0.9\\) \\(\\mathcal{L}(p;5)\\) \\(0.01\\) \\(0.10\\) \\(0.21\\) \\(0.19\\) \\(0.09\\) \\(0.02\\) \\(0.003\\) \\(10^{-4}\\) \\(210^{-7}\\) On tire de cette table que quand \\(p = 0.8\\), c’est-à-dire quand \\(X_1\\) est de loi \\(\\mathcal{B}(15,0.8)\\), il n’y a qu’une chance sur \\(10000\\) d’observer \\(x_1 = 5\\). En revanche, il y a \\(21\\%\\) de chances d’observer un \\(5\\) quand \\(p = 0.3\\). Il est donc beaucoup plus vraisemblable que \\(p\\) soit égal à \\(0.3\\) plutôt qu’à \\(0.8\\). En suivant ce raisonnement, on aboutit à dire que la valeur la plus vraisemblable de \\(p\\) est celle pour laquelle la probabilité d’observer un \\(5\\) est maximale. C’est donc la valeur de \\(p\\) qui maximise la fonction de vraisemblance. Pour la calculer, on peut annuler la dérivée de la vraisemblance (en fonction de \\(p\\)). Mais on remarque que la vraisemblance est un produit. Comme il est plus commode de maximiser (ou de dériver) une somme qu’un produit, on utilise le fait que la valeur qui rend maximale une fonction rend aussi maximal son logarithme. On va donc plutôt maximiser le logarithme de la fonction de vraisemblance, qu’on appelle la log-vraisemblance. Pour notre exemple, la log-vraisemblance vaut: \\[\\ln \\mathcal{L}(p;x_1)=\\ln C_{15}^{x_1} + x_1 \\ln p + (15-x_1) \\ln (1-p)\\] Sa dérivée est: \\[ \\frac{\\partial }{\\partial p } \\ln \\mathcal{L}(p;x_1)= \\frac{x_1}{p} - \\frac{15-x_1}{1-p} = \\frac{x_1 - 15 p}{p(1-p)} \\] qui s’annule pour \\(p = \\frac{x_1}{15} = \\frac{5}{15} = \\frac{1}{3}\\). Donc la valeur la plus vraisemblable de \\(p\\) est \\(\\frac{1}{3}\\). La vraisemblance maximale est \\(\\mathcal{L}(\\frac{1}{3};5) = 21.4\\%\\). La valeur qui rend maximale une fonction rend aussi maximal son logarithme. 3.4.2 L’estimateur de maximum de vraisemblance (EMV) En suivant le raisonnement précédent, pour \\(n\\) quelconque, il est logique de dire que la valeur la plus vraisemblable de \\(\\theta\\) est la valeur pour laquelle la probabilité d’observer \\(x_1 ,\\ldots,x_n\\) est la plus forte possible. Cela revient à faire comme si c’était l’éventualité la plus probable qui s’était produite au cours de l’expérience. Définition 3.4 L’estimation de maximum de vraisemblance de \\(\\theta\\) est la valeur \\(\\hat{\\theta}_n\\) de \\(\\theta\\) qui rend maximale la fonction de vraisemblance \\(\\mathcal{L}(\\theta;x_1 ,\\ldots,x_n)\\). L’estimateur de maximum de vraisemblance (EMV) de \\(\\theta\\) est la variable aléatoire correspondante. Comme dans l’exemple, dans la plupart des cas, la fonction de vraisemblance s’exprime comme un produit. Donc \\(\\hat{\\theta}_n\\) sera en général calculé en maximisant la log-vraisemblance: \\[ \\hat{\\theta}_{n}=\\arg \\max _{\\theta} \\,\\, \\ln \\mathcal{L}\\left(\\theta ; x_{1}, \\ldots, x_{n}\\right) \\] Quand \\(\\theta = (\\theta_1 ,\\ldots,\\theta_d ) \\in \\mathbb{R}^d\\) et que toutes les dérivées partielles ci-dessous existent, \\(\\hat{\\theta}_{n}\\) est solution du système d’équations appelées équations de vraisemblance: \\[ \\forall j \\in\\{1, \\ldots, d\\}, \\quad \\frac{\\partial}{\\partial \\theta_{j}} \\ln \\mathcal{L}\\left(\\theta ; x_{1}, \\ldots, x_{n}\\right)=0 \\] A priori, une solution de ce système d’équations pourrait être un minimum de la vraisemblance. Mais on peut montrer que la nature d’une fonction de vraisemblance fait que c’est bien un maximum que l’on obtient. Il est fréquent que le système des équations de vraisemblance n’ait pas de solution explicite. Dans ce cas, on le résoud par des méthodes numériques, comme la méthode de Newton-Raphson (lien 1 , lien 2 ). En , la maximisation numérique peut se faire à l’aide de la commande optim(). 3.4.2.1 Exemples 3.4.2.1.1 Exemple 1: loi de Bernoulli Soit les \\(X_i\\), sont de loi \\(\\mathcal{B}(p)\\), on a: \\[P\\left(X_{i}=x_{i} ; p\\right)=\\left\\{\\begin{array}{cc}{p} &amp; {\\text { si } x_{i}=1} \\\\ {1-p} &amp; {\\text { si } x_{i}=0}\\end{array}\\quad \\right\\} = p^{x_{i}}(1-p)^{1-x_{i}}\\] Donc la fonction de vraisemblance est: \\[\\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} P\\left(X_{i}=x_{i} ; p\\right)=\\prod_{i=1}^{n} p^{x_{i}}(1-p)^{1-x_{i}}=p^{\\sum_{i=1}^{n} x_{i}}(1-p)^{\\sum_{i=1}^{n}\\left(1-x_{i}\\right)}\\] D’où \\(\\ln \\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\left(\\sum_{i=1}^{n} x_{i}\\right) \\ln p+\\left(n-\\sum_{i=1}^{n} x_{i}\\right) \\ln (1-p)\\). Alors \\[\\frac{\\partial}{\\partial p} \\ln \\mathcal{L}\\left(p ; x_{1}, \\ldots, x_{n}\\right)=\\frac{\\sum_{i=1}^{n} x_{i}}{p}-\\frac{n-\\sum_{i=1}^{n} x_{i}}{1-p}=\\frac{\\sum_{i=1}^{n} x_{i}-n p}{p(1-p)}\\] qui s’annule pour \\(p=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}=\\overline{x}_{n}\\). Par conséquent, l’EMV de \\(p\\) est \\(\\hat{p}_n= \\overline{X}_n\\). L’EMV de \\(p\\) est \\(\\hat{p}_n= \\overline{X}_n\\). Le même que l’EMM de \\(p\\). 3.4.2.1.2 Exemple 2: loi exponentielle Si les \\(X_i\\) sont de loi \\(\\mathcal{E}(\\lambda)\\), la fonction de vraisemblance est: \\[\\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} f_{X_{i}}\\left(x_{i} ; \\lambda\\right)=\\prod_{i=1}^{n} \\lambda e^{-\\lambda x_{i}}=\\lambda^{n} e^{-\\lambda \\sum_{i=1}^{n} x_{i}}\\] D’où \\(\\ln \\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=n \\ln \\lambda-\\lambda \\sum_{i=1}^{n} x_{i}\\). Alors \\(\\frac{\\partial}{\\partial \\lambda} \\ln \\mathcal{L}\\left(\\lambda ; x_{1}, \\ldots, x_{n}\\right)=\\frac{n}{\\lambda}-\\sum_{i=1}^{n} x_{i}\\), qui s’annule pour \\(\\lambda=\\frac{n}{\\sum_{i=1}^{n} x_{i}}=\\frac{1}{\\overline{x}_{n}}\\). Par conséquent, l’EMV de \\(\\lambda\\) est \\(\\hat{\\lambda}_n = \\frac{1}{\\overline{X}_n}\\). 3.4.2.1.3 Exemple 3: loi normale Si les \\(X_i\\) sont de loi \\(\\mathcal{N}(m,\\sigma^2)\\), la fonction de vraisemblance est: \\[\\begin{aligned} \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right) &amp;=\\prod_{i=1}^{n} f_{X_{i}}\\left(x_{i} ; m, \\sigma^{2}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{\\left(x_{i}-m\\right)^{2}}{2 \\sigma^{2}}} \\\\ &amp;=\\frac{1}{(\\sigma \\sqrt{2 \\pi})^{n}} e^{-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}} \\end{aligned}\\] D’où \\(\\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{n}{2} \\ln \\sigma^{2}-\\frac{n}{2} \\ln 2 \\pi-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\). On doit annuler les dérivées partielles de ce logarithme par rapport à \\(m\\) et \\(\\sigma^2\\). On a: \\(\\frac{\\partial}{\\partial m} \\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}-2\\left(x_{i}-m\\right)=\\frac{1}{\\sigma^{2}}\\left(\\sum_{i=1}^{n} x_{i}-n m\\right)\\), qui s’annule pour \\(m=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}=\\overline{x}_{n}\\). \\(\\frac{\\partial}{\\partial \\sigma^{2}} \\ln \\mathcal{L}\\left(m, \\sigma^{2} ; x_{1}, \\ldots, x_{n}\\right)=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\), qui s’annule pour \\(\\sigma^2=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-m\\right)^{2}\\). \\(\\hat{m}_n\\) et \\(\\hat{\\sigma}_n^2\\) sont les valeurs de \\(m\\) et \\(\\sigma^2\\) qui vérifient les deux conditions en même temps. On a donc \\(\\hat{m}_{n}=\\overline{X}_{n}\\) et \\(\\hat{\\sigma}_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=S_{n}^{2}\\). Remarque: Dans ces les trois exemples, la méthode des moments et la méthode du maximum de vraisemblance donnent les mêmes résultats. Ce n’est le cas que pour quelques lois de probabilité parmi les plus élémentaires. En fait, dans la plupart des cas, les deux méthodes fournissent des estimateurs différents. C’est le cas de la loi gamma. Cela amène à se poser la question de la qualité et de l’optimalité d’un estimateur, ce qui fait l’objet de la section suivante. 3.5 Qualité d’un estimateur En toute généralité, \\(\\theta\\) peut être un paramètre à plusieurs dimensions, mais on supposera dans toute cette section et dans la suivante que \\(\\theta\\) est un réel. Cela signifie par exemple que, quand \\(X\\) est de loi normale \\(\\mathcal{N}(m,\\sigma^2)\\), on s’intéressera séparément à la qualité des estimateurs de \\(m\\) et de \\(\\sigma^2\\). Les estimateurs \\(T_n\\) considérés ici seront donc des variables aléatoires réelles. Pour \\(\\theta \\in \\mathbb{R}^d, d \\geq 2\\), toutes les notions de ces sections sont généralisables, mais la complexité des résultats augmente notablement. Par exemple, la notion de variance est remplacée par celle de matrice de covariance. 3.5.1 Estimateur sans biais et de variance minimale (ESBVM) Un estimateur \\(T_n\\) de \\(\\theta\\) sera un bon estimateur s’il est suffisamment proche, en un certain sens, de \\(\\theta\\). Il faut donc définir une mesure de l’écart entre \\(\\theta\\) et \\(T_n\\). On appelle cette mesure le risque de l’estimateur. On a intérêt à ce que le risque d’un estimateur soit le plus petit possible. Par exemple, les risques \\(T_n - \\theta,\\,\\, |T_n - \\theta|\\,\\, \\text{et} \\,\\, (T_n - \\theta)^2\\) expriment bien un écart entre \\(T_n\\) et \\(\\theta\\). Mais comme il est plus facile d’utiliser des quantités déterministes que des quantités aléatoires, on s’intéresse en priorité aux espérances des quantités précédentes. En particulier: Définition 3.5 (biais) On appelle le biais de \\(T_n\\) la quantité \\(E(T_n)-\\theta\\). Définition 3.6 (risque quadratique) On appelle le risque quadratique ou erreur quadratique moyenne: \\[ EQM(T_n)=E[(T_n-\\theta)^2]\\] Dans le cas du biais, le risque peut être nul: Définition 3.7 (estimateur sans biais) Un estimateur \\(T_n\\) de \\(\\theta\\) est sans biais si et seulement si \\(E(T_n) = \\theta\\). Il est biaisé si et seulement si \\(E(T_n) \\neq \\theta\\). Le biais mesure une erreur systématique d’estimation de \\(\\theta\\) par \\(T_n\\). Par exemple, si \\(E(T_n)-\\theta &lt; 0\\), cela signifie que \\(T_n\\) aura tendance à sous-estimer \\(\\theta\\). D’autre part, l’erreur quadratique moyenne (nommée aussi risque quadratique) s’écrit: \\[\\begin{aligned} E Q M\\left(T_{n}\\right) &amp; = E\\left[\\left(T_{n}-\\theta\\right)^{2}\\right]=E\\left[\\left(T_{n}-E\\left(T_{n}\\right)+E\\left(T_{n}\\right)-\\theta\\right)^{2}\\right] \\\\ &amp; = E\\left[\\left(T_{n}-E\\left(T_{n}\\right)\\right)^{2}\\right]+2 E\\left[T_{n}-E\\left(T_{n}\\right)\\right] E\\left[E\\left(T_{n}\\right)-\\theta\\right]+E\\left[\\left(E\\left(T_{n}\\right)-\\theta\\right)^{2}\\right] \\\\ &amp; = \\operatorname{Var}\\left(T_{n}\\right)+\\left[E\\left(T_{n}\\right)-\\theta\\right]^{2} \\\\ &amp; = \\text { Variance de l&#39;estimateur }+\\text { carré de son biais } \\end{aligned}\\] Si \\(T_n\\) est un estimateur sans biais, \\(EQM(T_n ) = Var(T_n )\\). On a donc intérêt à ce qu’un estimateur soit sans biais et de faible variance. Par ailleurs, on en déduit immédiatement que de deux estimateurs sans biais, le meilleur est celui qui a la plus petite variance. On a intérêt à ce qu’un estimateur soit sans biais et de faible variance. La variance d’un estimateur mesure sa variabilité. Si l’estimateur est sans biais, cette variabilité est autour de \\(\\theta\\). Si on veut estimer correctement \\(\\theta\\), il ne faut pas que cette variabilité soit trop forte. En pratique, si on observe plusieurs jeux de données similaires, on obtient une estimation de \\(\\theta\\) pour chacun d’entre eux. Alors si l’estimateur est de faible variance, ces estimations seront toutes proches les unes des autres, et s’il est sans biais leur moyenne sera très proche de \\(\\theta\\). Il est logique de s’attendre à ce que, plus la taille des données augmente, plus on a d’information sur le phénomène aléatoire observé, donc meilleure sera l’estimation. En théorie, avec une observation infinie, on devrait pouvoir estimer \\(\\theta\\) sans aucune erreur. On peut traduire cette affirmation par le fait que le risque de l’estimateur \\(T_n\\) doit tendre vers \\(0\\) quand la taille \\(n\\) de l’échantillon tend vers l’infini. Cela revient à dire que l’estimateur \\(T_n\\) doit converger, en un certain sens, vers \\(\\theta\\). Il s’agit en fait d’étudier la convergence de la suite de variables aléatoires \\(\\{T_n\\}_{n \\geq 1}\\) vers la constante \\(\\theta\\). On sait qu’il existe plusieurs types de convergence de suites de variables aléatoires. On peut étudier la convergence presque sûre ou la convergence en probabilité, mais on s’intéresse en général à la convergence en moyenne quadratique (ou convergence dans \\(L^2\\)). Définition 3.8 L’estimateur \\(T_n\\) converge en moyenne quadratique vers \\(\\theta\\) si et seulement si son erreur quadratique moyenne tend vers \\(0\\) quand \\(n\\) tend vers l’infini: \\[T_{n} \\stackrel{M Q}{\\longrightarrow} \\theta \\Leftrightarrow \\lim _{n \\rightarrow \\infty} E\\left[\\left(T_{n}-\\theta\\right)^{2}\\right]=0\\] Si \\(T_n\\) est sans biais, il sera convergent en moyenne quadratique si et seulement si sa variance tend vers \\(0\\) quand \\(n\\) tend vers l’infini. Finalement, on considèrera que le meilleur estimateur possible de \\(\\theta\\) est un estimateur sans biais et de variance minimale (ESBVM). Un tel estimateur n’existe pas forcément. 3.6 Propriétés des estimateurs des moments (EMM) Propriétés de \\(\\overline{X}_n\\) Si \\(\\theta = E(X)\\), alors l’EMM de \\(\\theta\\) est \\(\\hat{\\theta}_n = \\overline{X}_n\\). La justification de cette méthode est la loi des grands nombres, qui dit que \\(\\overline{X}_n\\) converge presque sûrement vers \\(E(X)\\). Donc, si \\(\\theta = E(X)\\), \\(\\overline{X}_n\\) est un estimateur de \\(\\theta\\) convergent presque sûrement. Autrement dit, si on a beaucoup d’observations, on peut estimer une espérance par une moyenne empirique. On peut en fait montrer facilement que \\(\\overline{X}_n\\) est un bon estimateur de \\(\\theta = E(X)\\), sans utiliser la loi des grands nombres: \\[ E\\left(\\overline{X}_{n}\\right)=E\\big[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\big]=\\frac{1}{n} \\sum_{i=1}^{n} E\\left(X_{i}\\right)=\\frac{1}{n} n \\theta=\\theta \\] Donc \\(\\overline{X}_{n}\\) est un estimateur sans biais de \\(\\theta = E(X)\\). La variance de \\(\\overline{X}_{n}\\) est: \\[ \\operatorname{Var}\\left(\\overline{X}_{n}\\right)=\\operatorname{Var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}\\left(X_{i}\\right)=\\frac{\\operatorname{Var}(X)}{n} \\] car les \\(X_i\\) sont indépendantes, donc la variance de leur somme est égale à la somme de leurs variances, qui sont toutes égales à \\(Var(X)\\). \\(Var(\\overline{X}_{n})\\) tend vers \\(0\\) quand \\(n\\) tend vers l’infini. Par conséquent: Propriété: La moyenne empirique \\(\\overline{X}_{n}\\) est un estimateur sans biais et convergent en moyenne quadratique de \\(E(X)\\). Propriétés de la variance empirique \\(S_{n}^{2}\\) On considère maintenant l’estimation de la variance de la loi des \\(X_i\\) par la variance empirique de l’échantillon \\(S_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}-\\overline{X}_{n}^{2}\\). Déterminons le biais de cet estimateur. \\[ \\begin{aligned} E\\left(S_{n}^{2}\\right) &amp;=E\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}-\\overline{X}_{n}^{2}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} E\\left(X_{i}^{2}\\right)-E\\left(\\overline{X}_{n}^{2}\\right)=E\\left(X^{2}\\right)-E\\left(\\overline{X}_{n}^{2}\\right) \\\\ &amp;=\\operatorname{Var}(X)+E(X)^{2}-\\operatorname{Var}\\left(\\overline{X}_{n}\\right)-E\\left(\\overline{X}_{n}\\right)^{2} \\\\ &amp;=\\operatorname{Var}(X)+E(X)^{2}-\\frac{\\operatorname{Var}(X)}{n}-E(X)^{2}=\\left(1-\\frac{1}{n}\\right) \\operatorname{Var}(X) \\\\ &amp;=\\frac{n-1}{n} \\operatorname{Var}(X) \\neq \\operatorname{Var}(X) \\end{aligned} \\] Donc contrairement à ce qu’on pourrait croire, la variance empirique \\(S_n^2\\) n’est pas un estimateur sans biais de \\(Var(X)\\). Cet estimateur n’est qu’asymptotiquement sans biais. En revanche, on voit que \\(E\\left(\\frac{n}{n-1} S_{n}^{2}\\right)=\\frac{n}{n-1} E\\left(S_{n}^{2}\\right)=\\operatorname{Var}(X)\\). On pose donc \\({S_n^{*}}^2=\\frac{n}{n-1} S_{n}^{2}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\big(X_{i}-\\overline{X}_{n}\\big)^{2}\\). \\({S_n^{*}}^2\\) est appelée variance estimée de l’échantillon. Le résultat précédent montre que c’est un estimateur sans biais de \\(\\operatorname{Var}(X)\\). Par ailleurs, on montre que \\[ \\operatorname{Var}\\left({S_n^{*}}^2\\right)=\\frac{1}{n(n-1)}\\left[(n-1) E\\left[(X-E(X))^{4}\\right]-(n-3) \\operatorname{Var}(X)^{2}\\right] \\] qui tend vers \\(0\\) quand \\(n\\) tend vers l’infini. Par conséquent: Propriété: La variance estimée \\({S_n^{*}}^2=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}\\) est un estimateur sans biais et convergent en moyenne quadratique de \\(Var(X)\\). La commande var(x) en donne la variance estimée, et non pas la variance empirique de l’échantillon x. On peut montrer également que \\({S_n^{*}}^2\\) et \\(S_{n}^{2}\\) convergent toutes les deux presque sûrement vers \\(Var(X)\\). Remarque 1: On n’a pas de résultat général sur la qualité de \\(S_{n}\\) comme estimateur de l’écart-type de la loi, \\(\\sigma(X)=\\sqrt{\\operatorname{Var}(X)}\\). A priori, ni \\(S_{n}\\) ni \\(S_{n}^{*}\\) ne sont des estimateurs sans biais de \\(\\sigma(X)\\). Remarque 2: Le simple exemple de la variance montre qu’un estimateur des moments n’est pas forcément sans biais. On peut montrer qu’un EMM est asymptotiquement sans biais et convergent presque sûrement. 3.7 Propriétés des estimateurs de maximum de vraisemblance (EMV) Un estimateur de maximum de vraisemblance n’est pas forcément unique (la vraisemblance peut avoir plusieurs maxima), ni sans biais, ni de variance minimale, ni efficace. Mais il possède d’excellentes propriétés asymptotiques (non évoqués dans ce cours). Le fait que l’EMV soit asymptotiquement sans biais et efficace fait que, si on a beaucoup de données, on est pratiquement certains que la méthode du maximum de vraisemblance est la meilleure méthode d’estimation possible. C’est pourquoi cette méthode est considérée comme globalement la meilleure et est utilisée de préference à toute autre méthode, y compris celle des moments. car \\(E(X)=1/\\lambda\\) si \\(X \\thicksim \\mathcal{E}(\\lambda)\\)↩ \\(S_n^2\\) est la variance empirique de l’échantillon \\(S_n^2= \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - \\overline{X}_n^2\\)↩ "],
["exercices-2.html", "Exercices", " Exercices Exercice 3.1 (La variance corrigée) Soit \\(X\\) une variable aléatoire ayant une espérance \\(m\\) et une variance \\(\\sigma^2\\), sa variance empirique est \\(s_{n}^2=\\frac{1}{n} \\sum X_{i}^{2}-\\overline{X}_{n}^{2}\\) avec \\(\\overline{X}_{n}\\) la moyenne empirique de \\(X\\) et \\(\\frac{1}{n} \\sum X_{i}^{2}\\) la moyenne empirique de \\(X^{2}\\). Calculer \\(E\\left(\\overline{X}_{n}\\right)\\) et \\(V\\left(\\overline{X}_{n}\\right)\\) et en déduire \\(E(\\overline{X}_{n}^{2})\\). Montrer enfin que \\(E\\left(s_{n}^2\\right)=\\frac{n-1}{n} V(X)\\) et en déduire un estimateur sans biais de la variance (on nomme cet estimateur \\(s_{n}^{*^2}\\)). Exercice 3.2 (EMM) On considère l’échantillon statistique \\[1,0,2,1,1,0,1,0,0\\] Cacluler sa moyenne et sa variance empirique. En supposant que les données de cet échantillon sont des réalisations d’une variable de loi inconnue, donner une estimation non biaisée de l’espérance et de la variance de cette loi. On choisit de modéliser les valeurs de cet échantillon par une loi binomiale \\(\\mathcal{B}(2, p)\\). Utiliser la moyenne empirique pour proposer une estimation ponctuelle pour \\(p\\). Avec le même modèle, utiliser la variance empirique pour proposer une autre estimation de \\(p\\). On choisit de modéliser les valeurs de cet échantillon par une loi de Poisson \\(\\mathcal{P}(\\lambda)\\), qui a pour espérance \\(\\lambda\\). Quelle estimation ponctuelle proposez-vous pour \\(\\lambda\\)? Exercice 3.3 (EMV loi normale) Considérons un échantillon aléatoire \\((X_1,\\ldots,X_n)\\) (les \\(X_i\\) sont iid) et issu d’une variable alétoire parente \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\). Calculer les estimateurs de maximum de vraisemblance (EMV) de \\(\\mu\\) et \\(\\sigma^2\\). Ces estimateurs sont-ils sans biais? Exercice 3.4 (EMV loi géométrique) Les oiseaux d’un certain type prennent leur envol après avoir effectué quelques sauts sur le sol. On suppose que ce nombre \\(X\\) de sauts peut être modélisé par une distribution géométrique sur \\(\\mathbb{N}^*\\): \\[P(X=x)= p(1-p)^{x-1} \\quad x \\geq 1\\] Pour \\(n=130\\) oiseaux de ce type, on a relevé les données suivantes: Nombre de sauts \\(x\\) 1 2 3 4 5 6 7 8 9 10 11 12 Effectifs 48 31 20 9 6 5 4 2 1 1 2 1 Quel est l’estimateur du maximum de vraisemblance de \\(p\\)? Calculer la valeur de cet estimateur avec les données de l’échantillon. Exercice 3.5 (Comparaison d’estimateurs) Soit \\(X\\) une VAR de loi uniforme sur un intervalle \\([0, a]\\) où \\(a\\) est un paramètre inconnu, et on dispose de \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) un \\(n\\) -echantillon de \\(X\\). On note \\(\\overline{X}_{n}\\) la moyenne empirique de \\(X\\). Soit \\(T_{n}=2 \\overline{X}_{n}\\), l’estimateur par la méthode de moments de \\(a\\). Montrer que \\(T_{n}\\) est un estimateur sans biais de \\(a\\) et calculer son risque quadratique. Soit \\(T_{n}^{\\prime}=\\max \\left(X_{1}, \\ldots, X_{n}\\right)\\). Montrer que \\(T_n^{\\prime}\\) est l’estimateur par maximum de vraisemblance de \\(a\\). Donner la fonction de répartition de \\(T_{n}^{\\prime}\\). En déduire une densité de \\(T_{n}^{\\prime}\\), puis son biais et son risque quadratique. Soit \\(T_{n}^{\\prime \\prime}=\\frac{n+1}{n} T_{n}^{\\prime}\\). Déterminer son biais et son risque quadratique. Pour de grandes valeurs de \\(n\\), quel est le meilleur estimateur de \\(a\\)? Extra Exercice 3.6 On considère une variable aléatoire \\(X\\) de densité \\(f_{\\theta}\\) avec \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) et \\[ f_{\\theta}(x)=\\left\\{\\begin{array}{cl}{\\frac{1}{2}-\\theta} &amp; {\\text { si } x \\in[-1,0]} \\\\ {\\frac{1}{2}+\\theta} &amp; {\\text { si } x \\in[0,1]} \\\\ {0} &amp; {\\text { sinon }}\\end{array}\\right. \\] Représenter \\(f_{\\theta}\\) et justifier que \\(f_{\\theta}\\) est bien une densité de probabilité pour \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\). Calculer l’espérance et la variance de \\(X\\). On considère maintenant \\(X_{1}, \\ldots X_{n}\\) des variables aléatoires indépendantes et de même loi. On suppose que la loi commune est de densité \\(f_{\\theta}\\) avec \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) inconnu. On va chercher à estimer \\(\\theta\\). Proposer un esitmateur \\(\\hat{\\theta}\\) de \\(\\theta\\) basé sur la méthode des moments. On prend \\(\\hat{\\theta}_{n}=\\frac{1}{n} \\sum X_{i}\\). Calculer son biais et son risque quadratique. On va maintenant s’intéresser à l’estimateur du maximum de vraisemblance de \\(\\theta\\). On note \\(N_{1}=\\sum_{i=1}^{n} \\mathbf{1}_{\\left\\{X_{i} \\geq 0\\right\\}}\\) et \\(N_{2}=\\sum_{i=1}^{n} \\mathbf{1}_{\\left\\{X_{i}&lt;0\\right\\}}\\). Soient \\(x_{1}, \\ldots, x_{n} \\in [-1,1]\\) et \\(-\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}\\) écrire la vraisemblance du modèle au point \\(\\left(x_{1}, \\ldots, x_{n} ; \\theta\\right)\\) en fonction de \\(N_{1}, N_{2}\\) et \\(\\theta\\). Montrer que l’estimateur du maximum devraisemblance existe et vaut: \\[ \\hat{\\theta}_{M V}=\\frac{N_{1}-N_{2}}{2\\left(N_{1}+N_{2}\\right)}=\\frac{N_{1}}{n}-\\frac{1}{2} \\] On pose \\(1_{\\left\\{X_{i} \\geq 0\\right\\}}\\), pour \\(1 \\leq i \\leq n\\). Calculer l’espérance et la variance des variables aléatoires \\(Y_{i}\\). En déduire l’espérance et le risque quadratique de \\(\\hat{\\theta}_{M V}\\). Quel estimateur vaut-il mieux utiliser entre \\(\\hat{\\theta}\\) et \\(\\hat{\\theta}_{M V}\\)? Justifier. "],
["tp-estimation.html", "TP estimation 3.8 Comparaison d’estimateurs 3.9 Le maximum de vraisemblance 3.10 Loi de Weibull", " TP estimation Il faut finir les TPs de la 1ère et 2ème séances avant de commencer ce TP. 3.8 Comparaison d’estimateurs Soit \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) un échantillon de la loi uniforme sur \\([0, \\theta]\\) où \\(\\theta\\) est un paramètre inconnu. On considère les estimateurs convergents (on dit aussi consistants) suivants du paramètre \\(\\theta\\). \\[ \\begin{aligned} T_{1}=&amp; \\frac{2}{n} \\times\\left(X_{1}+\\ldots+X_{n}\\right) \\\\ T_{2}=&amp; \\sqrt{\\frac{3}{n} \\times\\left(X_{1}^{2}+\\ldots+X_{n}^{2}\\right)} \\\\ T_{3}=&amp;\\left(\\frac{4}{n} \\times\\left(X_{1}^{3}+\\ldots+X_{n}^{3}\\right)\\right)^{\\frac{1}{3}} \\\\ T_{4}=&amp;\\left(\\frac{3}{2 n} \\times(\\sqrt{X_{1}}+\\ldots+\\sqrt{X_{n}})\\right)^{2} \\\\ T_{5}=&amp;\\left(\\frac{1}{2 n} \\times\\left(\\frac{1}{\\sqrt{X_{1}}}+\\ldots+\\frac{1}{\\sqrt{X_{n}}}\\right)\\right)^{-2} \\\\ T_{6}=&amp; \\exp (1) \\times\\left(X_{1} \\times \\ldots \\times X_{n}\\right)^{\\frac{1}{n}} \\\\ T_{7}=&amp; \\max \\left\\{X_{1}, \\ldots, X_{n}\\right\\} \\\\ T_{8}=&amp; \\frac{n+1}{n} \\max \\left\\{X_{1}, \\ldots, X_{n}\\right\\} \\end{aligned} \\] \\(T1\\) est l’emm de \\(\\theta\\). \\(T7\\) est l’emv de \\(\\theta\\) (biaisé). \\(T8\\) est l’emv corrigé de \\(\\theta\\). 1. Choisir une valeur de \\(\\theta\\) et simuler 1000 échantillons de taille 100 de la loi uniforme sur \\([0, \\theta]\\). Calculer pour chacun de ces échantillons la valeur prise par les 8 estimateurs. On pourra ensuite créer une matrice ayant 1000 lignes et 8 colonnes dont la jème colonne contient les 1000 réalisations de l’estimateur \\(T_{j}\\): T &lt;- cbind(T1,T2,T3,T4,T5,T6,T7,T8) 2. Calculer la moyenne empirique et la variance empirique des 8 échantillons de taille 1000 ainsi obtenus. En déduire une estimation du biais et de l’erreur quadratique de chacun des 8 estimateurs. On rappelle que pour un estimateur \\(T\\), le biais est \\(E[T]-\\theta\\) et l’erreur quadratique est \\(E\\left[(T-\\theta)^{2}\\right]\\). 3. Quels estimateus sont les moins biaisés? Et les estimateurs de risque quadratique minimale? 4. Représenter sur un même graphique les boites à moustaches (boxplots) des 8 estimateurs. Superposer sur le même graphique la vraie valeur du paramètre, en rouge. Commenter la pertinence de chacun des estimateurs: lequel préfereriez-vous utiliser? On pourra utiliser boxplot(data.frame(T)) abline(h=theta,col=&quot;red&quot;) 3.9 Le maximum de vraisemblance Pour cette partie, imaginons qu’on a une série de valeurs, ça peut par exemple être l’âge de 1000 étudiants pris au hasard dans une ville. Nous avons tracé l’histogramme de l’échantillon et obtenu le suivant: On peut voir ici que la distribution des valeurs suit approximativement une loi normale avec une moyenne aux alentours de 22 et un écart-type difficile à évaluer au premier coup d’oeil. En effet, pour obtenir les données de cet échantillon, j’ai généré un échantillon suivant la loi normale de moyenne 22 et d’écart-type mystère que vous devez découvrir en utilisant la méthode du maximum de vraisemblance. Les données ont été générée avec la commande suivante: data = rnorm(1000, mean = 22, sd = mystere) # où j&#39;ai caché ici la valeur mystère de l&#39;écart-type utilisé. Les données de l’échantillon obtenu se trouvent dans le fichier mystere.txt. Pour lire les données de l’échantillon du fichier mystere.txt, vous pouvez les copier et les saisir dans un vecteur en utilisant la fonction c() ou bien utiliser la fonction scan(). Par exemple: donnees &lt;- scan(\"mystere.txt\", sep=\",\") Sur l’intervalle \\([1,4]\\), chercher la valeur de \\(\\sigma\\) qui maximise la vraisemblance. On la notera \\(\\hat{\\sigma}\\). On procédera de façon empirique en représentant le graphe de la Log-vraisemblance sur l’intervalle \\([1,4]\\). On se contentera d’une valeur approchée de \\(\\hat{\\sigma}\\) à \\(10^{−1}\\) près. Quelle estimation de \\(\\sigma\\) par maximum de vraisemblance proposez-vous? Re-tracer ensuite l’histogramme montré ci dessus en superposant la densité de la loi normale de moyenne 22 et d’écart-type estimé. 3.10 Loi de Weibull En fiabilité, la durée de vie \\(X\\) d’un composant électronique est souvent modélisée par une loi de Weibull de paramètre \\(\\theta&gt;0\\). Une variable aleatoire \\(X\\) distribuée selon une loi de Weibull de paramètre \\(\\theta\\) a pour densité: \\[\\begin{equation} f(x)=\\theta x^{\\theta-1} \\exp (-x^{\\theta}) \\, \\times \\, 1_{\\mathbb{R}^+} \\tag{3.1} \\end{equation}\\] A noter que quand \\(\\theta=1\\) la loi de Weibull coincide avec la loi Exponentielle de paramètre égal à \\(1\\). L’interprétation du paramètre \\(\\theta\\) est la suivante. Quand \\(\\theta&gt;1\\) cela signifie que la propension à tomber en panne à l’instant \\(t\\) augmente avec \\(t\\), quand \\(\\theta&lt;1\\), c’est l’inverse. Enfin, quand \\(\\theta=1\\) la propension à tomber en panne à l’instant \\(t\\) ne dépent pas de \\(t\\) (la loi exponentielle est sans mémoire). On entend ici par panne toute défaillance du composant électronique rendant celui-ci hors d’usage. 1. Représenter le graphe de la densité d’une loi de Weibull quand \\(\\theta = 1/2, 1, 2,\\text{et } 3\\). On se limitera à l’intervalle \\([0,5]\\) pour \\(x\\). Pour cette question, vous devez créer la fonction dweibull qui calcule la densité comme définie dans l’équation (3.1). Pour estimer \\(\\theta\\) on dispose de la durée de vie de \\(n=1000\\) composants. Les données (en milliers d’heures) se trouvent dans le fichier weibull.txt. 2. Chercher la valeur de \\(\\theta\\) qui maximise la vraisemblance. (\\(\\theta \\in [0,5]\\)). On la notera \\(\\hat{\\theta}\\). 3. Afficher l’histogramme de l’échantillon et superposer la densité de Weibull de paramètre \\(\\hat{\\theta}\\). "],
["estimation-par-intervalle-de-confiance.html", "Chapitre 4 Estimation par Intervalle de confiance 4.1 Intervalles de confiance pour les paramètres de la loi normale 4.2 Intervalle de confiance pour une proportion 4.3 Récapitulatif pour la construction d’intervalles de confiance", " Chapitre 4 Estimation par Intervalle de confiance Jusqu’à présent, on a estimé un paramètre \\(\\theta\\) par une unique valeur \\(\\hat{\\theta}_{n}\\) (estimation ponctuelle). Si l’estimateur \\(\\hat{\\theta}_{n}\\) possède de bonnes propriétés (sans biais, variance minimale), on peut s’attendre à ce que \\(\\hat{\\theta}_{n}\\) soit proche de la vraie valeur de \\(\\theta\\). Cependant, il est très peu probable que \\(\\hat{\\theta}_{n}\\) soit exactement égal à \\(\\theta\\). En particulier, si la loi de \\(\\hat{\\theta}_{n}\\) est continue, on est certains que \\(P\\left(\\hat{\\theta}_{n}=\\theta\\right)=0\\) Par conséquent, plutôt que d’estimer \\(\\theta\\) par la seule valeur \\(\\hat{\\theta}_{n}\\), il semble raisonnable de proposer un ensemble de valeurs vraisemblables pour \\(\\theta,\\) qu’il est logique de prendre proches de \\(\\hat{\\theta}_{n}\\). Cet ensemble de valeurs est appelé région de confiance. Dire que toutes les valeurs de cet ensemble sont vraisemblables pour \\(\\theta\\), c’est dire qu’il y a une forte probabilité que \\(\\theta\\) appartienne à cet ensemble. On supposera dans ce chapitre que \\(\\theta \\in \\mathbb{R}\\), donc la région de confiance sera un intervalle. Définition 4.1 Un intervalle de confiance de seuil (ou niveau de signification) \\(\\alpha \\in [0,1]\\) pour un paramètre \\(\\theta\\), est un intervalle aléatoire \\(I\\) tel que \\(P(\\theta \\in I)=1-\\alpha\\). \\(\\alpha\\) est la probabilité que le paramètre \\(\\theta\\) n’appartienne pas à l’intervalle \\(I\\), c’est à dire la probabilité que l’on se trompe en affirmant que \\(\\theta \\in I\\). C’est donc une probabilité d’erreur, qui doit être assez petite. Les valeurs usuelles de \\(\\alpha\\) sont \\(10 \\%, 5 \\%, 1 \\%,\\) etc. Remarque fondamentale: Les intervalles de confiance suscitent souvent des erreurs d’interprétation et des abus de langage. La raison essentielle de ce problème est expliquée ci dessous. Dans l’écriture \\(P(\\theta \\in I)\\), \\(\\theta\\) est une grandeur inconnue mais non aléatoire. Ce sont les bornes de l’intervalle \\(I\\) qui sont aléatoires. Posons \\(I=[Z_1,Z_2]\\). \\(Z_1\\) et \\(Z_2\\) sont des variables aléatoires. Soient \\(z_1\\) et \\(z_2\\) les réalisations de \\(Z_1\\) et \\(Z_2\\) pour une expérience donnée. Il est correct de dire une phrase du type : “\\(\\theta\\) a \\(95 \\%\\) de chances d’être compris entre \\(Z_1\\) et \\(Z_2\\)”, mais il est incorrect de dire: “\\(\\theta\\) a \\(95 \\%\\) de chances d’être compris entre \\(z_1\\) et \\(z_2\\)”. En fait, si on recommence 100 fois l’expérience, on aura 100 réalisations du couple \\(\\left(Z_{1}, Z_{2}\\right)\\), et donc 100 intervalles de confiance différents. En moyenne, \\(\\theta\\) sera dans 95 de ces intervalles. Par conséquent, il vaut mieux dire : “on a une confiance de \\(95 \\%\\) dans le fait que \\(\\theta\\) soit compris entre \\(z_1\\) et \\(z_2\\)”. Le problème à régler est de trouver un procédé pour déterminer un intervalle de confiance pour un paramètre \\(\\theta\\). Il semble logique de proposer un intervalle de confiance centré sur un estimateur performant \\(\\hat{\\theta}_{n},\\) c’est-à-dire de la forme \\(I=\\left[\\hat{\\theta}_{n}-\\varepsilon, \\hat{\\theta}_{n}+\\varepsilon\\right]\\) . Il reste alors à déterminer \\(\\varepsilon\\) de sorte que : \\[ P(\\theta \\in I)=P\\left(\\hat{\\theta}_{n}-\\varepsilon \\leq \\theta \\leq \\hat{\\theta}_{n}+\\varepsilon\\right)=P\\left(\\left|\\hat{\\theta}_{n}-\\theta\\right| \\leq \\varepsilon\\right)=1-\\alpha \\] Si cet intervalle de confiance est petit, l’ensemble des valeurs vraisemblables pour \\(\\theta\\) est resserré autour de \\(\\hat{\\theta}_{n}\\). Si l’ntervalle de confiance est grand, des valeurs vraisemblables pour \\(\\theta\\) peuvent être éloignées de \\(\\hat{\\theta}_{n}\\). Donc un intervalle de confiance construit à partir d’un estimateur permet de mesurer la précision de cet estimateur. Soit \\(a\\) et \\(b\\) les bornes d’un intervalle de confiance \\(I C_{1-\\alpha}(\\theta)\\) de niveau de confiance \\(1-\\alpha\\) pour le paramètre \\(\\theta\\). On a: \\[ p(a \\leq \\theta \\leq b)=1-\\alpha \\text { et donc } p(\\theta&lt;a)+p(\\theta&gt;b)=\\alpha \\] En posant \\(\\alpha=\\alpha_{1}+\\alpha_{2}\\), il existe une infinité de choix possibles pour \\(\\alpha_{1}\\) et \\(\\alpha_{2}\\), et donc de choix pour \\(a\\) et \\(b\\). Nous ne considérons que le cas d’un intervalle bilatéral à risques symétriques, pour lesquels le risque est partagé en deux parts égales \\(\\alpha_{1}=\\alpha_{2}=\\frac{\\alpha}{2}\\). Néanmoins il arrive en pratique que l’on s’intéresse à des risques unilatéraux, mais nous en parlerons plus en détail dans les chapitres suivants sur les tests statistiques. Pour trouver un intervalle de confiance, il existe plusieurs méthodes. La plus efficace consiste à chercher une fonction pivotale, c’est à dire une variable aléatoire fonction à la fois du paramètre \\(\\theta\\) et des observations \\(X_{1}, \\ldots, X_{n}\\), dont la loi de probabilité ne dépende pas de \\(\\theta\\). Dans la suite de ce chapitre, nous allons illustrer cette méthodologie par des exemples, en déterminant des intervalles de confiance pour: La moyenne et la variance dans un échantillon de loi normale. Une proportion, c’est-à-dire le paramètre d’un échantillon de loi de Bernoulli. Remarques: L’intervalle de confiance est fonction de l’estimation \\(\\hat{\\theta}_{n}\\) de \\(\\theta\\). L’intervalle de confiance est également fonction de \\(\\alpha\\). Plus \\(\\alpha\\) est petit, plus le niveau de confiance est grand, et donc plus l’intervalle s’élargit. Lorsque la taille de l’échantillon grandit, l’estimateur \\(\\hat{\\theta}_{n}\\) étant convergeant la variance \\(V(\\hat{\\theta}_{n})\\) diminue, et l’intervalle se rétrécit. Il est recommandé avant de poursuivre la lecture de se rappeler des lois déduites de la loi normale: loi de \\(\\chi^{2}\\), loi de Student \\(St(n)\\) et loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\). Suite à la définition de ces lois, nous introduisons le théorème de Fisher: Théorème 4.1 (Théorème de Fisher) Si \\(X_{1}, \\ldots, X_{n}\\) sont indépendantes et de même loi \\(\\mathcal{N}\\left(m, \\sigma^{2}\\right)\\), alors, en posant \\(\\overline{X}_{n}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\) et \\(S_{n}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}\\), on a: \\(\\sum_{i=1}^{n} X_{i}\\) est de loi \\(\\mathcal{N}\\left(n m, n \\sigma^{2}\\right)\\). \\(\\overline{X}_{n}\\) est de loi \\(\\mathcal{N} \\left(m, \\frac{\\sigma^{2}}{n}\\right)\\). \\(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(X_{i}-m\\right)^{2}\\) est de loi \\(\\chi_{n}^{2}\\). \\(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}_{n}\\right)^{2}=\\frac{n S_{n}^{2}}{\\sigma^{2}}\\) est de loi \\(\\chi_{n-1}^{2}\\). \\(\\overline{X}_{n}\\) et \\(S_{n}^{2}\\) sont indépendantes. \\(\\sqrt{n-1} \\frac{\\overline{X}_{n}-m}{S_{n}}\\) est de loi de Student \\(S t(n-1)\\). 4.1 Intervalles de confiance pour les paramètres de la loi normale 4.1.1 Intervalle de confiance pour l’espérance d’une loi normale avec variance connue Soit \\(X \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2}\\right)\\) avec \\(\\sigma\\) connu. Le meilleur estimateur de \\(\\mu\\) est \\(\\overline{X}\\). Comme \\(X\\) est de loi normale on a \\[ Z=\\frac{\\overline{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\sim \\mathcal{N}(0,1) \\] En prenant des risques symétriques, on peut lire dans les tables les quantiles \\(z_\\frac{\\alpha}{2}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\) de la loi normale centrée réduite d’ordres respectifs \\(\\frac{\\alpha}{2}\\) et \\(1-\\frac{\\alpha}{2}\\), tels que : \\[ P\\left(z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{1-\\frac{\\alpha}{2}}\\right)=1-\\alpha \\] ou encore \\[ P\\left(Z \\leq z_{\\frac{\\alpha}{2}}\\right)=p\\left(Z \\geq z_{1-\\frac{\\alpha}{2}}\\right)=\\frac{\\alpha}{2} \\] Les quantiles La notion du quantile est définie de la façon suivante: Définition 4.2 Pour une variable aléatoire continue \\(X\\), le nombre \\(q_{\\alpha}\\) tel que \\(P(X \\leq q_{\\alpha}) = \\alpha\\), est le quantile d’ordre \\(\\alpha\\) de la loi de \\(X\\). Ces quantiles sont notés de différentes façons: \\(z_{\\alpha}\\) pour la loi normale, \\(t_{n,\\alpha}\\) pour la loi de Student à \\(n\\) degrés de liberté, \\(z_{n,\\alpha}\\) pour la loi de \\(\\chi_{n}^{2}\\), etc. La figure 4.1 suivante illustre la définition de ces quantiles: Figure 4.1: Quantiles \\(z_{\\frac{\\alpha}{2}}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\) de la loi normale centrée réduite Comme la loi normale est symétrique, on a la propriété suivante: \\[z_{1-\\frac{\\alpha}{2}} = - z_{\\frac{\\alpha}{2}}\\] Ces quantiles sont donnés par les tables statistiques. Par exemple, pour \\(\\alpha=0.05=5\\%\\), on obtient \\(z_{\\frac{\\alpha}{2}}=-1.96\\). Pour \\(\\alpha=0.05=5\\%\\), on obtient selon la table de la loi normale centrée réduite \\(z_{\\frac{\\alpha}{2}}=-1.96\\) et \\(z_{1-\\frac{\\alpha}{2}} = 1.96\\). Revenons à la détermination d’IC pour l’espérance d’une loi normale avec variance connue, on a \\[P(z_{\\frac{\\alpha}{2}} \\leq Z \\leq z_{1-\\frac{\\alpha}{2}}) = 1- \\alpha\\] qui peut s’écrire \\[P(z_{\\frac{\\alpha}{2}} \\leq Z \\leq -z_{\\frac{\\alpha}{2}}) = 1- \\alpha\\] d’où on tire \\[P(\\overline{X} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\overline{X} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}) = 1- \\alpha\\] D’où l’intervalle de confiance: \\[I C_{1-\\alpha}(\\mu)= [\\overline{X} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}},\\overline{X} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}]\\] Pour une réalisation numérique \\(x_1,\\ldots,x_n\\) d’un échantillon \\(X_1,\\ldots,X_n\\) de taille \\(n\\), on obtient l’intervalle de confiance sur \\(\\hat{\\mu}\\) au niveau de confiance \\(1-\\alpha\\): \\[I C_{1-\\alpha}(\\mu)= [\\overline{x} + z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}},\\overline{x} - z_{\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}]\\] qui donne pour \\(\\alpha=0.05\\): \\[[\\overline{x} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\\overline{x} +1.96 \\frac{\\sigma}{\\sqrt{n}}]\\] La table de la loi normale centrée réduite et une application interactive permettant de calculer les quantiles de loi normale se trouvent dans l’annexe B. Une deuxième table qui donne directement le quantile \\(z_{1-\\frac{\\alpha}{2}}\\) par rapport à \\(\\alpha\\) est présentée dans l’annexe C. Le problème est que cet intervalle n’est utilisable que si on connaît la valeur de \\(\\sigma\\). Or, dans la pratique, on ne connaît jamais les vraies valeurs des paramètres. Une idée naturelle est alors de remplacer \\(\\sigma\\) par un estimateur. 4.1.2 Intervalle de confiance pour l’espérance d’une loi normale avec variance inconnue Si la variance \\(\\sigma^2\\) est inconnue, on utilisera à sa place son meilleur estimateur \\(S_n^{*2}\\). Selon le théorème de Fisher, on sait que \\(\\frac{n }{\\sigma^{2}}S_{n}^{2}\\) est de loi \\(\\chi_{n-1}^{2}\\), et que \\(\\frac{n-1}{\\sigma^{2}} S_{n}^{*2}\\) est de loi \\(\\chi_{n-1}^{2}\\) aussi. La statistique que l’on utilisera est donc \\[T_{n-1} = \\frac{\\overline{X}-\\mu}{S_n^*/\\sqrt{n}}\\] En remarquant qu’elle s’écrit \\[T_{n-1} = \\frac{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{\\frac{n-1}{\\sigma^2}S^{*2}}{n-1}}}\\] on trouve qu’elle suit une loi de Student à \\(n-1\\) degrés de liberté, comme rapport d’une loi normale centrée réduite sur la racine d’un \\(\\chi^2\\) divisé par son degré de liberté. Comme précédemment, on obtient l’intervalle de confiance: \\[I C_{1-\\alpha}(\\mu)= [\\overline{x} + t_{n-1,\\frac{\\alpha}{2}} \\frac{S^*}{\\sqrt{n}},\\overline{x} - t_{n-1,\\frac{\\alpha}{2}} \\frac{S^*}{\\sqrt{n}}]\\] où \\(t_{n-1,\\frac{\\alpha}{2}}\\) est le quantile d’ordre \\(\\frac{\\alpha}{2}\\) de la loi de Student à \\(n-1\\) degrés de liberté. Une table de la loi de Student se trouve dans l’annexe D. Remarque: La variable aléatoire \\(\\frac{\\overline{X}-\\mu}{S^*/\\sqrt{n}}\\) est une fonction des observations \\(X_1,\\ldots,X_n\\) et du paramètre \\(\\mu\\) pour lequel on recherche un intervalle de confiance, dont la loi de probabilité ne dépend pas des paramètres du modèle \\(\\mu\\) et \\(\\sigma^2\\). C’est ce qu’on a appelé une fonction pivotale et c’est ce que nous utiliserons pour construire des intervalles de confiance. Si la loi de \\(X\\) n’est pas une loi normale: Dans ce cas, lorsque la taille de l’échantillon \\(n\\) est supérieure ou égale à 30, le théorème central limite nous permet d’utiliser le fait que \\(\\overline{X}_n\\) suit une loi normale, et donc les résultats précédents sont applicables. 4.1.3 Intervalle de confiance pour la variance d’une loi normale Conformément à ce qui précède, on recherche une fonction pivotale, c’est à dire une fonction des observations \\(X_{1}, \\ldots, X_{n}\\) et de \\(\\sigma^{2}\\), dont la loi de probabilité ne dépend ni de \\(\\mu\\) ni de \\(\\sigma^{2}\\). Une telle fonction est donnée par le théorème de Fisher: \\(\\frac{n S_{n}^{2}}{\\sigma^{2}}\\) est de loi \\(\\chi_{n-1}^{2}\\). On a donc, quels que soient les réels \\(a\\) et \\(b\\), \\(0&lt;a&lt;b\\) : \\[\\begin{aligned} P\\left(a \\leq \\frac{n S_{n}^{2}}{\\sigma^{2}} \\leq b\\right) &amp;=P\\left(\\frac{n S_{n}^{2}}{b} \\leq \\sigma^{2} \\leq \\frac{n S_{n}^{2}}{a}\\right) \\quad \\text { d&#39;une part } \\\\ &amp;=F_{\\chi_{n-1}^{2}}(b)-F_{\\chi_{n-1}^{2}}(a) \\quad \\text { d&#39;autre part. } \\end{aligned}\\] Il y a une infinité de façons possibles de choisir \\(a\\) et \\(b\\) de sorte que cette probabilité soit égale à \\(1-\\alpha\\). La façon la plus usuelle de procéder est d’équilibrer les risques, c’est-à-dire de prendre \\(a\\) et \\(b\\) tels que \\(F_{\\chi_{n-1}^{2}}(b)=1-\\frac{\\alpha}{2}\\) et \\(F_{\\chi_{n-1}^{2}}(a)=\\frac{\\alpha}{2}\\). La table de la loi du \\(\\chi^{2}\\) donne la valeur \\(z_{n, \\alpha}\\) telle que, quand \\(Z\\) est une variable aléatoire de loi \\(\\chi_{n}^{2}\\), alors \\(P\\left(Z&gt;z_{n, \\alpha}\\right)=1-F_{\\chi_{n}^{2}}\\left(z_{n, \\alpha}\\right)=\\alpha\\). Alors, pour \\(b=z_{n-1, 1-\\alpha / 2}\\) et \\(a=z_{n-1,\\alpha / 2}\\), on a bien \\[P\\left(\\frac{n S_{n}^{2}}{b} \\leq \\sigma^{2} \\leq \\frac{n S_{n}^{2}}{a}\\right)=1-\\alpha\\] D’où le résultat : Un intervalle de confiance de seuil \\(\\alpha\\) pour le paramètre \\(\\sigma^2\\) de la loi \\(\\mathcal{N}(\\mu,\\sigma^2)\\) est: \\[\\left[\\frac{n S_{n}^{2}}{z_{n-1, 1-\\alpha / 2}}, \\frac{n S_{n}^{2}}{z_{n-1,\\alpha / 2}}\\right]=\\left[\\frac{(n-1) S_{n}^{* 2}}{z_{n-1, 1-\\alpha / 2}}, \\frac{(n-1) S_{n}^{* 2}}{z_{n-1,\\alpha / 2}}\\right]\\] Remarque 1: \\(P\\left(a \\leq \\sigma^{2} \\leq b\\right)=P(\\sqrt{a} \\leq \\sigma \\leq \\sqrt{b}),\\) donc un intervalle de confiance de seuil \\(\\alpha\\) pour l’écart-type \\(\\sigma\\) est: \\[ \\left[\\sqrt{\\frac{n}{z_{n-1, 1-\\alpha / 2}}} S_{n}, \\sqrt{\\frac{n}{z_{n-1,\\alpha / 2}}} S_{n}\\right] \\] Remarque 2: L’intervalle de confiance est de la forme \\(\\left[\\varepsilon_{1} S_{n}^{2}, \\varepsilon_{2} S_{n}^{2}\\right]\\) avec \\(\\varepsilon_{1}&lt;1\\) et \\(\\varepsilon_{2}&gt;1\\) et non pas de la forme \\(\\left[S_{n}^{2}-\\varepsilon, S_{n}^{2}+\\varepsilon\\right] .\\) En fait, si on cherche un intervalle de confiance pour \\(\\sigma^{2}\\) de la forme \\(\\left[S_{n}^{2}-\\varepsilon, S_{n}^{2}+\\varepsilon\\right],\\) la démarche ne va pas aboutir, et on ne peut pas le savoir à l’avance. C’est l’intérêt des fonctions pivotales, qui imposent d’elles-mêmes la forme de l’intervalle de confiance. Une table de la loi de \\(\\chi^2\\) se trouve dans l’annexe E. 4.2 Intervalle de confiance pour une proportion Le probleme connu sous le nom d’intervalle de confiance pour une proportion est en fait le problème de la détermination d’un intervalle de confiance pour le paramètre \\(p\\) de la loi de Bernoulli, au vu d’un échantillon \\(X_{1}, \\ldots, X_{n}\\) de cette loi. On a montré dans le chapitre précédent que le meilleur estimateur de \\(p\\) est \\(\\hat{P}_{n}=\\overline{X}_{n}\\). Supposant que la proportion \\(p\\) d’individus présentant un certain caractère \\(C\\) (par exemple, les individus ayant voté pour un certain candidat) au sein d’une population est inconnue. Le meilleur estimateur de \\(p\\) est \\(\\hat{P}_{n}=\\overline{X}_{n} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) où \\(X_i\\) est une v.a. de Bernoulli de paramètre \\(p\\), définie par: \\[X_{i}=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { si l&#39;individu } i \\text{ possède la caractère } C} \\\\ {0} &amp; {\\text { sinon }}\\end{array}\\right.\\] Comme \\(X\\) suit une loi de Bernoulli \\(\\mathcal{B}(p)\\), \\(n \\hat{p} = \\sum_{i=1}^n X_i\\) suit une loi Binomiale \\(\\mathcal{B}(n,p)\\). Si \\(n\\) est faible, on utilisera les tables de la loi binomiale. Si \\(n\\) est suffisamment grand5, on peut considérer (d’après le TCL6) que \\(\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(np, np(1-p))\\), d’où \\(\\hat{p}_n\\) suit une loi normale \\(\\mathcal{N}(p, \\frac{p(1-p)}{n})\\), et donc \\(Z = \\frac{\\hat{p}_n - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) suit une loi \\(\\mathcal{N}(0,1)\\). Le résultat précédent n’est applicable que si \\(n\\) est suffisamment grand. On obtient alors, en fonction des quantiles \\(P(z_{\\alpha/2} \\leq Z \\leq - z_{\\alpha/2})=1-\\alpha\\), l’intervalle de confiance pour \\(p\\): \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{p(1-p)}{n}}\\Big]\\] Cet intervalle recouvre \\(p\\) avec la probabilité \\(1-\\alpha\\), mais il est toutefois inopérant puisque ses bornes dépendent de \\(p\\). En pratique, il existe trois façons d’obtenir l’intervalle de confiance. Nous retiendrons celle qui remplace \\(p\\) par son estimateur \\(\\hat{P}_n\\). Ainsi, on obtient l’intervalle de confiance sur la proportion \\(p\\) en fonction de la valeur \\(\\hat{p}_n\\) de \\(\\hat{P}_n\\) sur notre échantillon: \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\Big]\\] 4.3 Récapitulatif pour la construction d’intervalles de confiance Pour construire un IC, il faut d’abord déterminer le paramètre concerné par l’étude et vérifier s’il y a d’autres paramètres à estimer. Le tableau suivant résume les cas les plus communs et qui sont présentés dans ce chapitre. Paramètre à borner dans un IC Symbole Autres paramètres? Section Moyenne d’une loi normale \\(\\mu\\) Variance \\(\\sigma^2\\) connue 4.1.1 Moyenne d’une loi quelconque, échantillon grand \\(\\mu\\) \\(\\sigma^2\\) connue et \\(n\\) suffisamment grand pour appliquer TCL 4.1.1 Moyenne d’une loi normale \\(\\mu\\) \\(\\sigma^2\\) inconnue 4.1.2 Variance (ou écart-type) d’une loi normale \\(\\sigma^2\\) Moyenne inconnue, mais estimée 4.1.3 Proportion \\(p\\) Rien 4.2 de sorte que \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).↩ Théorème centrale limite↩ "],
["exercices-3.html", "Exercices", " Exercices Exercice 4.1 Pour une population de loi normale de variance \\(\\sigma^2\\) connue, répondre aux questions suivantes: Quel est le niveau de confiance pour l’intervalle \\(\\overline{x} - 2.14 \\sigma/\\sqrt{n} \\leq \\mu \\leq \\overline{x} + 2.14 \\sigma/\\sqrt{n}\\) ? Quel est le niveau de confiance pour l’intervalle \\(\\overline{x} - 1.85 \\sigma/\\sqrt{n} \\leq \\mu \\leq \\overline{x} + 1.85 \\sigma/\\sqrt{n}\\) ? Quel est le niveau de confiance pour l’intervalle \\(\\mu \\leq \\overline{x} + 1.96 \\sigma/\\sqrt{n}\\) ? Exercice 4.2 Nous souhaitons estimer un intervalle de confiance du gain d’un circuit électronique7. On admet que le gain est distribué selon une loi normale d’espérance inconnue \\(m\\) et d’écart type \\(\\sigma = 20\\). Décrire l’expérience aléatoire, l’univers et la variable aléatoire \\(X\\). Soit \\(n\\) la taille de l’échantillon et \\(\\alpha\\) l’erreur. Donner l’expression de l’intervalle de confiance de \\(m\\) en fonction de \\(n\\), \\(\\sigma\\) et \\(\\alpha\\). Comment doit varier la longueur de l’intervalle de confiance en fonction de la taille d’échantillon et le niveau de confiance? Confirmer votre réponse en donnant l’intervalle de confiance de \\(m\\) selon les cas suivants: IC à 95% où \\(n=10\\) et \\(\\hat{m} = 1000\\). IC à 95% où \\(n=25\\) et \\(\\hat{m} = 1000\\). IC à 99% où \\(n=10\\) et \\(\\hat{m} = 1000\\). IC à 99% où \\(n=25\\) et \\(\\hat{m} = 1000\\). Quelle doit être la taille de l’échantillon \\(n\\) pour que la longueur de l’intervalle de confiance à \\(95\\%\\) soit \\(4\\)? Exercice 4.3 Le poids de paquets de poudre de lessive, à l’issue de l’empaquetage, est supposé suivre une loi normale \\(\\mathcal{N}(\\mu, \\sigma^2 )\\) dont l’écart-type \\(\\sigma\\) est supposé connu et égal à \\(5 g\\). \\(\\sigma\\) représente la variabilité du poids due à l’imprécision de la machine. Le poids marqué sur les paquets est de \\(710 g\\). Toutes les heures, \\(10\\) paquets sont prélevés au hasard et pesés. On obtient pour une heure donnée, pour un échantillon de \\(10\\) paquets un poids moyen de \\(707g\\). Donner un estimateur puis une estimation du poids moyen des paquets de lessive. Donner un intervalle de confiance à \\(95\\%\\), puis à \\(99\\%\\) pour le poids moyen des paquets de lessive Déterminer \\(\\alpha\\) (à l’unité près) pour qu’au seuil de risque \\(\\alpha \\%\\) un intervalle de confiance du poids moyen des paquets de lessive soit \\([705g;709g]\\). Exercice 4.4 Dans la population française, le pourcentage d’individus dont le sang est de rhésus négatif est de \\(15\\%\\). Dans un échantillon représentatif de \\(200\\) Basques français on observe que \\(44\\) personnes sont de rhésus négatif. Donner un intervalle de confiance à \\(99\\%\\) de la proportion de Basques français ayant un rhésus négatif. Exercice 4.5 Le Bureau de météorologie du gouvernement australien a fourni les précipitations annuelles moyennes (en millimètres) en Australie entre 1983 et 2002 comme suit8 499.2 555.2 398.8 391.9 453.4 459.8 483.7 417.6 469.2 452.4 499.3 340.6 522.8 469.9 527.2 565.5 584.1 727.3 558.6 338.6 Supposant que les précipitations annuelles moyennes suivent une loi normale de paramètres inconnus9. Construire un intervalle de confiance \\(95\\%\\) pour la moyenne annuelle de précipitations. Exercice 4.6 Le pourcentage de titane dans un alliage utilisé en aéronautique est mesuré sur \\(51\\) pièces sélectionnées de manière aléatoire. Ce pourcentage suit une distribution normale de paramètres inconnus. L’écart type (corrigé) de l’échantillon est \\(s = 0.37\\). Construire un intervalle de confiance bilatéral à \\(95\\%\\) pour \\(\\sigma\\). Exercice 4.7 Les ampoules électriques d’un fabricant A ont une durée de vie normalement distribuée et de moyenne \\(\\mu_1\\) avec un écart-type \\(\\sigma_1 = 200h\\) et celles d’un fabricant B ont une durée de vie normalement distribuée et de moyenne \\(\\mu_2\\) avec un écart-type \\(\\sigma_2 = 100h\\). Un échantillon de \\(150\\) ampoules de A a donné une durée de vie moyenne de \\(1400h\\). Un échantillon de \\(100\\) ampoules B a donné une durée de vie moyenne de \\(1200h\\). Déterminer un intervalle de confiance à \\(95\\%\\) puis à \\(99 \\%\\) de la différence des durées de vie moyenne des variétés A et B. En électronique, le gain désigne la capacité d’un circuit électronique à augmenter la puissance ou l’amplitude d’un signal. (source: Wikipedia).↩ Lien↩ On peut vérifier cette assomption, càd la normalité de données avec la figure connue sous le nom Droite de Henry ou QQ-plot. On pourra la tracer dans R avec la fonction qqnorm()↩ "],
["tp-intervalle-de-confiance.html", "TP Intervalle de confiance IC pour la moyenne et la variance de la loi normale IC pour une proportion et effet de la confiance IC pour le paramètre \\(\\lambda\\) d’une loi exponentielle", " TP Intervalle de confiance IC pour la moyenne et la variance de la loi normale On se propose d’étudier le poids des poulpes femelles. On va construire des intervalles de confiances pour la moyenne et la variance de cette variable, à l’aide du fichier de données poulpe.csv . Récupérer le fichier poulpe.csv et charger le dans votre session . Calculer les estimateurs de la moyenne et de la variance. Représenter les données sur un histogramme. Déterminer un intervalle de confiance à \\(95\\%\\) pour la moyenne, en calculant les bornes de l’intervalle avec la fonction qt(). Retrouver cet intervalle à l’aide de la fonction t.test(). Déterminer un intervalle de confiance à \\(95\\%\\) pour la variance. Créer une fonction ICvar() ayant comme arguement un vecteur d’observations, et un risque \\(\\alpha\\), et qui sort l’intervalle de confiance pour la variance de l’échantillon. IC pour une proportion et effet de la confiance Soit \\(X_1,\\ldots,X_n\\) un échantillon de la loi de Bernoulli de paramètre \\(p\\). L’emm et l’emv de \\(p\\) est \\(\\hat{p}_n = \\overline{X}_n=\\frac{1}{n} \\sum_{i=1}^n X_i\\). L’intervalle de confiance approximativement de niveau \\(1-\\alpha\\) pour \\(p\\) si \\(np &gt; 5\\) et \\(n(1-p)&gt;5\\) est \\[IC_{1-\\alpha}(p)= \\Big[\\hat{p}_n + z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}, \\hat{p}_n - z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}_n(1-\\hat{p}_n)}{n}}\\Big]\\] où \\(z_{\\alpha/2}\\) désigne le quantile d’ordre \\(\\alpha/2\\) d’une loi normale centrée réduite. Choisir une valeur de \\(p\\), strictement comprise entre \\(0\\) et \\(1\\). Tirer \\(100\\) échantillons de taille \\(100\\) de la loi de Bernoulli de paramètre \\(p\\). Pour \\(\\alpha=0.1\\), puis \\(0.05\\), puis \\(0.01\\), calculer les valeurs prises par les \\(100\\) intervalles de confiance bilatéraux de niveau \\(1-\\alpha\\) pour \\(p\\). Calculer en pourcentage le nombre d’intervalles qui contiennent la valeur de \\(p\\). Interpréter. Représenter graphiquement les intervalles par des segments horizontaux superposés, et la vraie valeur du paramètre \\(p\\) par un trait rouge vertical. (Indication: utiliser les fonctions matplot() et abline(). On pourra aussi colorier différemment les IC qui contiennent ou pas \\(p\\)). Refaire la simulation précédante en tirant \\(100\\) échantillons de taille \\(20\\) au lieu de \\(100\\) et en choisissant \\(p\\) entre \\(0\\) et \\(0.2\\). Pour \\(\\alpha=0.05\\), calculer le nombre d’intervalles qui ne contiennent pas la valeur de \\(p\\). Interpréter. IC pour le paramètre \\(\\lambda\\) d’une loi exponentielle Soit \\(X_1,\\ldots,X_n\\) un échantillon de la loi exponentielle de paramètre \\(\\lambda\\). L’emv de \\(\\lambda\\) est \\(T = n/(X_1+\\ldots+X_n)\\). La variable aléatoire \\(Y=2\\lambda X\\) suit une loi exponentielle de paramètre \\(1/2\\), qui est aussi une loi \\(\\chi^2(2)\\) (preuve ). Soit donc \\(Y_i = 2\\lambda X_i\\) un échantillon iid de loi \\(\\chi^2(2)\\). Soit \\(T=2\\lambda \\sum_{i=1}^n X_i = \\sum_{i=1}^n Y_i\\). Donc \\(T\\) suit la loi \\(\\chi^2(2n)\\). Construire un IC pour \\(\\lambda\\) (sur papier). Un modèle théorique suggère que le durée des appels téléphoniques suit une distribution exponentielle de paramètre \\(\\lambda\\). Un échantillon aléatoire de \\(n = 10\\) durées d’appel suivantes (en minutes): 2.84 2.37 7.52 2.76 3.83 1.32 8.43 2.25 1.63 0.27 Calculer l’IC de \\(\\lambda\\) selon cet échantillon et l’IC de la moyenne de durées des appels selon cet échantillon. Simulation Choisir maintenant une valeur de \\(\\lambda\\). Tirer \\(100\\) échantillons de taille \\(10\\) de la loi exponentielle de paramètre \\(\\lambda\\). Calculer les \\(100\\) valeurs prises par l’estimateur \\(T\\) sur ces échantillons. Pour \\(\\alpha=0.1\\), puis \\(0.05\\), puis \\(0.01\\), calculer les valeurs prises par les \\(100\\) intervalles de confiance bilatéraux de niveau \\(1-\\alpha\\) pour \\(\\lambda\\). Calculer en pourcentage le nombre d’intervalles qui contiennent la valeur de \\(\\lambda\\). Représenter graphiquement les intervalles par des segments horizontaux superposés, et la vraie valeur du paramètre \\(\\lambda\\) par un trait rouge vertical. "],
["tests-dhypotheses.html", "Chapitre 5 Tests d’hypothèses 5.1 Introduction: le problème de décision 5.2 Les tests d’hypothèses à partir d’un seul échantillon", " Chapitre 5 Tests d’hypothèses 5.1 Introduction: le problème de décision Dans tous les domaines, de l’expérimentation scientifique à la vie quotidienne, on est amené à prendre des décisions sur une activité risquée au vu de résultats d’expériences ou d’observation de phénomènes dans un contexte incertain. Par exemple: Informatique: au vu des résultats des tests d’un nouveau système informatique, on doit décider si ce système est suffisamment fiable et performant pour être mis en vente. Essais thérapeutiques: décider si un nouveau traitement médical est meilleur qu’un ancien au vu du résultat de son expérimentation sur des malades. Finance: au vu du marché, décider si on doit ou pas se lancer dans une opération financière donnée. Dans chaque cas, le problème de décision consiste à trancher, au vu d’observations, entre une hypothèse appelée hypothèse nulle, notée \\(H_0\\), et une autre hypothèse dite hypothèse alternative, notée \\(H_1\\). En général, on suppose qu’une et une seule de ces deux hypothèses est vraie. Un test d’hypothèses est une procédure qui permet de choisir entre ces deux hypothèses. Dans un problème de décision, deux types d’erreurs sont possibles: Erreur de première espèce: décider que \\(H_1\\) est vraie alors que \\(H_0\\) est vraie. Erreur de seconde espèce: décider que \\(H_0\\) est vraie alors que \\(H_1\\) est vraie. Les conséquences de ces deux erreurs peuvent être d’importances diverses. En général, une des erreurs est plus grave que l’autre: Informatique: si on conclut à tort que le système n’est pas assez fiable et performant, on engagera des dépenses inutiles pour le tester et l’analyser et on risque de se faire souffler le marché par la concurrence; si on décide à tort qu’il est suffisamment fiable et performant, on va mettre en vente un produit qui ne satisfera pas la clientèle, ce qui peut coûter cher en image de marque comme en coût de maintenance. Essais thérapeutiques: on peut adopter un nouveau traitement moins efficace, voire pire que l’ancien, ou se priver d’un nouveau traitement plus efficace que l’ancien. Finance: si on décide à tort que l’on peut lancer l’opération, on risque de perdre beaucoup d’argent; si on décide à tort de ne pas lancer l’opération, on peut se priver d’un bénéfice important. A toute décision correspond une probabilité de décider juste et une probabilité de se tromper: La probabilité de l’erreur de première espèce, qui est la probabilité de rejeter à tort \\(H_0\\), est notée \\(\\alpha\\) et est appelée seuil ou niveau de signification du test. C’est la même terminologie que pour les intervalles de confiance, ce qui n’est pas un hasard10. Dans certains contextes, cette probabilité est appelée risque fournisseur. La probabilité de l’erreur de deuxième espèce est notée \\(\\beta\\) et est parfois appelée risque client. C’est la probabilité d’accepter à tort \\(H_0\\). La probabilité de décider \\(H_1\\) ou de rejeter \\(H_0\\) à raison est \\(1-\\beta\\). Elle est appelée puissance du test. \\(1-\\alpha\\) est appelée niveau de confiance du test. Les deux tableaux suivants résument simplement le rôle de ces probabilités. Décision \\(\\backslash\\) Vérité \\(H_0\\) \\(H_1\\) \\(H_0\\) conclusion correcte erreur de deuxième espèce \\(H_1\\) erreur de première espèce conclusion correct Décision \\(\\backslash\\) Vérité \\(H_0\\) \\(H_1\\) \\(H_0\\) niveau de confiance \\(1-\\alpha\\) risque \\(\\beta\\) \\(H_1\\) risque \\(\\alpha\\) Puissance de test \\(1-\\beta\\) L’idéal serait évidemment de trouver une procédure qui minimise les deux risques d’erreur en même temps. Malheureusement, on montre qu’ils varient en sens inverse, c’est- à-dire que toute procédure diminuant \\(\\alpha\\) va en général augmenter \\(\\beta\\) et réciproquement. Dans la pratique, on va donc considérer que l’une des deux erreurs est plus importante que l’autre, et tâcher d’éviter que cette erreur se produise. Il est alors possible que l’autre erreur survienne. Par exemple, dans le cas du procès, on fait en général tout pour éviter de condamner un innocent, quitte à prendre le risque d’acquitter un coupable. On va choisir \\(H_0\\) et \\(H_1\\) de sorte que l’erreur que l’on cherche à éviter soit l’erreur de première espèce, \\(\\alpha\\). Mathématiquement cela revient à se fixer la valeur du seuil du test \\(\\alpha\\). Plus la conséquence de l’erreur est grave, plus \\(\\alpha\\) sera choisi petit. Les valeurs usuelles de \\(\\alpha\\) sont \\(10\\%\\), \\(5\\%\\), \\(1\\%\\), ou beaucoup moins. Le principe de précaution consiste à limiter au maximum la probabilité de se tromper, donc à prendre \\(\\alpha\\) très petit. On choisit \\(H_0\\) et \\(H_1\\) de sorte que l’erreur que l’on cherche à éviter soit l’erreur de première espèce, \\(\\alpha\\). Le choix d’un test sera le résultat d’un compromis entre risque de premier espèce et puissance du test. On appelle règle de décision une règle qui permette de choisir entre \\(H_0\\) et \\(H_1\\) au vu des observations \\(x_1 ,\\ldots,x_n\\), sous la contrainte que la probabilité de rejeter à tort \\(H_0\\) est égale à \\(\\alpha\\) fixé. Une idée naturelle est de conclure que \\(H_0\\) est fausse s’il est très peu probable d’observer \\(x_1 ,\\ldots,x_n\\) quand \\(H_0\\) est vraie. Par exemple, admettons que l’on doive décider si une pièce est truquée ou pas au vu de 100 lancers de cette pièce. Si on observe 90 piles, il est logique de conclure que la pièce est truquée et on pense avoir une faible probabilité de se tromper en concluant cela. Mais si on observe 65 piles, que conclure? Une fois que l’on a fixé raisonnablement \\(\\alpha\\), il faut choisir une variable de décision, qui doit apporté le maximum d’information sur le problème posé, et dont la loi sera différente selon que \\(H_0\\) ou \\(H_1\\) est vraie. La loi sous \\(H_0\\) doit être connue. On définit alors la région critique qui est l’ensemble des valeurs de la variable de décision qui conduisent à rejeter \\(H_0\\) au profit de \\(H_1\\). Sa forme est déterminée par la nature de \\(H_1\\), et sa détermination exacte est donnée par \\(P(RC|H_0) = \\alpha\\). La région d’acceptation est son complémentaire \\(\\overline{RC}\\). Remarque: il vaut mieux dire “ne pas rejeter \\(H_0\\)” que “accepter \\(H_0\\)”. En effet, si on rejette \\(H_0\\), c’est que les observations sont telles qu’il est très improbable que \\(H_0\\) soit vraie. Si on ne rejette pas \\(H_0\\), c’est qu’on ne dispose pas de critères suffisants pour pouvoir dire que \\(H_0\\) est fausse. Mais cela ne veut pas dire que \\(H_0\\) est vraie. Un test permet de dire qu’une hypothèse est très probablement fausse ou seulement peut-être vraie. Par conséquent, dans un problème de test, il faut choisir les hypothèses \\(H_0\\) et \\(H_1\\) de façon à ce que ce qui soit vraiment intéressant, c’est de rejeter \\(H_0\\). 5.1.1 Les tests unilatérales et bilatérales Une hypothèse alternative bilatérale est appropriée lorsqu’on souhaite vérifier si le paramètre \\(\\theta\\) d’une distribution diffère d’une valeur arbitraire \\(\\theta_0\\). Par exemple, s’il est important de détecter les valeurs de la moyenne réelle \\(\\mu\\) plus grandes ou plus petites que \\(\\mu_0\\), on doit utiliser l’alternative bilatérale: \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu\\neq\\mu_{0} \\] Par contre, supposons qu’on désire rejeter \\(H_0\\) seulement lorsque la vraie valeur de la moyenne excède \\(\\mu_0\\). Ainsi, les hypothèses sont \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] La région critique est alors située dans l’aile supérieure de la distribution de la statistique. On rejette \\(H_0\\) si \\(\\overline{x}\\) est trop grand. Ceci est un test unilatéral à droite. Une troisième possibilité consiste à faire un test unilatéral à gauche en posant les hypothèses \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] On rejette alors \\(H_0\\) lorsque la moyenne échantillonnale est trop petite. 5.1.2 Relation entre tests d’hypothèses bilatéraux et intervalles de confiance Il existe une relation étroite entre le test d’une hypothèse sur un paramètre \\(\\theta\\) et l’intervalle de confiance pour \\(\\theta\\). Si \\([L,U]\\) est un intervalle de confiance de niveau \\(1-\\alpha\\) pour le paramètre \\(\\theta\\), alors le test de seuil \\(\\alpha\\) de l’hypothèse \\[ H_{0}: \\theta=\\theta_{0} \\text { contre } H_{1}: \\theta\\neq \\theta_{0} \\] mènera au rejet de \\(H_0\\) si et seulement si \\(\\theta_0\\) n’appartient par à l’intervalle \\([L,U]\\). Exemple 5.1 Considérons une situation où un intervalle de confiance de niveau \\(1-\\alpha=0.95\\) pour la moyenne d’une population a été calculé à partir d’un échantillon, et où on a obtenu \\([41,43]\\). Comme la valeur \\(40\\) n’appartient pas à cet intervalle, alors on sait qu’en conduisant un test d’hypothèse sur le même échantillon, l’hypothèse \\(H_0 : \\mu = 40\\) sera rejetée au seuil \\(\\alpha=0.05\\), au profit de son alternative bilatérale. 5.1.3 La démarche générale d’un test d’hypothèse Récapitulons l’ensemble de la démarche à suivre pour effectuer un test d’hypothèses: Choisir \\(H_0\\) et \\(H_1\\) de sorte que ce qui importe, c’est le rejet de \\(H_0\\). Se fixer \\(\\alpha\\) selon la gravité des conséquences de l’erreur de première espèce. Déterminer la région critique: établir la règle de rejet de \\(H_0\\) à partir de la distribution de la statistique du test (lorsque \\(H_0\\) est vraie). Calculer la valeur observée de la statistique du test à partir de l’échantillon. Comparer la valeur observée à la valeur critique: Regarder si les observations se trouvent ou pas dans la région critique.11 Conclure au rejet ou au non-rejet de \\(H_0\\). 5.1.4 Types de test d’hypothèses On distingue différentes catégories de tests: Les tests paramétriques ont pour objet de tester une certaine hypothèse relative à un ou plusieurs paramètres d’une variable aléatoire de loi spécifiée (généralement supposée normale). Lorsque le test est toujours valide pour des variables non gaussiennes, on dit que le test est robuste (à la loi). Les tests non paramétriques qui portent généralement sur la fonction de répartition de la variable aléatoire, sa densité, etc. Les tests libres (distributions free) qui ne supposent rien sur la loi de probabilité de la variable aléatoire étudiée (et qui sont donc robuste). Ces tests sont souvent non paramétriques, mais pas toujours. On s’intéressera d’abord dans ce chapitre à des tests d’hypothèses paramétriques quand l’observation est un échantillon d’une loi de probabilité. Puis on étudiera dans les chapitres suivants des tests de comparaison de deux échantillons, et on terminera en présentant le plus célèbre des tests d’hypothèses, le test du \\(\\chi^2\\). 5.2 Les tests d’hypothèses à partir d’un seul échantillon Dans cette section, nous abordons les détails de la procédure des tests d’hypothèses dans diverses situations: les tests sur la moyenne d’une distribution normale avec variance connue et inconnue, les tests sur la variance d’une distribution normale, et les tests sur une proportion (pour une grande taille d’échantillon). 5.2.1 Les tests sur la moyenne d’une distribution normale de variance connue 5.2.1.1 La procédure pour le test bilatéral Supposons que la variable aléatoire \\(X\\) représente un certain processus ou une certain population d’intérêt. On suppose que \\(X\\) suit une loi normale ou que, dans le cas contraire, le théorème central limite s’applique. De plus on suppose que la moyenne \\(\\mu\\) de \\(X\\) est inconnue, mais que sa variance \\(\\sigma^2\\) est connue. On veut tester les hypothèses \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu\\neq\\mu_{0} \\] où \\(\\mu_0\\) est une constante spécifiée. Un échantillon aléatoire simple \\(X_1,\\ldots,X_n\\) de taille \\(n\\) est disponible. Chaque observation dans cet échantillon a une moyenne théorique \\(\\mu\\) inconnue et une variance théorique \\(\\sigma^2\\) connue. Si l’hypothèse nulle \\(H_0: \\mu = \\mu_0\\) est vraie, alors \\(\\overline{X}_n \\thicksim \\mathcal{N}(\\mu_0,\\sigma^2/n)\\). La procédure de test utilise la statistique: \\[ Z_0 = \\frac{ \\overline{X}_n - \\mu_0 }{\\sigma/\\sqrt{n}} \\] laquelle, sous l’hypothèse nulle, suit la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). Donc, si \\(H_0: \\mu = \\mu_0\\) est vraie, la probabilité est \\(1-\\alpha\\) qu’une valeur de la statistique \\(Z_0\\) soit entre \\(z_{\\frac{\\alpha}{2}}\\) et \\(z_{1-\\frac{\\alpha}{2}}\\), où \\(z_{1-\\frac{\\alpha}{2}}\\) est le quantile de la loi normale centrée réduite tel que \\(P(Z_0 \\geq z_{1-\\frac{\\alpha}{2}})=\\alpha/2\\). Les figures 5.1 et 5.2 illustrent respectivement la distribution de \\(\\overline{X}_n\\) et \\(Z_0\\) lorsque \\(H_0\\) est vraie. Figure 5.1: La distribution de \\(\\overline{X}_n\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie Figure 5.2: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie Visiblement, un échantillon donnant une valeur de la statistique qui appartient aux extrémités de la distribution de \\(Z_0\\) est peu probable si \\(H_0: \\mu = \\mu_0\\) est vraie. Le cas échéant, on aurait raison de remettre en doute la véracité de \\(H_0\\). Critère de rejet de \\(H_0: \\mu = \\mu_0\\) (test bilatéral, loi normale, variance connue) On se donne comme règle de rejeter \\(H_0: \\mu = \\mu_0\\) au profit de \\(H_1: \\mu \\neq \\mu_0\\) si \\[\\begin{equation} z_0 &lt; z_{\\alpha/2} \\text{ ou si } z_0 &gt; z_{1-\\alpha/2} \\tag{5.1} \\end{equation}\\] et de na pas rejeter \\(H_0\\) si \\[\\begin{equation} z_{\\alpha/2} \\leq z_0 \\leq z_{1-\\alpha/2} \\tag{5.2} \\end{equation}\\] L’équation (5.1) définit la région critique ou de rejet de \\(H_0\\). La probabilité de l’erreur de première espèce de cette procédure du test est \\(\\alpha\\). On peut déduire de l’équation (5.2) que les deux valeurs critique de la figure 5.1 sont les suivantes: \\[ v_1 = \\mu_0 + z_{\\alpha/2} \\, \\sigma/\\sqrt{n} \\quad \\text{ et } \\quad v_2 = \\mu_0 + z_{1-\\alpha/2} \\, \\sigma/\\sqrt{n} \\] Exemple 5.2 On étudie la vitesse de combustion du carburant d’une fusée. Le cahier des charges exige que la vitesse moyenne de combustion soit de \\(40\\, cm/{s}\\). Supposons que l’écart-type de la vitesse de combustion est d’environ \\(2 \\, cm/s\\) et que cette variable suit une distribution normale. Hypothèses et seuil: L’expérimentateur décide de spécifier une probabilité d’erreur de première espèce \\(\\alpha=0.05\\) et base le test sur un échantillon aléatoire de taille \\(n=25\\). Il veut tester les hypothèses \\({H}_{0}: \\mu=40\\, cm/s\\) contre \\({H}_{1}: \\mu \\neq 40\\, cm/s\\). Règle de rejet de \\({H}_{0}\\): Puisque le test est bilatéral, on rejettera \\(H_{0}\\) si \\(\\overline{x}\\) est trop grand ou trop petit par rapport à \\(40\\, cm/s\\). On devra comparer la valeur observée \\(z_{0}\\) de la statistique du test \\(Z_{0}\\) avec les valeurs critiques \\(\\pm z_{0.025}=\\pm 1,96\\). Si \\(-1.96 \\leq z_{0} \\leq 1.96\\), l’hypothèse nulle ne sera pas rejetée. Calcul de la statistique observée: Les 25 unités sont essayées, et la vitesse moyenne de combustion de l’échantillon obtenue est \\(\\overline{x}=41.25\\, cm/s\\). La valeur de la statistique dans l’équation \\(Z_0\\) est \\[ z_{0}=\\frac{\\overline{x}-\\mu_{0}}{\\sigma / \\sqrt{n}}=\\frac{41.25-40}{2 / \\sqrt{25}}=3.125 \\] Décision: On remarque que \\(z_0\\) appartient à la région critique, car \\(3.125&gt;1.96\\). Donc, \\(H_0\\) est rejetée. Conclusion: La vitesse moyenne de combustion excède \\(40\\, cm/s\\) de façon significative au seuil de \\(5 \\%\\). 5.2.1.2 La procédure pour les tests unilatéraux Supposons maintenant qu’on désire tester l’hypothèse alternative unilatérale à droite, soit \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] En définissant la région critique de ce test, on remarque qu’une valeur négative de la statistique \\(Z_0\\) ne conduira jamais à conclure que \\(H_0: \\mu=\\mu_0\\) est fausse. On doit donc placer la région critique dans l’aile supérieure de la distribution \\(\\mathcal{N}(0,1)\\) et rejeter \\(H_0\\) pour les trop grandes valeurs de \\(z_0\\). Autrement dit, on doit rejeter \\(H_0\\) si \\(z_0 &gt; z_{1-\\alpha}\\). De même, pour tester l’hypothèse unilatérale à gauche \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] on doit calculer la statistique \\(Z_0\\) et rejeter \\(H_0\\) pour les trop petites valeurs de \\(z_0\\). Autrement dit, la région critique est dans l’aile inférieure de la distribution \\(\\mathcal{N}(0,1)\\), et on rejette \\(H_0\\) si \\(z_0 &lt; z_{\\alpha}\\) Figure 5.3: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie contre \\(H_1: \\mu &gt; \\mu_0\\) (Upper-tailed test) Figure 5.4: La distribution de \\(Z_0\\) lorsque \\(H_0 : \\mu = \\mu_0\\) est vraie contre \\(H_1: \\mu &lt; \\mu_0\\) (Lower-tailed test) 5.2.1.3 Le calcul du risque de deuxième espèce et la puissance du test Lorsque l’analyste teste des hypothèses, il choisit directement la probabilité de l’erreur de première espèce \\(\\alpha\\). Cependant, la probabilité de l’erreur de deuxième espèce \\(\\beta\\) doit aussi être prise en compte. Considérons par exemple l’hypothèse unilatérale: \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] Supposans que l’hypothèse nulle est fausse et que la valeur réelle de la moyenne est \\(\\mu_1\\), une valeur plus grande que \\(\\mu_0\\). Puisque \\(H_1\\) est vraie, la moyenne échantillonnale \\(\\overline{X}\\) suit la loi \\(\\mathcal{N}(\\mu_1,\\sigma^2/n)\\). La figure 5.5 représente la distribution de \\(\\overline{X}\\) sous l’hypothèse nulle \\(H_0\\) et sous l’hypothèse alternative \\(H_1\\). On rejette \\(H_0\\) si \\(\\overline{x}\\) est supérieure à la valeur critique \\(v\\), qui vaut \\(\\mu_0 + z_{1-\\alpha} \\sigma/\\sqrt{n}\\). Rappelons qu’une erreur de deuxième espèce est commise si \\(H_0\\) n’est pas rejetée alors qu’elle est fausse. L’examen de cette figure révèle que si \\(H_1\\) est vraie, une erreur de deuxième espèce ne sera commise que si \\(\\overline{x} &lt; v\\). Autrement dit, la probabilité de l’erreur de deuxième espèce \\(\\beta\\) correspond à la région verte de la figure 5.5. Figure 5.5: La distribution de \\(\\overline{X}\\) sous \\(H_0\\) et sous \\(H_1\\) Figure 5.6: La distribution de \\(Z_0\\) sous \\(H_0\\) et sous \\(H_1\\) Les facteurs qui influencent le risque de deuxième espèce \\(\\beta\\) du test sur une moyenne: \\(\\beta\\) diminue lorsque \\(\\mu_1\\) s’éloigne de \\(\\mu_0\\). \\(\\beta\\) diminue lorsque \\(n\\) augmente. \\(\\beta\\) diminue lorsque \\(\\alpha\\) augmente. Concernant la puissance du test, qui vaut \\(1 - \\beta\\): La puissance d’un test est fonction de la nature de \\(H_1\\), un test unilatéral est plus puissant qu’un test bilatéral. La puissance d’un test augmente lorsque \\(n\\) augmente. La puissance d’un test diminue lorsque \\(\\alpha\\) diminue. Considérons le problème du combustible de fusée de l’exemple 5.2 et tenons pour acquis qu’on réalise un test unilatérale à droite: \\[ H_{0}: \\mu=40\\, cm/{s} \\text { contre } H_{1}: \\mu &gt; \\, cm/{s} \\] Supposons que l’analyste veut rejeter \\(H_0\\) avec une forte probabilité si la véritable vitesse moyenne de combustion est \\(\\mu=41\\, cm/{s}\\). En d’autres termes, il veut une faible probabilité d’erreur de deuxième espèce si \\(\\mu=41\\, cm/{s}\\). Montrer que la puissance du test est égale à \\(80.5\\%\\). Indication: Commencer par calculer: \\[\\beta = P(\\text{Ne pas rejeter } H_0 | H_1 \\text{ est vraie})\\] Puisque la puissance du test est affectée par la taille de l’échantillon, il est courant de fixer \\(1-\\beta\\) et de trouver la valeur de \\(n\\) qui nous permet de l’atteindre. 5.2.2 Le tests sur la moyenne d’une distribution normale de variance inconnue On a élaboré la procédure de test pour l’hypothèse nulle \\(H_0:\\mu=\\mu_0\\) en supposant que la variance \\(\\sigma^2\\) est connue. Cependant, dans de nombreuses situations pratiques, \\(\\sigma^2\\) n’est pas connue. Pour tester des hypothèses sur la moyenne \\(\\mu\\) d’une population lorsque la variance \\(\\sigma^2\\) est inconnue, on suppose que la distribution de la variable \\(X\\) est normale12. Cette supposition mène à une procédure de test souvent appelée test \\(t\\) ou test de Student, car elle fait appel à la loi de Student13. 5.2.2.1 La procédure pour le test bilatéral Supposons que \\(X\\) est une variable aléatoire de distribution normale de moyenne \\(\\mu\\) et de variance \\(\\sigma^{2}\\) inconnues. Testons l’hypothèse que \\(\\mu\\) est égale a une constante \\(\\mu_{0}\\). On remarque que cette situation est similaire à celle qui a été traitée dans la section précédente sauf que, maintenant, les deux paramètres \\(\\mu\\) et \\(\\sigma^{2}\\) sont inconnus. Supposons qu’on dispose d’un échantillon de taille \\(n\\), soit \\(X_{1}, X_{2}, \\ldots, X_{n}\\), et soit respectivement \\(\\overline{X}\\) et \\({S_n^{*}}^2=\\frac{n}{n-1} S_{n}^{2}\\) la moyenne et la variance de l’échantillon. Supposons qu’on désire tester l’hypothèse alternative bilatéral \\[ {H}_{0}: \\mu=\\mu_{0} \\text{ contre } {H}_{1}: \\mu\\neq \\mu_{0} \\] La procédure du test \\(t\\) repose sur la statistique \\[ T_{0}=\\frac{\\overline{X}-\\mu_{0}}{S^* / \\sqrt{n}} \\] qui suit la loi \\(St\\) avec \\(n-1\\) degrés de liberté si l’hypothèse nulle \\({H}_{0}: \\mu=\\mu_{0}\\) est vraie. Critère de rejet de \\(H_0: \\mu = \\mu_0\\) (test bilatéral, loi normale, variance inconnue) On se donne comme règle de rejeter \\(H_0: \\mu = \\mu_0\\) au profit de \\(H_1: \\mu \\neq \\mu_0\\) si \\[|t_0| &gt; t_{n-1,\\frac{\\alpha}{2}}\\] où \\(t_0\\) est la valeur observée de la Statistique \\(T_0\\) et \\(t_{n-1,\\frac{\\alpha}{2}}\\) est le quantile d’ordre \\(1-\\alpha/2\\) de la loi \\(t\\) de Student avec \\(n-1\\) degrés de liberté. 5.2.2.2 La procédure pour les tests unilatéraux Pour l’hypothèse alternative unilatérale à droite \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &gt; \\mu_{0} \\] On calcule la valeur observée \\(t_0\\) et on rejette \\(H_0\\) si \\(t_0 &gt; t_{n-1,{1-\\alpha}}\\). Pour l’hypothèse alternative unilatérale à gauche \\[ H_{0}: \\mu=\\mu_{0} \\text { contre } H_{1}: \\mu &lt; \\mu_{0} \\] On rejette \\(H_0\\) si \\(t_0 &lt; t_{n-1,{\\alpha}}\\). Les tests sur la moyenne dans s’effectuent à l’aide de la fonction t.test() 5.2.3 Les tests sur la variance d’une distribution normale Soit un \\(n\\)-échantillon \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) issu d’une population de loi normale, de moyenne \\(\\mu\\) et de variance \\(\\sigma^{2}\\). La normalité est indispensable pour ce test sur la variance. 5.2.3.1 La procédure pour le test bilatéral Supposons qu’on désire tester l’hypothèse que la variance \\(\\sigma^2\\) d’une distribution normale est égale à une valeur spécifiée \\(\\sigma_0^2\\), on teste \\[ H_{0}: \\sigma^2=\\sigma_{0}^2 \\text { contre } H_{1}: \\sigma^2 \\neq \\sigma_{0}^2 \\] On utilisera la statistique \\[ U_0 = \\frac{(n-1) {S_n^{*}}^2}{\\sigma_{0}^2} \\] où \\({S_n^{*}}^2\\) est la variance corrigée de l’échantillon. Si \\(H_0:\\sigma^2=\\sigma_{0}^2\\) est vraie, alors la statistique \\(U_0\\) suit la loi du \\(\\chi^2(n-1)\\). Critère de rejet de \\(H_0:\\sigma^2=\\sigma_{0}^2\\) (test bilatéral, loi normale) On rejette \\(H_0:\\sigma^2=\\sigma_{0}^2\\) si \\[u_0 &gt; \\chi^2_{n-1,\\frac{\\alpha}{2}} \\quad \\text{ ou } \\quad u_0 &lt; \\chi^2_{n-1,1-\\frac{\\alpha}{2}}\\] où \\(u_0\\) est la valeur observée de la Statistique \\(U_0\\), \\(\\chi^2_{n-1,\\frac{\\alpha}{2}}\\) et \\(\\chi^2_{n-1,1-\\frac{\\alpha}{2}}\\) sont les quantile d’ordre \\(\\alpha/2\\) et \\(1-\\alpha/2\\) de la loi \\(\\chi^2\\) avec \\(n-1\\) degrés de liberté. Figure 5.7: La distribution de \\(U_0\\) lorsque \\(H_0 : \\sigma^2 = \\sigma^2_0\\) est vraie Attention, contrairement à la loi de Student et la loi normale centrée réduite, la densité de la loi du \\(\\chi^2\\) n’est pas symétrique. 5.2.3.2 La procédure pour les tests unilatéraux sur la variance Pour l’hypothèse alternative unilatérale à droite \\[ H_{0}: \\sigma^2=\\sigma^2_{0} \\text { contre } H_{1}: \\sigma^2&gt; \\sigma^2_{0} \\] On calcule la valeur observée \\(u_0\\) et on rejette \\(H_0\\) si \\(u_0 &gt; \\chi^2_{n-1,\\alpha}\\). Pour l’hypothèse alternative unilatérale à gauche \\[ H_{0}: \\sigma^2=\\sigma^2_{0} \\text { contre } H_{1}: \\sigma^2&lt; \\sigma^2_{0} \\] On rejette \\(H_0\\) si \\(u_0 &lt; \\chi^2_{n-1,1-\\alpha}\\). 5.2.4 Les tests sur une proportion Dans la population étudiée, une proportion \\(p\\) des individus possèdent un certain caractère \\(C\\). On se propose de comparer cette proportion \\(p\\) à une valeur de référence \\(p_0\\). On considère un échantillon d’individus de taille \\(n\\) de cette population. La variable aléatoire \\(X_i\\) égale à \\(1\\) si l’individu \\(i\\) possède le caractère \\(C\\) suit une loi de Bernoulli \\(\\mathcal{B}(p)\\). Si \\(n\\) est suffisamment grand, on peut considérer que \\(\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(np,np(1-p))\\), d’où la fréquence empirique \\(\\hat{p}_n= \\frac{1}{n}\\sum_{i=1}^n X_i\\) suit une loi normale \\(\\mathcal{N}(p,\\frac{p(1-p)}{n})\\). Supposons qu’on désire tester \\[ H_0: p = p_0 \\text { contre } H_1: p \\neq p_0 \\] La statistique du test est donc la fréquence empirique \\(\\hat{p}_n\\) qui suit sous \\(H_0\\) la loi \\(\\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\\). Pour tester \\(H_0: p = p_0\\) on calcule la statistique du test \\[ Z_0 = \\frac{\\hat{p}_n - p_0}{\\sqrt{ \\frac{p_0(1-p_0)}{n}}} \\] qui suit approximativement la loi \\(\\mathcal{N}(0,1)\\) si l’hypothèse nulle est vraie. Critère de rejet de \\(H_0:p = p_0\\) (test bilatéral, grand échantillon) On rejette \\(H_0:p = p_0\\) si \\[|z_0| &gt; z_{1-\\alpha/2}\\] où \\(z_{1-\\alpha/2}\\) est le quantile d’ordre \\(1-\\alpha/2\\) de la loi \\(\\mathcal{N}(0,1)\\). On situe comme d’habitude les régions critiques des hypothèses alternatives unilatérales. Il y a dualité entre intervalles de confiance et tests d’hypothèses↩ ou comparer la valeur \\(p\\) (p-value) avec le seuil du test. Des explications de cette méthode seront présentées plus tard.↩ supposition raisonnable dans de nombreux cas↩ Si la supposition est déraisonnable, on peut spécifier une autre loi (exponentielle, Weibull, etc..) et utiliser une méthode générale de construction de test pour obtenir une procédure valide, ou on peut utiliser un des tests non paramétriques qui sont valides pour toute distribution.↩ "],
["exercices-4.html", "Exercices", " Exercices Exercice 5.1 Une entreprise commercialise des pieds de lit de type boule. Pour ces pieds on utilise une bague en matière plastique de diamètre intérieur \\(x\\). On définit ainsi une variable aléatoire \\(X\\) qui à chaque bague tirée au hasard dans la production, associe son diamètre intérieur \\(x\\) mesuré en millimètres. On admet que \\(X\\) suit le loi normale de moyenne \\(\\mu\\) et d’écart-type \\(0.04\\). Le fournisseur affirme que \\(\\mu=12.1 \\, mm\\). On a un doute sur cette affirmation. On prélève un échantillon de \\(64\\) pièces dans la livraison. Le diamètre intérieur moyen sur cet échantillon est de \\(12.095\\, mm\\). Que conclure au seuil de signification de \\(10\\%\\) quant au diamètre intérieur moyen des bagues? Exercice 5.2 Une usine fabrique des câbles. Un câble est considéré “conforme” si sa résistance à la rupture est supérieure à \\(3\\) tonnes. L’ingénieur responsable de la production voudrait connaître, en moyenne, la résistance à la rupture des câbles fabriqués. Il n’est, bien sûr, pas question de faire le test de rupture sur toute la production (l’usine perdrait toute sa production!). Notons \\(X\\) la variable aléatoire correspondant à la force à exercer sur le câble pour le rompre (en tonnes). Pour cette question, on supposera que \\(X \\thicksim \\mathcal{N}(\\mu, \\sigma^2)\\) et que la valeur de \\(\\sigma\\) est ici connue et égale à \\(0.38\\). Un technicien prélève donc un échantillon de \\(100\\) câbles dans la production. Avec les données de l’échantillon, le technicien obtient les résultats suivants : la résistance moyenne à la rupture des \\(100\\) câbles de l’échantillon est de \\(3.5\\) tonnes. Décrire l’expérience aléatoire, la population, la probabilité utilisé sur l’espace probabilisable et la variable aléatoire étudiée. Que représente le paramètre \\(\\mu\\)? Donner un estimateur puis une estimation de \\(\\mu\\). Peut-on dire, avec un risque d’erreur de \\(2.5\\%\\) que la résistance moyenne à la rupture de l’ensemble des câbles de la production est strictement supérieure à \\(3\\) tonnes? 2ème partie La proportion de câbles dont la résistance est supérieure à \\(3\\) tonnes dans cet échantillon est de \\(0.85\\). Décrire la nouvelle variable aléatoire étudiée \\(Y\\). Quelle est sa loi? Donner une estimation ponctuelle de la proportion \\(p\\) de câbles conformes dans la production. Peut-on dire, avec un risque d’erreur de \\(5\\%\\) que la proportion \\(p\\) de câbles conformes dans la production est strictement supérieure à \\(0.80\\)? Exercice 5.3 On utilise une nouvelle variété de pommes de terre dans une exploitation agricole. Le rendement moyen de l’ancienne variété était de \\(41.5\\) tonnes à l’hectare. La nouvelle variété est cultivée sur \\(100\\) hectares, avec un rendement moyen de \\(45\\) tonnes à l’hectare et un écart-type (échantillonnal) de \\(11.25\\). Faut-il, avec un risque d’erreur de \\(1\\%\\), favoriser la culture de la nouvelle variété? Calculer la puissance du test précédent si le “vrai” rendement moyen de la nouvelle variété est supposée égal à \\(44\\) tonnes. Calculez le risque d’erreur de deuxième espèce. Interpréter. Exercice 5.4 Un article dans le journal Growth14 a rapporté les résultats d’une étude qui a mesuré le poids (en grammes) pour les cobayes à la naissance: 421 494.6 110.7 102.4 317 447.8 879 273 279.3 452.6 373.8 96.4 241 290.9 687.6 88.8 268 258.5 456.1 90.5 81.7 296 256.5 705.7 296 227.5 296 Tester l’hypothèse que le poids moyenne est \\(300\\) grammes avec niveau de confiance \\(95\\%\\). Expliquer comment répondre à la question précédente avec un IC bilatéral pour le poids moyen. Exercice 5.5 Un article dans un journal d’agriculture15 a déterminé que la composition en acides aminés essentiels (Lysine) des repas de soja sont comme indiqué ici (g/kg): 22.2 24.7 20.9 26 27 24.8 26.5 23.8 25.6 23.9 Peut-on dire, à risque \\(1\\%\\) que la variance de la quantité de Lysine est égale à \\(1\\)? Confirmer le résultat du test avec un IC bilatéral pour cette variance. Article intitulé “Comparison of Measured and Estimated Fat-Free Weight, Fat, Potassium and Nitrogen of Growing Guinea Pigs”↩ Article intitulé “Non-Starch Polysaccharides and Broiler Performance on Diets Containing Soyabean Meal as the Sole Protein Concentrate” dans le journal “Australian Journal of Agricultural Research”↩ "],
["tests-dhypotheses-sur-deux-echantillons.html", "Chapitre 6 Tests d’hypothèses sur deux échantillons 6.1 Comparaison de deux échantillons gaussiens indépendants", " Chapitre 6 Tests d’hypothèses sur deux échantillons Il est très fréquent que l’on ait à comparer deux populations selon un critère quantitatif particulier. Par exemple: Comparer deux traitements médicaux au vu de leurs effets sur les patients. Comparer différents groupes d’étudiants au vu de leurs résultats scolaires. Comparer les fréquences d’occurrences de maladies chez les fumeurs et les non fumeurs. Statistiquement, cela signifie que l’on dispose d’observations de variables aléatoires \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) i.i.d. constituant le premier échantillon, et de variables aléatoires \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) i.i.d. constituant le deuxième échantillon. Comparer les deux échantillons revient à comparer les paramètres des lois de probabilité des \\(X_{1}\\) et des \\(X_{2}\\). Un test de comparaison de deux échantillons gaussiens indépendants consiste à supposer que les deux échantillons sont indépendants et de lois normales, et à comparer les moyennes et les variances de ces lois. Un test de comparaison de deux proportions consiste à supposer que les deux échantillons sont indépendants et de lois de Bernoulli, et à comparer les paramètres de ces lois. Un test de comparaison de deux échantillons gaussiens appariés consiste à supposer que les échantillons sont de lois normales, mais pas indépendants (en un certain sens) et à comparer les moyennes de ces lois. Il faut dans ce cas que les échantillons soient de même taille (ce dernier test n’est pas abordé dans ce cours). 6.1 Comparaison de deux échantillons gaussiens indépendants Dans cette section, on supposera que les deux échantillons sont indépendants et de lois normales et comparera leurs moyennes et leurs variances. \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) sont supposées indépendantes et de même loi \\(\\mathcal{N}(m_1,\\sigma_1^2)\\) et \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) indépendantes et de même loi \\(\\mathcal{N}(m_2,\\sigma_2^2)\\). Les \\(X_{1}\\) et \\(X_{2}\\) sont indépendantes. Les moyennes empiriques et variances estimées de deux échantillons sont notées respectivement \\(\\overline{X}_{1}, S_1^2, {S_1^*}^2, \\overline{X}_{2}, S_2^2\\) et \\({S_2^*}^2\\). Exemple 6.1 Deux groupes d’étudiants de tailles respectives \\(n_1 = 25\\) et \\(n_2 = 31\\) ont suivi le même cours de statistique et passé le même examen. Les moyennes et écarts-types empiriques des notes obtenues dans les deux groupes sont respectivement: \\[ \\begin{array}{ll}{\\text { Premier groupe: }} &amp; {\\overline{x}_{{1}}=12.8, \\quad s_{1}^{*}=3.4} \\\\ {\\text { Deuxième groupe : }} &amp; {\\overline{x}_{{2}}=11.3, \\quad s_{2}^{*}=2.9}\\end{array} \\] On suppose que les notes sont réparties dans les deux groupes selon des lois normales et qu’elles sont toutes indépendantes. Peut-on considérer que le premier groupe est meilleur que le deuxième, c’est-à-dire qu’un point et demi d’écart entre les moyennes est significatif d’une différence de niveau? La procédure à suivre consiste à tester d’abord l’égalité des variances, puis l’égalité des moyennes. 6.1.1 Test de Fisher de comparaison des variances Comparer les variances des deux échantillons, c’est tester \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2\\neq\\sigma_2^2 \\] Pour tester cette hypothèse on utilise la statistique \\[ F_0 = \\frac{{S_1^*}^2}{{S_2^*}^2} \\] Cette statistique suit la loi \\(F\\) de Fisher, avec \\(n_1-1\\) et \\(n_2-1\\) degrés de liberté si l’hypothèse \\(H_0\\) est vraie. (preuve ) Rappel de la définition de la loi de Fisher: Soit \\(U\\) et \\(V\\) deux variables aléatoires indépendantes suivant une loi de \\(\\chi^2\\) respectivement à \\(n\\) et \\(m\\) degrés de liberté. On dit que \\(F= \\frac{U/n}{V/m}\\) suit une loi de Fisher-Snedecor à \\((n,m)\\) degrés de liberté. \\(F \\thicksim \\mathcal{F}(n,m)\\) Figure 6.1: Densités de la loi de Fisher \\(F(n,m)\\) La procédure pour le test bilatéral Pour tester \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2\\neq\\sigma_2^2 \\] On calcule \\[ f_0 = \\frac{{S_1^*}^2}{{S_2^*}^2} \\] Critère de rejet de \\(H_{0}: \\sigma_1^2=\\sigma_2^2\\) On rejette \\(H_{0}: \\sigma_1^2=\\sigma_2^2\\) si \\[f_0 &gt; f_{1-\\alpha/2; \\, n_1-1,n_2-1} \\quad \\text{ ou } \\quad f_0 &lt; f_{\\alpha/2; \\, n_1-1,n_2-1}\\] où \\(f_{\\alpha/2; \\, n_1-1,n_2-1}\\) et \\(f_{1-\\alpha/2; \\, n_1-1,n_2-1}\\) sont les quantiles d’ordre \\(\\alpha/2\\) et \\(1-\\alpha/2\\) de la loi de Fisher \\(F\\) avec \\(n_1-1\\) et \\(n_2-1\\) degrés de liberté. Plusieurs tables de la loi de Fisher (selon la valeur de \\(\\alpha\\)) se trouvent dans l’annexe F. Remarque: Ces tables ne donnent que les quantiles de l’aile supérieure de la loi \\(F\\), de sorte que pour trouver \\(f_{\\alpha/2; \\, n_1-1,n_2-1}\\) on doit utiliser l’égalité \\[ f_{\\alpha/2; \\, n_1-1,n_2-1} = \\frac{1}{f_{1-\\alpha/2; \\, n_2-1,n_1-1}} \\] Dans l’exemple 6.1, \\({s_{1}^{*}}^2 &gt; {s_{2}^{*}}^2\\) et \\(\\frac{{s_{1}^{*}}^2}{{s_{2}^{*}}^2} = 1.37\\). Selon la table de la loi de Fisher dans l’annexe F, au seuil \\(\\alpha=5\\%\\), \\(f_{1-0.025; \\, 24,30}=2.14\\). On trouve que \\(f_0 = \\frac{{s_{1}^{*}}^2}{{s_{2}^{*}}^2}=1.37 &lt; f_{1-0.025; \\, 24,30}=2.14\\) donc on n’est pas dans la région de rejet de \\(H_0\\). On ne peut pas donc conclure que les variances des deux échantillons sont différentes. Des deux rapports \\(\\frac{{s_1^*}^2}{{s_2^*}^2}\\) et \\(\\frac{{s_2^*}^2}{{s_1^*}^2}\\), un seul est plus grand que \\(1\\). Or on peut montrer que pour \\(\\alpha &lt; 1/2\\), \\(f_{1-\\alpha; \\, n,m} &gt; 1\\). Donc, pour la règle de décision du rejet de \\(H_0\\), il suffit de retenir celui des deux rapports qui est supérieur à \\(1\\). La procédure pour les tests unilatéraux On peut utiliser la même statistique pour tester les hypothèses alternatives unilatérales. Comme la notation \\(X\\) et \\(Y\\) est arbitraire, on suppose que la population \\(X\\) a la plus grande variance. Donc les hypothèses du test unilatéral sont \\[ H_{0}: \\sigma_1^2=\\sigma_2^2 \\text { contre } H_{1}: \\sigma_1^2 &gt; \\sigma_2^2 \\] On rejette donc \\(H_0:\\sigma_1^2=\\sigma_2^2\\) si \\[ f_0 &gt; f_{1-\\alpha; \\, n_1-1,n_2-1} \\] 6.1.2 Tests de comparaison de moyennes Dans cette section, nous introduisons les testes sur les moyennes de deux distributions normales avec variances connues et inconnues. 6.1.2.1 Cas de variances connues Supposons que \\(X_1\\) et \\(X_2\\) sont normalement distribuées ou, si elle ne le sont pas, que les conditions du théorème central limite s’appliquent. Comparer les moyennes de deux échantillons, c’est tester \\[ H_{0}: m_1 = m_2 \\text { contre } H_{1}: m_1 \\neq m_2 \\] La procédure de ce test repose sur la distribution de la différence des moyennes des échantillons, \\(\\overline{X}_1 - \\overline{X}_2\\). On peut résumer la situation ainsi: Populations \\(X_1 \\thicksim \\mathcal{N}(m_1,\\sigma_1^2)\\) \\(X_2 \\thicksim \\mathcal{N}(m_2,\\sigma_2^2)\\) Echantillons \\(X_{11},X_{12},\\ldots,X_{1n_1}\\) \\(X_{21},X_{22},\\ldots,X_{2n_2}\\) Loi des moyennes empiriques \\(\\overline{X}_1 \\thicksim \\mathcal{N}(m_1,\\sigma_1^2/n_1)\\) \\(\\overline{X}_2 \\thicksim \\mathcal{N}(m_2,\\sigma_2^2/n_2)\\) Loi de la différence des moyennes empiriques \\(\\overline{X}_1 - \\overline{X}_2 \\thicksim \\mathcal{N}\\big(m_1-m_2,\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\\big)\\) Donc, si l’hypothèse nulle \\(H_0: m_1=m_2\\) est vraie, la statistique du test \\[\\begin{equation} Z_0 = \\frac{\\overline{X}_1 -\\overline{X}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\tag{6.1} \\end{equation}\\] suit la loi \\(\\mathcal{N}(0,1)\\). Critère de rejet de $H_{0}: Test de \\(H_0: m_1 = m_2\\) contre \\(m_1 \\neq m_2\\): On rejette \\(H_0\\) si \\(|z_0| &gt; z_{1-\\alpha/2}\\). Test de \\(H_0: m_1 \\leq m_2\\) contre \\(m_1 &gt; m_2\\): On rejette \\(H_0\\) si \\(z_0 &gt; z_{1-\\alpha}\\). Test de \\(H_0: m_1 \\geq m_2\\) contre \\(m_1 &lt; m_2\\): On rejette \\(H_0\\) si \\(z_0 &lt; z_{\\alpha}\\). 6.1.2.2 Cas de variances inconnues mais égales \\(\\sigma_1^2=\\sigma_2^2=\\sigma^2\\) (test de Student) Les variances \\(\\sigma_1^2\\) et \\(\\sigma_2^2\\) étant inconnues, on ne peut pas utiliser directement la statistique de l’équation (6.1) pour construire le test. Comme dans ce cas, \\({S_1^*}^2\\) et \\({S_2^*}^2\\) estiment tous les deux la variance commune \\(\\sigma^2\\), on peut les combiner pour obtenir une seule estimation de \\(\\sigma^2\\), soit \\(S_p^2 = \\frac{(n_1-1){S_1^*}^2 + (n_2-1){S_2^*}^2}{n_1+n_2-2}\\) Pour tester \\(H_0:m_1=m_2\\), on calcule la statistique du test \\[ T_0 = \\frac{\\overline{X}_1 -\\overline{X}_2}{S_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\] Cette statistique, sous \\(H_0\\) suit la loi de Student avec \\(n_1+n_2-2\\) degrés de libertés \\(St(n_1+n_2-2)\\). Critère de rejet de $H_{0}: Test de \\(H_0: m_1 = m_2\\) contre \\(m_1 \\neq m_2\\): On rejette \\(H_0\\) si \\(|t_0| &gt; t_{1-\\alpha/2; \\, n_1+n_2-2}\\). Test de \\(H_0: m_1 \\leq m_2\\) contre \\(m_1 &gt; m_2\\): On rejette \\(H_0\\) si \\(t_0 &gt; t_{1-\\alpha; \\, n_1+n_2-2}\\). Test de \\(H_0: m_1 \\geq m_2\\) contre \\(m_1 &lt; m_2\\): On rejette \\(H_0\\) si \\(t_0 &lt; t_{\\alpha; \\, n_1+n_2-2}\\). 6.1.2.3 Cas de variances inconnues mais inégales A priori, si le test de Fisher rejette l’égalité des variances, on ne peut pas appliquer le test sur les moyennes. En fait, le théorème central limite permet de montrer que, si \\(n_1\\) et \\(n_2\\) sont suffisamment grands (supérieurs à 30), alors la loi de \\(T_0\\) est approximativement la loi \\(\\mathcal{N}(0,1)\\) même si les deux variances sont différentes et en fait même si les deux échantillons ne sont pas de loi normale. Par conséquent, si on a beaucoup d’observations, le test de Student permet de comparer les moyennes d’échantillons issus de n’importe quelle loi de probabilité. En revanche, si on a peu d’observations, ce test ne fonctionne pas. On utilise alors d’autres tests16. La généralisation de ce problème à la comparaison des moyennes de \\(k\\) échantillons gaussiens indépendants fait l’objet d’un domaine important de la statistique appelé l’analyse de variance (Anova). comme le test de Smith-Satterthwaite ou le test d’Aspin-Welch↩ "],
["variables-aleatoires-discretes.html", "Variables Aléatoires Discrètes Notions de Probabilités Notion de variable aléatoire réelle (v.a.r.) Variables aléatoires discrètes Moments d’une variable aléatoire discrète Couple de variables aléatoires discrètes Lois usuelles discrètes", " Variables Aléatoires Discrètes Notions de Probabilités Espace Probabilisable Exemple fondamental: Considérons le jeu du lancé d’un dé. Expérience aléatoire \\(\\varepsilon\\) : “lancer un dé équilibré” \\(\\longleftarrow\\) Action. Univers: l’ensemble de tous les résultats possibles de cette expérience aléatoire \\[\\Omega= \\{1,2,3,4,5,6\\}\\] Evénements: Dans cette expérience aléatoire, on peut s’intéresser à des événements plus complexes qu’un simple résultat élémentaire. L’ensemble de parties de \\(\\Omega\\), appelé \\(\\mathcal{P}({\\Omega})\\), est l’ensemble des sous-ensembles de \\(\\Omega\\). Une famille \\(\\mathcal{A}\\) de parties (i.e. de sous ensembles) de \\(\\Omega\\). Ces parties sont appelées des événements. On dit que l’événement \\(A\\) s’est réalisé si et seulement si le résultat \\(\\Omega\\) de \\(\\Omega\\) qui s’est produit appartient à \\(A\\). Tribu: On appelle tribu sur \\(\\Omega\\), toute famille \\(\\mathcal{A}\\) de parties de \\(\\Omega\\) vérifiant: \\(\\Omega \\in \\mathcal{A}\\). si \\(A \\in \\mathcal{A}\\), alors \\(\\bar{A} \\in \\mathcal{A}\\). si \\((A_n)_{n\\in\\mathbb{N}}\\) est une suite d’éléments de \\(\\mathcal{A}\\), alors \\(\\bigcup\\limits_{n\\in\\mathbb{N}} A_n \\in \\mathcal{A}\\). \\(({\\Omega},\\mathcal{A})\\) est un espace probibilisable. Notions sur les Evénements Soit \\(({\\Omega},\\mathcal{A})\\) un espace probibilisable: L’ensemble \\(\\mathcal{A}\\) est appelé tribu des événements. Les éléments de \\(\\mathcal{A}\\) s’appellent les événements. L’événement \\(\\Omega\\) est appelé événement certain. L’événement \\(\\emptyset\\) est appelé événement impossible. Opérations sur les événements. Soient \\(A\\) et \\(B\\) deux événements: \\(\\bar{A}\\) est l’événement contraire de \\(A\\) (on note aussi \\(A^c\\)). \\(\\bar{A}={\\Omega}\\setminus A\\). \\(\\bar{A}\\) se réalise si et seulement si \\(A\\) ne se réalise pas. \\(A\\, {\\cap} \\,B\\) est l’événement &lt;&lt;\\(A\\) et \\(B\\)&gt;&gt;. \\(A\\, {\\cap} \\,B\\) se réalise lorsque les deux événements se réalisent. \\(A {\\cup} B\\) est l’événement &lt;&lt;\\(A\\) ou \\(B\\)&gt;&gt;. \\(A {\\cup} B\\) se réalise lorsque au moins un des deux événements se réalise. Incompatibilité: \\(A\\) et \\(B\\) sont incompatibles si leur réalisation simultanée est impossible: \\(A \\cap B = \\emptyset\\). Implication: \\(A\\) implique \\(B\\) signifie que si \\(A\\) se réalise, alors \\(B\\) se réalise aussi: \\(A \\subset B\\). Espace Probabilisé Soit \\(({\\Omega},\\mathcal{A})\\) un espace probabilisable. On appelle probabilité sur \\(({\\Omega},\\mathcal{A})\\), toute application \\[P : \\mathcal{A} \\rightarrow \\mathbb{R}\\] vérifiant: \\(\\forall A \\in \\mathcal{A}, P(A) \\geq 0\\). \\(P({\\Omega})=1\\). \\(\\forall (A_n)_{n\\in\\mathbb{N}^*} \\in \\mathcal{A}^{\\mathbb{N}^*}\\), une suite d’éléments de \\(\\mathcal{A}\\) deux à deux incompatibles, on a: \\[P(\\bigcup\\limits_{n\\in\\mathbb{N}^*} A_n) = \\sum_{n=1}^{+\\infty} P(A_n)\\] Le triplet \\(({\\Omega},\\mathcal{A},P)\\) est appelé espace probabilisé. Probabilité: Propriétés \\(P(\\emptyset) = 0\\). \\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )-P(A_1 \\cap A_2 )\\). Si \\(A_1\\) et \\(A_2\\) sont incompatibles, \\(A_1 \\cap A_2 = \\emptyset\\), \\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )\\). \\(P(A_1 \\cup A_2 \\cup A_3 ) = P(A_1 ) + P(A_2 ) + P(A_3 ) - P(A_1 \\cap A_2 ) - P(A_1 \\cap A_3 ) - P(A_2 \\cap A_3 )+P(A_1 \\cap A_2 \\cap A_3 )\\). \\(P(\\bar{A}) = 1-P(A)\\). \\(P(B\\setminus A)=P(B)-P(B\\cap A)\\). \\(A \\subset B \\Rightarrow P(A) \\leq P(B)\\). Probabilité uniforme sur \\(\\Omega\\) fini Soit \\(\\Omega\\) un univers fini. On dit que \\(P\\) est la probabilité uniforme sur l’espace probabilisable \\(({\\Omega},P({\\Omega}))\\) si: \\[\\forall {\\omega},{\\omega}&#39; \\in {\\Omega}, \\quad \\quad P(\\{{\\omega}\\})=P(\\{{\\omega}&#39;\\})\\] On dit aussi qu’il y a équiprobabilité des événements élémentaires. Soit \\(({\\Omega}, \\mathcal{P}({\\Omega}), P)\\) un espace probabilisé fini. Si \\(P\\) est la probabilité uniforme, alors \\[\\forall A \\in \\mathcal{A}, \\quad \\quad P(A)=\\frac{Card(A)}{Card({\\Omega})}\\] Probabilité conditionnelle Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé et \\(B \\in \\mathcal{A}\\) tel que \\(P(B) &gt; 0\\). L’application \\(P_B\\) définie sur \\(\\mathcal{A}\\) par: \\[P_B(A) = P(A|B) =\\frac{P(A\\cap B)}{P(B)}, \\quad \\quad \\forall A \\in \\mathcal{A}\\] est une probabilité sur \\(({\\Omega}, \\mathcal{A})\\); elle est appelée la probabilité conditionnelle sachant \\(B\\). C’est la probabilité pour que l’événement \\(A\\) se produise sachant que l’événement \\(B\\) s’est produit. Remarque: \\((A|B)\\) n’est pas un événement! On utilise la notation \\(P(A|B)\\) par simplicité, mais c’est \\(P_B (A)\\) qui est correcte. Formule des probabilités composées: \\[P(A\\cap B) = P(A|B)P(B) = P(B|A)P(A)\\] Formule des probabilités totales: \\(\\forall A \\in \\mathcal{A}, \\quad P(A) = P(A \\cap B) + P(A \\cap \\bar{B} )\\) On appelle système complet d’événements (SCE), toute partition dénombrable de \\(\\Omega\\) formée d’éléments de \\(A\\); c-à-d tout ensemble dénombrable d’événements, deux à deux incompatibles et dont l’union dénombrable est l’événement certain. Soit \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\). On a: \\[\\forall A \\in \\mathcal{A},\\quad \\quad P(A)=\\sum_{n\\geq 0} P(A \\cap B_n)\\] Indépendance: Les événement \\(A\\) et \\(B\\) sont indépendants ssi \\(P(A\\cap B)=P(A)P(B)\\). Formule de Bayes Première formule de Bayes Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé. Pour tous événements \\(A\\) et \\(B\\) tels que \\(P(A) \\neq 0\\) et \\(P(B) \\neq 0\\), on a: \\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\] Deuxième formule de Bayes Soit \\(({\\Omega},\\mathcal{A},P)\\) une espace probabilisé et \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\) t.q. pour tout \\(n\\geq 0 \\,\\, P(B_n)\\neq 0\\). On a pour tout \\(A \\in \\mathcal{A}\\) t.q. \\(P(A)\\neq 0\\) \\[P(B_i|A) = \\frac{P(A|B_i) P(B_i)}{\\sum_{n\\geq 0} P(A|B_n) P(B_n)} \\quad \\quad \\forall i \\geq 0\\] Notion de variable aléatoire réelle (v.a.r.) Après avoir réalisé une expérience aléatoire, il arrive bien souvent qu’on s’intéresse plus à une fonction du résultat qu’au résultat lui-même. Expliquons ceci au moyen des exemples suivants: lorsqu’on joue au dés, certains jeux accordent de l’importance à la somme obtenue sur deux dés, 7 par exemple, plutôt qu’à la question de savoir si c’est la paire (1,6) qui est apparue, ou (2,5), (3,4), (4,3), (5,2) ou plutôt (6,1). Dans le cas du jet d’une pièce, il peut être plus intéressant de connaître le nombre de fois où le côté pile est apparue plutôt que la séquence détaillée des jets pile et face. Ces grandeurs auxquelles on s’intéresse sont en fait des fonctions réelles définies sur l’ensemble fondamental et sont appelées variables aléatoires. Du fait que la valeur d’une variable aléatoire est déterminée par le résultat de l’expérience, il est possible d’attribuer une probabilité aux différentes valeurs que la variable aléatoire peut prendre. Soient \\(\\varepsilon\\) une expérience aléatoire et \\((\\Omega,\\mathcal{A},P)\\) un espace probabilisé lié à cette expérience. Dans de nombreuses situations, on associe à chaque résultat \\(\\omega \\in \\Omega\\) un nombre réel noté \\(X(\\omega)\\); on construit ainsi une application \\(X : \\Omega \\rightarrow \\mathbb{R}\\). Historiquement, \\(\\varepsilon\\) était un jeu et \\(X\\) représentait le gain du joueur. Exemple: Un joueur lance un dé équilibré à 6 faces numérotées de 1 à 6, et on observe le numéro obtenu. Si le joueur obtient 1, 3 ou 5, il gagne 1 euro. S’il obtient 2 ou 4, il gagne 5 euros. S’il obtient 6, il perd 10 euros. Selon l’expérience aléatoire (lancer d’un dé équilibré) l’ensemble fondamental est \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{A} = \\mathcal{P}(\\Omega)\\) et \\(P\\) l’équiprobabilité sur \\((\\Omega,\\mathcal{A})\\). Soit \\(X\\) l’application de \\(\\Omega\\) dans \\(\\mathbb{R}\\) qui à tout \\(\\omega \\in \\Omega\\) associe le gain correspondant. On a donc \\(X(1) = X(3) = X(5) = 1\\) \\(X(2) = X(4) = 5\\) \\(X(6) = -10\\) On dit que \\(X\\) est une variable aléatoire sur \\(\\Omega\\). On peut s’intéresser à la probabilité de gagner 1 euro, c’est-à-dire d’avoir \\(X(\\omega) = 1\\), ce qui se réalise si et seulement si \\(\\omega \\in \\{1,3,5\\}\\). La probabilité cherchée est donc égale à \\(P(\\{1,3,5\\}) = 1/2\\). On écrira aussi \\(P(X=1) = 1/2\\). On pourra donc considérer l’événement : \\[\\{X=1\\} = \\{\\omega \\in \\Omega / X(\\omega) = 1\\} = \\{\\omega \\in \\Omega / X(\\omega) \\in \\{1\\}\\} = X^{-1} (\\{1\\}) = \\{1,3,5\\}.\\] On aura du même \\(P(X=5) = 1/3\\) et \\(P(X=-10) = 1/6\\). Ce que l’on peut présenter dans un tableau \\(x_i\\) \\(-10\\) \\(1\\) \\(5\\) \\(p_i=P(X = x_i)\\) \\(1/6\\) \\(1/2\\) \\(1/3\\) Cela revient à considérer un nouvel ensemble d’événements élémentaires: \\[\\Omega_X = X(\\Omega)= \\{-10,1,5\\}\\] et à munir cet ensemble de la probabilité \\(P_X\\) définie par le tableau des \\(P(X=x_i)\\) ci dessus. Cette nouvelle probabilité s’appelle loi de la variable aléatoire X. Remarquer que \\[P(\\bigcup_{x_i \\in \\Omega_X} \\{X=x_i\\}) = \\sum_{x_i \\in \\Omega_X} P(X=x_i) = 1\\] Dans ce chapitre, nous traitons le cas où \\(X(\\Omega)\\) est dénombrable. La variable aléatoire est alors dite discrète. Sa loi de probabilité, qui peut être toujours définie par sa fonction de répartition, le sera plutôt par les probabilités individuelles. Nous définirons les deux caractéristiques numériques principales d’une variable aléatoire discrète, l’espérance caractéristique de valeur centrale, et la variance, caractéristique de dispersion. Nous définirons aussi les couples de variables aléatoires. Variables aléatoires discrètes Définition, loi de probabilité Définition 6.1 On dit qu’une variable aléatoire réelle (v.a.r.) \\(X\\) est discrète (v.a.r.d.) si l’ensemble des valeurs que prend \\(X\\) est fini ou infini dénombrable. Si on suppose \\(X(\\Omega)\\) l’ensemble des valeurs de \\(X\\) qui admet un plus petit élément \\(x_1\\). Alors la v.a.r.d. \\(X\\) est entièrement définie par: L’ensemble \\(X(\\Omega)\\) des valeurs prises par \\(X\\), rangées par ordre croissant: \\(X(\\Omega) = \\{x_1, x_2,\\ldots,x_i,\\ldots\\}\\) avec \\(x_1 \\leq x_2 \\leq \\ldots \\leq x_i \\leq \\ldots\\). La loi de probabilité définie sur \\(X(\\Omega)\\) par \\[p_i = P(X=x_i) \\,\\,\\,\\,\\, \\forall \\,\\, i=1,2,\\ldots\\] Remarques: Soit \\(B\\) un ensemble de \\(\\mathbb{R}\\), \\[P(X \\in B) = \\sum_{i / x_i \\in B} p(x_i)\\] En particulier \\[P( a &lt; X \\leq b) = \\sum_{i / a &lt; x_i \\leq b} p(x_i)\\] Bien sûr tous les \\(p(x_i)\\) sont positives et \\(\\sum_{i=1}^{\\infty} p(x_i) =1\\). Si \\(X\\) ne prend qu’un petit nombre de valeurs, cette loi est généralement présentée dans un tableau. Fonction de répartition d’une variable aléatoire discrète Définition 6.2 On appelle fonction de répartition de la v.a. \\(X\\), qu’on note \\(F(a)\\) de la v.a.r.d. \\(X\\), ou \\(F_X(a)\\), la fonction définie pour tout réel \\(a\\), \\(-\\infty &lt; a &lt; \\infty\\), par \\[F(a)=P(X \\leq a)=\\sum_{i / x_{i}\\leq a} P(X=x_{i})\\] Cette valeur représente la probabilité de toutes les réalisations inférieures ou égales au réel \\(a\\). Propriétés: Voici quelques propriétés de cette fonction: C’est une fonction en escalier (constante par morceaux). \\(F(a) \\leq 1\\) car c’est une probabilité. \\(F(a)\\) est continue à droite. \\(\\lim\\limits_{a\\to - \\infty} F(a) = 0\\) et \\(\\lim\\limits_{a\\to\\infty} F(a) = 1\\) La fonction de répartition caractérise la loi de \\(X\\), autrement dit: \\(F_{X} = F_{Y}\\) si et seulement si les variables aléatoires \\(X\\) et \\(Y\\) ont la même loi de probabilité. Fonction de répartition et probabilités sur \\(X\\) Tous les calculs de probabilité concernant \\(X\\) peuvent être traités en termes de fonction de répartition. Par exemple, \\[P(a &lt; X \\leq b) = F(b) - F(a) \\quad \\quad \\text{pour tout } a &lt; b\\] On peut mieux s’en rendre compte en écrivant \\(\\{X \\leq b\\}\\) comme union des deux événements incompatibles \\(\\{X \\leq a\\}\\) et \\(\\{ a &lt; X \\leq b\\}\\), soit \\[\\{X \\leq b\\} = \\{X \\leq a\\} \\cup \\{ a &lt; X \\leq b\\}\\] et ainsi \\[P(X \\leq b) = P(X \\leq a) + P(a &lt; X \\leq b)\\] ce qui établit l’égalité ci dessus. On peut déduire de \\(F\\) les probabilités individuelles par: \\[p_{i}=F(x_{i})-F(x_{i-1})\\quad \\quad \\text{pour } 1 \\leq i \\leq n\\] Exemple: On joue trois fois à pile ou face. Soit \\(X\\) la variable aléatoire “nombre de pile obtenus”. Ici \\(\\Omega=\\{P, F\\}^3\\), et donc \\[X(\\Omega)=\\{0, 1, 2, 3\\}.\\] On a \\(card(\\Omega)=2^3=8\\). Calculons par exemple \\(P(X=1)\\), c’est à dire la probabilité d’avoir exactement une pile. \\[X^{-1}(1)=\\{(P, F, F), (F, P, F), (F, F, P) \\}\\] D’où \\(P(X=1)=\\frac{3}{8}\\). En procédant de la même façon, on obtient la loi de probabilité de \\(X\\): \\(k\\) \\(0\\) \\(1\\) \\(2\\) \\(k\\) \\(P(X = k)\\) \\(\\frac{1}{8}\\) \\(\\frac{3}{8}\\) \\(\\frac{3}{8}\\) \\(\\frac{1}{8}\\) La fonction de répartition de \\(X\\) est donc donnée par: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 1/8 &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1/2 &amp; \\quad \\text{si $1 \\leq x &lt; 2$}\\\\ 7/8 &amp; \\quad \\text{si $2 \\leq x &lt; 3$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 3$}\\\\ \\end{array} \\right.\\] Le graphe de cette dernière est représentée dans la figure suivante: Exemple: Soit \\(A\\) un événement quelconque. On appelle variable aléatoire indicatrice de cet événement \\(A\\), la variable aléatoire définie par: \\[X(\\omega) = \\left\\{ \\begin{array}{l l} 1 &amp; \\quad \\text{si $\\omega \\in A$}\\\\ 0 &amp; \\quad \\text{si $\\omega \\in \\bar{A}$}\\\\ \\end{array} \\right.\\] et notée \\(X=1_A\\). Ainsi: \\[P(X=1)=P(A)=p\\] \\[P(X=0)=P(\\bar{A})=1-p\\] La fonction de répartition de \\(X\\) est donc donnée par: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 1-p &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}\\\\ \\end{array} \\right.\\] On peut prendre par exemple le cas d’un tirage d’une boule dans une urne contenant 2 boules blanches et 3 boules noires. Soit \\(A:\\text{&quot;obtenir une boule blanche&quot;}\\) et \\(X\\) la variable indicatrice de \\(A\\). La loi de probabilité de \\(X\\) est alors \\(k\\) \\(0\\) \\(1\\) \\(P(X = k)\\) \\(\\frac{3}{5}\\) \\(\\frac{2}{5}\\) et sa fonction de répartition est: \\[F(x) = \\left\\{ \\begin{array}{l l} 0 &amp; \\quad \\text{si $x&lt;0$}\\\\ 3/5 &amp; \\quad \\text{si $0 \\leq x &lt; 1$}\\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}\\\\ \\end{array} \\right.\\] Moments d’une variable aléatoire discrète Espérance mathématique Définition 6.3 Pour une variable aléatoire discrète \\(X\\) de loi de probabilité \\(p(.)\\), on définit l’espérance de \\(X\\), notée \\(E(X)\\), par l’expression \\[E(X)=\\sum_{i \\in \\mathbb{N}} x_{i} p(x_i)\\] En termes concrets, l’espérance de \\(X\\) est la moyenne pondérée des valeurs que \\(X\\) peut prendre, les poids étant les probabilités que ces valeurs soient prises. Reprenons l’exemple où on joue 3 fois à pile ou face. L’espérance de \\(X=\\)“nombre de pile obtenus” est égal à: \\[E(X)=0 \\times \\frac{1}{8}+1 \\times \\frac{3}{8}+2 \\times \\frac{3}{8}+3 \\times \\frac{1}{8}=1.5\\] Dans le cas de la loi uniforme sur \\(X(\\Omega)=\\{x_{1},\\ldots, x_{k}\\}\\), c’est à dire avec équiprobabilité de toutes les valeurs \\(p_{i}=1/k\\), on obtient: \\[E(X)=\\frac{1}{k} \\sum_{i=1}^k x_{i}\\] et dans ce cas \\(E(X)\\) se confond avec la moyenne arithmétique simple \\(\\bar{x}\\) des valeurs possibles de \\(X\\). Pour le jet d’un dé équilibré par exemple: \\[E(X)=\\frac{1}{6} \\sum_{i=1}^6 i=\\frac{7}{2}=3.5\\] Espérance d’une fonction d’une variable aléatoire Théorème 6.1 (Théorème du transfert) Si X est une variable aléatoire discrète pouvant prendre ses valeurs parmi les valeurs \\(x_i\\), \\(i \\geq 1\\), avec des probabilités respectives \\(p(x_i)\\), alors pour toute fonction réelle \\(g\\) on a \\[E(g(X)) = \\sum_i g(x_i)p(x_i)\\] Exemple 6.2 Soit \\(X\\) une variable aléatoire qui prend une des trois valeurs \\(\\{-1,0,1\\}\\) avec les probabilités respectives \\[P(X=-1) = 0.2 \\quad \\quad P(X=0)=0.5 \\quad \\quad P(X=1) = 0.3\\] Calculer \\(E(X^2)\\). Solution: Première approche: Soit \\(Y=X^2\\). La distribution de \\(Y\\) est donnée par \\[\\begin{aligned} P(Y=1) &amp;= P(X=-1) + P(X=1) = 0.5 \\\\ P(Y=0) &amp;= P(X=0) = 0.5 \\end{aligned}\\] Donc \\[E(X^2)=E(Y) = 1(0.5) + 0(0.5) = 0.5\\] Deuxième approche: En utilisant le théorème \\[\\begin{aligned} E(X^2) &amp;= (-1)^2(0.2) + 0^2(0.5) + 1^2 (0.3) \\\\ &amp;= 1(0.2+0.3)+0(0.5)=0.5 \\end{aligned}\\] Remarquer que \\[0.5=E(X^2) \\neq (E(X))^2 = 0.01\\] Linéarité de l’espérance Propriétés de l’espérance \\(E(X+a)=E(X)+a, \\quad a \\in \\mathbb{R}\\) résultat qui se déduit de: \\[\\sum_{i}p_{i}(x_{i}+a)= \\sum_{i}p_{i}x_{i}+\\sum_{i}ap_{i}=\\sum_{i}p_{i}x_{i}+a \\sum_{i}p_{i}=\\sum_{i}p_{i}x_{i}+a\\] \\(E(aX)=aE(X), \\quad a\\in \\mathbb{R}\\) il suffit d’écrire: \\[\\sum_{i}p_{i}a x_{i}=a\\sum_{i}p_{i}x_{i}\\] \\(E(X+Y)=E(X)+E(Y)\\), \\(X\\) et \\(Y\\) étant deux variables aléatoire. On peut résumer ces trois propriétés en disant que l’espérance mathématique est linéaire: \\[E(\\lambda X + \\mu Y)= \\lambda E(X)+\\mu E(Y), \\quad \\forall \\lambda \\in \\mathbb{R}, \\, \\forall \\mu \\in \\mathbb{R}.\\] Variance Définition 6.4 La variance est un indicateur mesurant la dispersion des valeurs \\(x_{i}\\) que peut prendre la v.a. \\(X\\) et son espérance \\(E(X)\\). On appelle variance de X, que l’on note \\(V(X)\\), la quantité \\[V(X)=E\\big[ (X-E(X))^2 \\big]\\] lorsque cette quantité existe. C’est l’espérance mathématique du carré de la v.a. centrée \\(X-E(X)\\). On peut établir une autre formule pour le calcul de \\(V(X)\\): \\[V(X)=E(X^2)-E^2(X)\\] Or: \\[\\begin{aligned} V(X)&amp;= E\\left[X^2-2XE(X)+E^2(X)\\right] \\\\ &amp;=E(X^2)-E[2XE(X)]+ E[E^2(X)]\\\\ &amp;=E(X^2)-2E^2(X)+E^2(X) \\\\ &amp;=E(X^2)-E^2(X) \\end{aligned}\\] On cherche \\(V(X)\\) où \\(X\\) est le nombre obtenu lors du jet d’un dé équilibré. On a vu dans l’exemple que \\(E(X) = \\frac{7}{2}\\). De plus, \\[\\begin{aligned} E(X^2) &amp;= 1^2 \\bigg(\\frac{1}{6}\\bigg) + 2^2 \\bigg(\\frac{1}{6}\\bigg) + 3^2 \\bigg(\\frac{1}{6}\\bigg) + 4^2 \\bigg(\\frac{1}{6}\\bigg) + 5^2 \\bigg(\\frac{1}{6}\\bigg) + 6^2 \\bigg(\\frac{1}{6}\\bigg) \\\\ &amp;=\\bigg(\\frac{1}{6}\\bigg) (91) = \\frac{91}{6}.\\end{aligned}\\] Et donc \\[V(X) = \\frac{91}{6} - \\bigg(\\frac{7}{2}\\bigg)^2 = \\frac{35}{12}\\] Propriétés de la variance \\(V(X) \\geq 0\\) \\(V(X+a)=V(X)\\) en effet: \\[\\begin{aligned} V(X+a) &amp;= E\\big[\\left[X+a-E(X+a)\\right]^2\\big] \\\\ &amp;=E\\big[\\left[X+a-E(X)-a\\right]^2\\big] \\\\ &amp;=E\\big[\\left[X-E(X)\\right]^2\\big] \\\\ &amp;=V(X). \\end{aligned}\\] \\(V(aX)=a^2V(X)\\) en effet: \\[\\begin{aligned} V(aX) &amp;= E\\big[\\left[aX-E(aX)\\right]^2\\big] \\\\ &amp;=E\\big[\\left[aX-aE(X)\\right]^2\\big] \\\\ &amp;=E\\big[a^2\\left[X-E(X)\\right]^2\\big] \\\\ &amp;=a^2\\big[E\\left[X-E(X)\\right]^2\\big] \\\\ &amp;= a^2V(X). \\end{aligned}\\] Ecart-type Définition 6.5 La racine carrée de \\(V(X)\\) est appelée l’écart-type de \\(X\\), qui se note \\(\\sigma_{X}\\). On a \\[\\sigma_{X} = \\sqrt{V(X)}\\] \\(\\sigma_{X}\\) s’exprime dans les mêmes unités de mesure que la variable aléatoire \\(X\\). A noter: L’écart type sert à mesurer la dispersion d’un ensemble de données. Plus il est faible, plus les valeurs sont regroupées autour de la moyenne. Exemple: La répartition des notes d’une classe. Plus l’écart type est faible, plus la classe est homogène. L’espérance et l’écart-type sont reliés par l’inégalité de Bienaymé-Tchebychev. Inégalité de Bienaymé-Tchebychev Théorème 6.2 Soit \\(X\\) une variable aléatoire d’espérance \\(\\mu\\) et de variance \\(\\sigma^2\\). Pour tout \\(\\varepsilon &gt; 0\\), on a l’inégalité suivante: \\[P\\left(|X-E(X)| \\geq \\varepsilon \\right) \\leq \\frac{\\sigma^2}{\\varepsilon^2}\\] On peut l’écrire autrement. Soit \\(k=\\varepsilon/\\sigma\\). \\[P\\left(|X-E(X)| \\geq k\\sigma \\right) \\leq \\frac{1}{k^2}\\] Importance: Cette inégalité relie la probabilité pour \\(X\\) de s’écarter de sa moyenne \\(E(X)\\), à sa variance qui est justement un indicateur de dispersion autour de la moyenne de la loi. Elle montre quantitativement que “plus l’écart type est faible, plus la probabilité de s’écarter de la moyenne est faible”. Théorème 6.3 (Inégalité de Markov) Soit \\(X\\) une variable aléatoire à valeur non négatives. Pour tout réel \\(a &gt; 0\\) \\[P(X&gt;a) \\leq \\frac{E(X)}{a}\\] Moments non centrés et centrés On appelle moment non centré d’ordre \\(r \\in \\mathbb{N^*}\\) de \\(X\\) la quantité, lorsqu’elle existe: \\[m_{r}(X)=\\sum_{i \\in \\mathbb{N} } x_{i}^r p(x_{i})=E(X^r).\\] Le moment centré d’ordre \\(r \\in \\mathbb{N^*}\\) est la quantité, lorsqu’elle existe: \\[\\mu_{r}(X)=\\sum_{i \\in \\mathbb{N} } p_{i}\\left[x_{i}-E(X)\\right]^r=E\\left[X-E(X)\\right]^r.\\] Les premiers moments sont: \\[m_{1}(X)=E(X), \\quad \\mu_{1}(X)=0\\] \\[\\mu_{2}(X)=V(X)=m_{2}(X)-m_{1}^2(X)\\] Couple de variables aléatoires discrètes Considérons deux variables aléatoires discrètes \\(X\\) et \\(Y\\). Il nous faut pour modéliser le problème une fonction qui nous donne la probabilité que \\((X = x_i )\\) en même temps que \\((Y = y_j )\\). C’est la loi de probabilité conjointe. Soit \\(X\\) et \\(Y\\) deux variables aléatoires réelles discrètes, définies sur un espace probabilisé \\((\\Omega,\\mathcal{A},P)\\) et que \\[\\begin{aligned} X(\\Omega) &amp;= \\{x_1,x_2,\\ldots,x_l\\} \\\\ Y(\\Omega) &amp;= \\{y_1,y_2,\\ldots,y_k\\} \\\\ &amp; \\quad (l \\text{ et } k \\in \\mathbb{N})\\end{aligned}\\] La loi du couple \\((X,Y)\\), dite loi de probabilité conjointe ou simultanée, est entièrement définie par les probabilités: \\[p_{ij} = P(X=x_i;Y=y_j) = P(\\{X=x_i\\}\\cap\\{Y=y_j\\})\\] On a \\[p_{ij} \\geq 0 \\quad \\text{et} \\quad \\sum_{i=1}^{l} \\sum_{j=1}^{k} p_{ij} = 1\\] Le couple \\((X,Y)\\) s’appelle variable aléatoire à deux dimensions et peut prendre \\(l\\times k\\) valeurs. Table de probabilité conjointe Les probabilités \\(p_{ij}\\) peuvent être présentées dans un tableau à deux dimensions qu’on appelle table de probabilité conjointe: \\(X\\backslash Y\\) \\(y_1\\) \\(y_2\\) \\(\\ldots\\) \\(y_j\\) \\(\\ldots\\) \\(y_k\\) \\(x_1\\) \\(p_{11}\\) \\(p_{12}\\) \\(p_{1j}\\) \\(p_{1k}\\) \\(x_2\\) \\(p_{21}\\) \\(p_{22}\\) \\(p_{2j}\\) \\(p_{2k}\\) \\(\\vdots\\) \\(x_i\\) \\(p_{i1}\\) \\(p_{i2}\\) \\(p_{ij}\\) \\(p_{ik}\\) \\(\\vdots\\) \\(x_l\\) \\(p_{l1}\\) \\(p_{l2}\\) \\(p_{lj}\\) \\(p_{lk}\\) A la première ligne figure l’ensemble des valeurs de \\(Y\\) et à la première colonne figure l’ensemble des valeurs de \\(X\\). La probabilité \\(p_{ij} = P(X=x_i;Y=y_j)\\) est à l’intersection de la \\(i^{e}\\) et de la \\(j^{e}\\) colonne. Lois marginales Lorsqu’on connaît la loi conjointe des variables aléatoires \\(X\\) et \\(Y\\), on peut aussi s’intéresser à la loi de probabilité de \\(X\\) seule et de \\(Y\\) seule. Ce sont les lois de probabilité marginales. Loi marginale de \\(X\\): \\[p_{i.} = P(X=x_i) = P[\\{X=x_i\\}\\cap \\Omega] = \\sum_{j=1}^k p_{ij} \\quad \\quad \\forall \\, i=1,2,\\ldots,l\\] Loi marginale de \\(Y\\): \\[p_{.j} = P(Y=y_j) = P[ \\Omega \\cap \\{Y=y_j\\}] = \\sum_{i=1}^l p_{ij} \\quad \\quad \\forall \\, j=1,2,\\ldots,k\\] On peut calculer les lois marginales directement depuis la table de la loi conjointe. La loi marginale de \\(X\\) est calculée en faisant les totaux par ligne, tandis que celle de \\(Y\\) l’est en faisant les totaux par colonne. C’est le fait que les lois de \\(X\\) et \\(Y\\) individuellement puissent être lues dans les marges du tableau qui leur vaut leur nom de lois marginales. \\(X\\backslash Y\\) \\(y_1\\) \\(y_2\\) \\(\\ldots\\) \\(y_j\\) \\(\\ldots\\) \\(y_k\\) Marginale de \\(X\\) \\(x_1\\) \\(p_{11}\\) \\(p_{12}\\) \\(p_{1j}\\) \\(p_{1k}\\) \\(p_{1.}\\) \\(x_2\\) \\(p_{21}\\) \\(p_{22}\\) \\(p_{2j}\\) \\(p_{2k}\\) \\(p_{2.}\\) \\(\\vdots\\) \\(x_i\\) \\(p_{i1}\\) \\(p_{i2}\\) \\(p_{ij}\\) \\(p_{ik}\\) \\(p_{i.}\\) \\(\\vdots\\) \\(x_l\\) \\(p_{l1}\\) \\(p_{l2}\\) \\(p_{lj}\\) \\(p_{lk}\\) \\(p_{l.}\\) Marginale de \\(Y\\) \\(p_{.1}\\) \\(p_{.2}\\) \\(p_{.j}\\) \\(p_{.k}\\) \\(1\\) On tire au hasard 3 boules d’une urne contenant 3 boules rouges, 4 blanches et 5 noires. \\(X\\) et \\(Y\\) désignent respectivement le nombre de boules rouges et celui de boules blanches tirées. Déterminer la loi de probabilité conjointe du couple \\((X,Y)\\) ainsi que les lois marginales de \\(X\\) et de \\(Y\\). Lois conditionnelles Pour chaque valeur \\(y_j\\) de \\(Y\\) telle que \\(p_{.j} = P(Y=y_j) \\neq 0\\) on peut définir la loi conditionnelle de \\(X\\) sachant \\(Y=y_j\\) par \\[p_{i/j} = P(X=x_i / Y=y_j) = \\frac{P(X=x_i;Y=y_j)}{P(Y=y_j)} = \\frac{p_{ij}}{p_{.j}} \\quad \\quad \\forall i = 1,2,\\ldots,l\\] De même on définit la loi de \\(Y\\) sachant \\(X=x_i\\) par \\[p_{j/i} = P(Y=y_j / X=x_i) = \\frac{P(X=x_i;Y=y_j)}{P(X=x_i)} = \\frac{p_{ij}}{p_{i.}} \\quad \\quad \\forall j = 1,2,\\ldots,k\\] Indépendance de variables aléatoires Théorème 6.4 On dit que deux v.a.r.d sont indépendantes si et seulement si \\[P(X=x_i;Y=y_j) = P(X=x_i) P(Y=y_j) \\quad \\quad \\forall \\, i = 1,2,\\ldots,l \\text{ et } j = 1,2,\\ldots,k\\] On montre que \\[P(\\{X\\in A\\} \\cap \\{Y \\in B\\}) = P(\\{X\\in A\\}) P(\\{Y \\in B\\}) \\quad \\quad \\forall \\,\\, A \\text{ et } B \\in \\mathcal{A}\\] Propriétés Soit deux v.a.r.d. \\(X\\) et \\(Y\\), \\(E(X+Y)=E(X)+E(Y)\\) Si \\(X\\) et \\(Y\\) sont indépendantes alors \\(E(XY)=E(X)E(Y)\\). Mais la réciproque n’est pas toujours vraie. Covariance Soit \\(X\\) et \\(Y\\) deux v.a.r.d. On appelle covariance de \\(X\\) et de \\(Y\\) la valeur si elle existe de \\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))] = \\sum_i \\sum_j (x_i-E(X))(y_j-E(Y)) p_{ij}\\] qu’on peut calculer en utilisant la formule suivante \\[Cov(X,Y) = E(XY) - E(X)E(Y)\\] Propriétés \\(Cov(X,Y)=Cov(Y,X)\\) \\(Cov(aX_1+bX_2,Y) = a Cov(X_1,Y) + b Cov(X_2,Y)\\) \\(V(X+Y)= V(X) + V(Y) + 2 Cov(X,Y)\\) Si \\(X\\) et \\(Y\\) sont indépendantes alors \\(Cov(X,Y) = 0\\) (la réciproque n’est pas vraie) \\(V(X+Y) = V(X) + V(Y)\\) (la réciproque n’est pas vraie) Coefficient de corrélation linéaire On appelle coefficient de corrélation linéaire de \\(X\\) et de \\(Y\\) la valeur définie par \\[\\rho = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\] On peut montrer que \\[-1 \\leq \\rho(X,Y) \\leq 1\\] Pour le montrer on peut partir du fait que la variance est toujours positive ou nulle. Donc \\(V(\\frac{X}{\\sigma_X} + \\frac{Y}{\\sigma_Y}) \\geq 0\\) et \\(V(\\frac{X}{\\sigma_X} - \\frac{Y}{\\sigma_Y}) \\geq 0\\). Interprétation de \\(\\rho\\) Le coefficient de corrélation est une mesure du degré de linéarité entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proches de \\(1\\) ou \\(-1\\) indiquent une linéarité quasiment rigoureuse entre \\(X\\) et \\(Y\\). Les valeurs de \\(\\rho\\) proche de 0 indiquent une absence de toute relation linéaire. Lorsque \\(\\rho(X,Y)\\) est positif, \\(Y\\) a tendance à augmenter si \\(X\\) en fait autant. Lorsque \\(\\rho(X,Y) &lt; 0\\), \\(Y\\) a tendance à diminuer si \\(X\\) augmente. Si \\(\\rho(X,Y) =0\\), on dit que ces deux statistiques sont non corrélées. Lois usuelles discrètes Loi uniforme discrète \\(\\mathcal{U}(n)\\) Définition 6.6 Une distribution de probabilité suit une loi uniforme lorsque toutes les valeurs prises par la variable aléatoire sont équiprobables. Si \\(n\\) est le nombre de valeurs différentes prises par la variable aléatoire alors on a: \\[\\label{eq:unif} P(X=x_i)=\\frac{1}{n} \\qquad \\forall \\, i \\in \\{1,\\ldots, n\\}\\] Exemple: La distribution des chiffres obtenus au lancer de dé (si ce dernier est non pipé) suit une loi uniforme dont la loi de probabilité est la suivante : \\(x_i\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(6\\) \\(P(X = x_i)\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) \\(\\frac{1}{6}\\) Moments de loi uniforme discrète Dans le cas particulier d’une loi uniforme discrète où chaque valeur de la variable aléatoire \\(X\\) correspond à son rang, i.e. \\(x_i=i \\, \\, \\forall i \\in \\{1,\\ldots, n\\}\\), on a: \\[E(X)=\\frac{n+1}{2} \\quad \\text{et} \\quad V(X)=\\frac{n^2-1}{12}\\] La démonstration de ces résultats est établie en utilisant les égalités (cf. Annexe) \\[\\sum_{i=1}^n i=\\frac{n(n+1)}{2} \\quad \\text{et} \\quad \\sum_{i=1}^n i^2=\\frac{n(n+1)(2n+1)}{6}.\\] En revenant à l’exemple du lancer du dé de cette section, on peut calculer directement les moments de \\(X\\): \\[E(X)=\\frac{6+1}{2}=3.5\\] et \\[V(X)=\\frac{6^2-1}{12}=\\frac{35}{12}\\simeq 2.92.\\] Loi de Bernoulli \\(\\mathcal{B}(p)\\) Définition 6.7 On réalise une expérience dont le résultat sera interprété soit comme un succès soit comme un échec. On définit alors la variable aléatoire \\(X\\) en lui donnant la valeur 1 lors d’un succès et 0 lors d’un échec (variable indicatrice). La loi de probabilité de \\(X\\) est alors \\[\\begin{align} &amp;p(1)=P(X=1)=p \\tag{6.2} \\\\ &amp;p(0)=P(X=0)= 1-p=q \\notag \\end{align}\\] où \\(p\\) est la probabilité d’un succès, \\(0 \\leq p \\leq 1\\). Une variable aléatoire \\(X\\) est dite de Bernoulli \\(X \\sim \\mathcal{B} \\left({p}\\right)\\) s’il existe un nombre \\(p \\, \\in \\, ]0,1[\\) tel que la loi de probabilité de \\(X\\) soit donnée par (6.2). La fonction de répartition est définie par: \\[F(x) = \\left\\{ \\begin{array}{ll} 0 &amp; \\quad \\text{si $x &lt; 0$} \\\\ 1 - p &amp; \\quad \\text{si $0 \\leq x &lt; 1$} \\\\ 1 &amp; \\quad \\text{si $x \\geq 1$}. \\end{array} \\right.\\] L’espérance la loi de Bernoulli est \\(p\\), en effet \\[E(X) =1 \\times P(X=1)+0 \\times P(X=0)=P(X=1)=p\\] La variance la loi de Bernoulli est \\(np\\), en effet \\[V(X) =E(X^2)-E^2(X)=p-p^2=p(1-p)=pq\\] car \\[E(X^2) =1^2\\times P(X=1)+0^2 \\times P(X=0)=P(X=1)=p\\] Loi Binomiale \\(\\mathcal{B}(n,p)\\) Décrite pour la première fois par Isaac Newton en 1676 et démontrée pour la première fois par le mathématicien suisse Jacob Bernoulli en 1713, la loi binomiale est l’une des distributions de probabilité les plus fréquemment rencontrées en statistique appliquée. Supposons qu’on exécute maintenant \\(n\\) épreuves indépendantes, chacune ayant \\(p\\) pour probabilité de succès et \\(1-p\\) pour probabilité d’échec. La variable aléatoire \\(X\\) qui compte le nombre de succès sur l’ensemble des \\(n\\) épreuves est dite variable aléatoire binomiale de paramètres \\(n\\) et \\(p\\). Une variable de Bernoulli n’est donc qu’une variable binomiale de paramètres \\((1,p)\\). Définition 6.8 Si on effectue \\(n\\) épreuves successives indépendantes où on note à chaque fois la réalisation ou non d’un certain événement \\(A\\), on obtient une suite de la forme \\(AA\\bar{A}A\\bar{A}\\ldots \\bar{A}AA\\). Soit \\(X\\) le nombre de réalisations de \\(A\\). On définit ainsi une v.a. \\(X\\) qui suit une loi binomiale de paramètres \\(n\\) et \\(p=P(A)\\), caractérisée par \\(X(\\Omega)=\\{0, 1,\\ldots, n\\}\\) : \\[\\begin{equation} P(X=k)=\\binom{n}{k}p^k (1-p)^{n-k} \\qquad 0\\leq k \\leq n \\tag{6.3} \\end{equation}\\] On écrit \\(X \\sim \\mathcal{B} \\left({n, p}\\right)\\). Donc la loi binomiale modélise le nombre de réalisations de \\(A\\) (succès) obtenues lors de la répétition indépendante et identique de \\(n\\) épreuves de Bernoulli. Pour établir (6.3) il faut remarquer que \\(\\binom{n}{k}\\) est le nombre d’échantillons de taille \\(n\\) comportant exactement \\(k\\) événements \\(A\\), de probabilité \\(p^k\\), indépendamment de l’ordre, et donc \\(n-k\\) événements \\(\\bar{A}\\), de probabilité \\((1-p)^{n-k}\\). Remarque: Il est possible d’obtenir aisément les valeurs des combinaisons de la loi binomiale en utilisant le triangle de Pascal. En utilisant la formule du binôme de Newton, on vérifie bien que c’est une loi de probabilité: \\[{\\sum_{k=0}^nP(X=k)=\\sum_{k=0}^n\\binom{n}{k} p^{k}(1-p)^{n-k}=[p+(1-p)]^n=1}\\] Exemple: On jette cinq pièces équilibrées. Les résultats sont supposés indépendants. Donner la loi de probabilité de la variable \\(X\\) qui compte le nombre de piles obtenus. Moments de la loi Binomiale Pour calculer facilement les moments de cette loi, nous allons associer à chaque épreuve \\(i\\), \\(1\\leq i \\leq n\\), une v.a. de Bernoulli (variable indicatrice sur \\(A\\)): \\[{1}_A=X_i = \\left\\{ \\begin{array}{l l} 1 &amp; \\quad \\text{si $A$ est réalisé}\\\\ 0 &amp; \\quad \\text{si $\\bar{A}$ est réalisé}\\\\ \\end{array} \\right.\\] On peut écrire alors: \\(X=\\sum_{i=1}^nX_i=X_1+X_2+\\ldots+X_n\\), ce qui nous permet de déduire aisément: \\[\\begin{aligned} E(X)&amp;=E\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^nE(X_i)=np \\\\ \\text{et} \\nonumber \\\\ V(X)&amp;=V\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^nV(X_i)=np(1-p) \\quad \\text{car les v.a. $X_i$ sont indépendantes.} \\end{aligned}\\] Le calcul direct des moments de \\(X\\) peut s’effectuer à partir de la définition générale, mais de façon beaucoup plus laborieuse: \\[\\begin{aligned} E(X)&amp;= \\sum_{k=0}^nk \\binom{n}{k} p^{k}(1-p)^{n-k}=\\sum_{k=1}^nk \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &amp;= \\sum_{k=1}^n\\frac{n!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k}= np \\sum_{k=1}^n\\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1}(1-p)^{n-k} \\\\ &amp;= np \\sum_{j=0}^{n-1}\\frac{(n-1)!}{j!(n-1-j)!}p^j (1-p)^{n-1-j} =np \\sum_{j=0}^{n-1}\\binom{n-1}{j} p^{j}(1-p)^{n-1-j} \\\\ &amp;= np [p+(1-p)]^{n-1}=np \\end{aligned}\\] Pour obtenir \\(E(X^2)\\) par un procédé de calcul identique, on passe par l’intermédiaire du moment factoriel \\(E[X(X-1)]=E(X^2)-E(X)\\): \\[\\begin{aligned} E[X(X-1)]&amp;= \\sum_{k=0}^nk(k-1) \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\ &amp;= n(n-1)p^2 \\sum_{k=2}^{n}\\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2}(1-p)^{n-k} \\\\ &amp;= n(n-1)p^2 \\sum_{j=0}^{n-2}\\binom{n-2}{j} p^{j}(1-p)^{n-2-j} \\\\ &amp;= n(n-1)p^2[p+(1-p)]^{n-2}= n(n-1)p^2 \\end{aligned}\\] On en déduit alors: \\[E(X^2)=E[X(X-1)]+E(X)= n(n-1)p^2+np,\\] puis: \\[\\begin{aligned} V(X)&amp;=n(n-1)p^2+np-(np)^2 \\\\ &amp;=n^2p^2+np(1-p)-n^2p^2 \\\\ &amp;=np(1-p). \\end{aligned}\\] Le nombre de résultats pile apparus au cours de \\(n\\) jets d’une pièce de monnaie suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/2}\\right)\\): \\[P(X=k)=\\binom{n}{k}\\left(\\frac{1}{2}\\right)^k \\left(\\frac{1}{2}\\right)^{n-k}=\\frac{\\binom{n}{k}}{2^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/2\\) et \\(V(X)=n/4\\). Le nombre \\(N\\) de boules rouges apparues au cours de \\(n\\) tirages avec remise dans une urne contenant deux rouges, trois vertes et une noire suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/3}\\right)\\): \\[P(N=k)=\\binom{n}{k}\\left(\\frac{1}{3}\\right)^k \\left(\\frac{2}{3}\\right)^{n-k}=\\binom{n}{k} \\frac{2^{n-k}}{3^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/3\\) et \\(V(X)=2n/9\\). Théorème 6.5 Si \\(X_1 \\sim \\mathcal{B} \\left({n_1, p}\\right)\\) et \\(X_2 \\sim \\mathcal{B} \\left({n_2, p}\\right)\\), les v.a. \\(X_1\\) et \\(X_2\\) étant indépendantes, alors \\(X_1+X_2 \\sim \\mathcal{B} \\left({n_1+n_2, p}\\right)\\). Ceci résulte de la définition d’une loi binomiale puisqu’on totalise ici le résultat de \\(n_1+n_2\\) épreuves indépendantes. Loi de Poisson \\(\\mathcal{P}(\\lambda)\\) La loi de Poisson est découverte au début du XIX\\(^e\\) siècle par le magistrat français Siméon-Denis Poisson. Les variables aléatoires de Poisson ont un champ d’application fort vaste, en particulier du fait qu’on peut les utiliser pour approximer des variables aléatoires binomiales de paramètres \\((n,p)\\) pour autant que \\(n\\) soit grand et \\(p\\) assez petit pour que \\(np\\) soit d’ordre de grandeur moyen. Définition 6.9 Une v.a. \\(X\\) suit une loi de Poisson de paramètre \\(\\lambda&gt;0\\) si c’est une variable à valeurs entières, \\(X(\\Omega)=\\mathbb{N}\\), donc avec une infinité de valeurs possibles, de probabilité: \\[\\label{eq:poisson} P(X=k)=e^{-\\lambda} \\frac{\\lambda^k}{k!}, \\quad k \\in \\mathbb{N}\\] Cette loi ne dépend qu’un seul paramètre réel positif \\(\\lambda\\), avec l’écriture symbolique \\(X \\sim \\mathcal{P}(\\lambda)\\). Le développement en série entière de l’exponentielle \\(e^\\lambda=\\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\\) permet de vérifier qu’il s’agit bien d’une loi de probabilité: \\[\\sum_{k=0}^{\\infty} P(X=k)=\\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!}=e^{-\\lambda}\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}=e^{-\\lambda}e^{\\lambda}=1\\] Moments de loi de Poisson Le calcul de l’espérance mathématique se déduit du développement en série entière de l’exponentielle: \\[\\begin{aligned} E(X)&amp;=\\sum_{k=0}^{\\infty} k P(X=k)=\\sum_{k=1}^{\\infty} k e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\ &amp;=e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^k}{(k-1)!}=\\lambda e^{-\\lambda} \\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!} \\\\ &amp;= \\lambda e^{-\\lambda} \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}= \\lambda e^{-\\lambda} e^{\\lambda} \\\\ &amp;= \\lambda.\\end{aligned}\\] Pour calculer la variance nous n’allons pas calculer \\(E(X^2)\\) mais le moment factoriel \\(E[X(X-1)]\\) qui s’obtient plus facilement, selon la méthode précédente: \\[\\begin{aligned} E[X(X-1)] &amp;=\\sum_{k=0}^{\\infty} k(k-1)P(X=k)=\\sum_{k=2}^{\\infty} k(k-1) \\,e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\ &amp;=e^{-\\lambda} \\sum_{k=2}^{\\infty} \\frac{\\lambda^k}{(k-2)!}=\\lambda^2 e^{-\\lambda} \\sum_{k=2}^{\\infty} \\frac{\\lambda^{k-2}}{(k-2)!} \\\\ &amp;= \\lambda^2 e^{-\\lambda} \\sum_{j=0}^{\\infty} \\frac{\\lambda^{j}}{j!}= \\lambda^2 e^{-\\lambda} e^{\\lambda} = \\lambda^2.\\end{aligned}\\] On en déduit: \\[\\begin{aligned} V(X)&amp;=E(X^2)-E^2(X)=E[X(X-1)]+E(X)-E^2(X) \\\\ &amp;=\\lambda^2+\\lambda-\\lambda^2=\\lambda.\\end{aligned}\\] Théorème 6.6 Si \\(X\\) et \\(Y\\) sont deux variables indépendantes suivant des lois de Poisson \\[X \\sim \\mathcal{P}(\\lambda) \\quad \\text{et} \\quad Y \\sim \\mathcal{P}(\\mu)\\] alors leur somme suit aussi une loi de Poisson: \\[X+Y \\sim \\mathcal{P}(\\lambda+\\mu).\\] Exemple: Soit \\(X\\) la variable aléatoire associée au nombre de micro-ordinateurs vendus chaque jour dans le magasin. On suppose que \\(X\\) suit une loi de Poisson de paramètre \\(\\lambda=5\\). On écrit alors \\(X \\sim \\mathcal{P}(5).\\) La probabilité associée à la vente de 5 micro-ordinateurs se détermine par : \\[P(X=5)=e^{-5} \\frac{5^5}{5!}=e^{-5}\\simeq 0.1755\\] La probabilité de vendre au moins 2 micro-ordinateurs est égal à: \\[\\begin{aligned} P(X \\geq 2)&amp;=1-\\left(e^{-5} \\frac{5^0}{0!}+e^{-5} \\frac{5^1}{1!}\\right)\\simeq 0.9596\\end{aligned}\\] Le nombre moyen de micro-ordinateurs vendus chaque jour dans le magasin est égal à 5 puisque \\(E(X)=\\lambda=5\\). Approximation d’une loi binomiale Le théorème de Poisson nous montre que si \\(n\\) est suffisamment grand et \\(p\\) assez petit, alors on peut approcher la distribution d’une loi binomiale de paramètres \\(n\\) et \\(p\\) par celle d’une loi de Poisson de paramètre \\(\\lambda=np\\), en effet \\[\\text{si} \\; n \\rightarrow \\infty \\; \\text{et}\\; p \\rightarrow 0 \\; \\text{alors} \\; X: \\mathcal{B}(n, p) \\rightarrow \\mathcal{P}(\\lambda).\\] Une bonne approximation est obtenue si \\(n \\geq 50\\) et \\(np \\leq 5\\). Dans ce contexte, la loi de Poisson est souvent utilisée pour modéliser le nombre de succès lorsqu’on répète un très grand nombre de fois une expérience ayant une chance très faible de réussir par une loi de Poisson (nombre de personnes dans la population française atteints d’une maladie rare, par exemple). On cherche la probabilité de trouver au moins un centenaire parmi 200 personnes dans une population où une personne sur cent est un centenaire. La probabilité \\(p=1/100=0.01\\) étant faible et \\(n=200\\) étant suffisamment grand, on peut modéliser le nombre \\(X\\) de centenaires pris parmi 200 personnes par la loi de Poisson de paramètre \\(\\lambda=200 \\times 0.01=2\\). Donc on a: \\[P(X\\geq 1)=1-P(X=0)=1-e^{-2}\\simeq 0.86\\] Soit une v.a. \\(X\\) telle que \\(X \\sim \\mathcal{B}(100, 0.01)\\), les valeurs des probabilités pour \\(k\\) de 0 à 5 ainsi que leur approximation à \\(10^{-3}\\) avec une loi de Poisson de paramètre \\(\\lambda= np =1\\) sont données dans le tableau ci-dessous : \\(k\\) \\(1\\) \\(2\\) \\(3\\) \\(4\\) \\(5\\) \\(k\\) \\(P(X = k)\\) \\(0.366\\) \\(0.370\\) \\(0.185\\) \\(0.061\\) \\(0.015\\) \\(0.000\\) Approximation \\(0.368\\) \\(0.368\\) \\(0.184\\) \\(0.061\\) \\(0.015\\) \\(0.003\\) Dans le cas de cet exemple où \\(n =100\\) et \\(np =1\\), l’approximation de la loi binomiale par une loi de poisson donne des valeurs de probabilités identiques à \\(10^{-3}\\) près. Loi Géométrique ou de Pascal \\(\\mathcal{G}(p)\\) On effectue des épreuves successives indépendantes jusqu’à la réalisation d’un événement particulier \\(A\\) de probabilité \\(p=P(A)\\) et on note \\(X\\) le nombre aléatoire d’épreuves effectuées. On définit ainsi une v.a. à valeurs entières de loi géométrique, ou de Pascal. A chaque épreuve est associé l’ensemble fondamental \\(\\Omega=\\{A, \\bar{A}\\}\\) et l’événement \\(\\{X=k\\}\\) pour \\(k\\in \\mathbb{N^*}\\) est représenté par une suite de \\(k-1\\) événements \\(\\bar{A}\\), terminée par l’événement \\(A\\): \\[\\underbrace{\\bar{A}\\bar{A}\\ldots \\bar{A}}_{k-1}A\\] D’où: \\[\\begin{equation} P(X=k)=(1-p)^{k-1}p \\quad \\forall \\, k \\in \\mathbb{N^*} \\tag{6.4} \\end{equation}\\] Cette loi peut servir à modéliser des temps de vie, ou des temps d’attente, lorsque le temps est mesuré de manière discrète (nombre de jours par exemple). En utilisant la série entière \\[\\label{eq:serie_entiere} \\sum_{k=0}^\\infty x^k = 1/(1-x) \\quad \\text{pour} \\quad |x|&lt;1\\] on vérifie bien que c’est une loi de probabilité: \\[\\begin{aligned} \\sum_{k=1}^\\infty P(X=k)&amp;= \\sum_{k=1}^\\infty (1-p)^{k-1}p = p \\sum_{j=0}^\\infty (1-p)^{j} \\\\ &amp;= p \\frac{1}{1-(1-p)}=1\\end{aligned}\\] Moments de loi Géométrique En dérivant la série entière (6.4) ci-dessus, on obtient \\(\\sum_{k=1}^\\infty k x^{k-1}=1/(1-x)^2\\). Ceci permet d’obtenir l’espérance: \\[E(X)=\\sum_{k=1}^\\infty kp(1-p)^{k-1}=\\frac{p}{[1-(1-p)]^2}=\\frac{1}{p}\\] En d’autres termes, si des épreuves indépendantes ayant une probabilité \\(p\\) d’obtenir un succès sont réalisés jusqu’à ce que le premier succès se produise, le nombre espéré d’essais nécessaires est égal à \\(1/p\\). Par exemple, le nombre espéré de jets d’un dé équilibré qu’il faut pour obtenir la valeur 1 est 6. Le calcul de la variance se fait à partir du moment factoriel et en utilisant la dérivée seconde de la série entière (6.4): \\(\\sum_{k=2}^\\infty k(k-1) x^{k-2} = 2/(1-x)^3\\), Donc \\[\\begin{aligned} E[X(X-1)]&amp;=\\sum_{k=2}^\\infty k(k-1)p(1-p)^{k-1} \\\\ &amp;= p(1-p)\\sum_{k=2}^\\infty k(k-1)(1-p)^{k-2} \\\\ &amp;= \\frac{2p(1-p)}{[1-(1-p)]^3}=\\frac{2(1-p)}{p^2}\\end{aligned}\\] d’où on déduit: \\[V(X)=E[X(X-1)]+E(X)-E^2(X)=\\frac{1-p}{p^2}.\\] Si l’on considère la variable aléatoire \\(X\\) “nombre de naissances observées jusqu’à l’obtention d’une fille” avec p = 1/2 (même probabilité de naissance d’une fille ou d’un garçon), alors X suit une loi géométrique et on a pour tout \\(k\\in \\mathbb{N^*}\\): \\[P(X=k)=(1-1/2)^{k-1}(1/2)=1/2^k\\] avec \\(E(X)=2\\) et \\(V(X)=2.\\) Loi Binomiale Négative \\(\\mathcal{BN}(r,p)\\) \\(\\varepsilon\\): “On répéte l’épreuve de Bernoulli jusqu’à obtenir un total de \\(r\\) succès”. Exemple avec : \\[\\bar{A} \\quad {A} \\quad \\bar{A} \\quad \\bar{A} \\quad \\bar{A} \\quad {A} \\quad \\bar{A} \\quad \\bar{A} \\quad {A}\\] \\[{E} \\quad {S} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {E} \\quad {S}\\] Mais on peut obtenir d’autres façons: \\[{S} \\quad {E} \\quad {E} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {S}\\] \\[{E} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {S} \\quad {E} \\quad {S}\\] Chaque épreuve a \\({p}\\) pour probabilité de succès et \\({1-p}\\) pour probabilité d’échec. Désignons \\(X=\\)“le nombre d’épreuves nécessaires pour atteindre ce résultat”. \\[\\underbrace{\\overbrace{{E} \\quad {S} \\quad {E} \\quad {E} \\quad {E} \\quad {S} \\quad {E} \\quad {E}}^{ {r-1 \\, succès}\\, et \\, {k-r \\, échecs}} \\quad {S}}_{X=k}\\] \\(X(\\Omega)=\\{r,r+1,r+2,\\ldots\\}\\). On dit \\(X \\sim \\mathcal{BN}(r,p)\\). \\(\\forall \\, k \\in X(\\Omega),\\) \\[P(X=k) = \\binom{{k-1}}{{r-1}} {p^r} {(1-p)^{k-r}}\\] \\(\\mathcal{G}(p)=\\mathcal{BN}(1,p)\\) \\(\\varepsilon\\): “On répéte l’épreuve de Bernoulli jusqu’à obtenir un total de \\(r\\) succès”. Soit, \\[{E} \\quad \\ldots \\quad {E} \\quad {S} \\quad {E} \\quad \\ldots \\quad {E} \\quad {S} \\ldots \\quad {E} \\ldots \\quad {E} \\quad {S}\\] Soit, \\(Y_1\\) le nombre d’épreuves nécessaires jusqu’au premier succès, \\(Y_2\\) le nombre d’épreuves supplémentaires nécessaires pour obtenir un deuxième succès, \\(Y_3\\) celui menant au 3ème et ainsi de suite. Càd, \\[\\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_1} \\quad \\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_2} \\quad \\underbrace{\\ldots}_{\\ldots} \\quad \\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_r}\\] Les tirages étants indépendantes et ayant toujours la même probabilité de succès, chacune des variables \\(Y_1,Y_2,\\ldots,Y_r\\) est géométrique \\(\\mathcal{G}(p)\\). \\(X=\\)“le nombre d’épreuves nécessaires à l’obtention de \\(r\\) succès”\\(=Y_1 + Y_2 + \\ldots + Y_r\\). Donc, \\[E(X)= E(Y_1) + E(Y_2) + \\ldots + E(Y_r) = \\sum_{i=1}^r \\frac{1}{p} = \\frac{r}{p}\\] et \\[V(X)= \\sum_{i=1}^r V(Y_i) = \\frac{r(1-p)}{p^2}\\] car les \\(Y_i\\) sont indépendantes. "],
["variables-aleatoires-continues.html", "Variables Aléatoires Continues Densité d’une variable aléatoire continue Fonction de répartition d’une v.a.c Fonction d’une variable aléatoire continue Espérance et variance de variables aléatoires continues Lois usuelles de v.a.c Couple de variables aléatoires continues", " Variables Aléatoires Continues Densité d’une variable aléatoire continue Dans les chapitres précédents nous avons traité des variables aléatoires discrètes, c’est-à-dire de variables dont l’univers est fini ou infini dénombrable. Il existe cependant des variables dont l’univers est infini non dénombrable. On peut citer par exemple, l’heure d’arrivée d’un train à une gare donnée ou encore la durée de vie d’un transistor. Désignons par \\(X\\) une telle variable. Définition 6.10 \\(X\\) est une variable aléatoire continue s’il existe une fonction \\(f\\) non négative définie pour tout \\(x \\in \\mathbb{R}\\) et vérifiant pour tout ensemble \\(B\\) de nombres réels la propriété \\[\\begin{equation} P(X \\in B) = \\int_B f(x)dx \\tag{6.5} \\end{equation}\\] La fonction \\(f\\) est appelée densité de probabilité de la variable aléatoire \\(X\\). Tous les problèmes de probabilité relatifs à \\(X\\) peuvent être traités grâce à \\(f\\). Par exemple pour \\(B=[a,b]\\), on obtient grâce à l’équation (6.5) \\[\\begin{equation} P(a\\le X \\le b) = \\int_a^bf(x)dx \\tag{6.6} \\end{equation}\\] Graphiquement, \\(P(a\\le X \\le b)\\) est l’aire de la surface entre l’axe de \\(x\\), la courbe correspondante à \\(f(x)\\) et les droites \\(x=a\\) et \\(x=b\\). Voire Figure 6.2 et Figure 6.3. Figure 6.2: \\(P(a \\leq X \\leq B)=\\) surface grisée Figure 6.3: L’aire hachurée correspond à des probabilités. \\(f(x)\\) étant une fonction densité de probabilité Définition 6.11 Pour toute variable aléatoire continue \\(X\\) de densité \\(f\\): \\(f(x) \\ge 0 \\quad \\forall \\, x \\in \\mathbb{R}\\) \\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\) Si l’on pose \\(a=b\\) dans (6.6), il résulte \\[P(X=a)=\\int_a^a f(x)dx = 0\\] Ceci siginifie que la probabilité qu’une variable aléatoire continue prenne une valeur isolée fixe est toujours nulle. Aussi on peut écrire \\[P(X &lt; a) = P( X \\le a) = \\int_{-\\infty}^a f(x)dx\\] Soit \\(X\\) la variable aléatoire réelle de densité de probabilité \\[f(x)= \\left\\lbrace \\begin{array}{ll} kx &amp; \\mbox{si} \\quad 0\\le x \\le 5\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer \\(k\\). Calculer: \\(P(1 \\le X \\le 3), P(2 \\le X \\le 4)\\) et \\(P(X &lt; 3)\\). Soit \\(X\\) une variable aléatoire réelle continue ayant pour densité de probabilité \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{6} x + k &amp; \\mbox{si} \\quad 0\\le x \\le 3\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer \\(k\\). Calculer \\(P(1 \\le X \\le 2)\\) Fonction de répartition d’une v.a.c Définition 6.12 Si comme pour les variables aléatoires discrètes, on définit la fonction de répartition de \\(X\\) par: \\[\\begin{aligned} F_X \\colon \\mathbb{R} &amp;\\longrightarrow \\mathbb{R} \\\\ x &amp;\\longmapsto F_X(a) = P(X \\le a)\\end{aligned}\\] alors la relation entre la fonction de répartition \\(F_X\\) et la fonction densité de probabilité \\(f(x)\\) est la suivante: \\[\\forall \\quad a \\in \\mathbb{R} \\quad F_X(a)= P(X \\le a) = \\int_{-\\infty}^a f(x)dx\\] La fonction de répartition \\(F_X(a)\\) est la primitive de la fonction densité de probabilité \\(f(x)\\) (donc la densité d’une v.a.c est la dérivée de la fonction de répartition), et permet d’obtenir les probabilités associées à la variable aléatoire \\(X\\), en effet: Propriétés: Pour une variable aléatoire continue X: \\(F&#39;_X(x) = \\frac{\\text{d}}{\\text{d} x} F_X(x) = f(x)\\). Pour tous réels \\(a \\le b\\), \\[\\begin{aligned} P(a &lt; X &lt; b) &amp; = P(a &lt; X \\le b) \\\\ &amp; = P(a \\le X &lt; b) \\\\ &amp; = P( a \\le X \\le b) \\\\ &amp; = F_X(b) - F_X(a) = \\int_a^bf(x)dx \\end{aligned}\\] La fonction de répartition correspond aux probabilités cumulées associées à la variable aléatoire continue sur l’intervalle d’étude (Figure 6.4). Figure 6.4: L’aire hachurée en vert sous la courbe de la fonction densité de probabilité correspond à la probabilité \\(P ( X &lt; a ) = F_X ( a )\\) et vaut 0.5 car ceci correspond exactement à la moitié de l’aire totale sous la courbe Propriétés: Les propriétés associées à la fonction de répartition sont les suivantes: \\(F_X\\) est continue sur \\(\\mathbb{R}\\), dérivable en tout point où \\(f\\) est continue. \\(F_X\\) est croissante sur \\(\\mathbb{R}\\). \\(F_X\\) est à valeurs dans \\([0,1]\\). \\(\\lim\\limits_{x\\to - \\infty} F_X(x) = 0\\) et \\(\\lim\\limits_{x\\to +\\infty} F_X(x) = 1\\). Fonction d’une variable aléatoire continue Soit \\(X\\) une variable aléatoire continue de densité \\(f_X\\) et de fonction de répartition \\(F_X\\). Soit \\(h\\) une fonction continue définie sur \\(X(\\Omega)\\), alors \\(Y=h(X)\\) est une variable aléatoire. Pour déterminer la densité de \\(Y\\), notée \\(f_Y\\), on commence par calculer la fonction de répartition de \\(Y\\), notée \\(F_Y\\), ensuite nous dérivons pour déterminer \\(f_Y\\). Calcul de densités pour \\(h(X)=aX+b\\) \\(\\forall \\quad y \\in \\mathbb{R}\\), \\[F_Y(y) = P(Y\\leq y)=P(h(X) \\le y) = P(aX+b \\le y)\\] si \\(a&gt;0\\), \\[F_Y(y) = P(aX+b \\le y) = P(X\\leq \\frac{y-b}{a})=F_X(\\frac{y-b}{a})\\] si \\(a&lt;0\\), \\[F_Y(y) = P(aX+b \\le y) =P(X\\geq \\frac{y-b}{a})=1-F_X(\\frac{y-b}{a})\\] En dérivant on obtient la densité de \\(Y\\) \\[f_Y(y)=\\frac{1}{|a|}f_X(\\frac{y-b}{a}).\\] Calcul de densités pour \\(h(X)=X^2\\) si \\(y&lt;0\\), \\(F_Y(y) =P(Y\\leq y)=0\\). si \\(y&gt;0\\), \\[F_Y(y) =P(Y\\leq y)=P(X^2 \\le y)=P(-\\sqrt{y}\\leq X \\leq \\sqrt{y})=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\\] En dérivant on obtient la densité de \\(Y\\), \\[f_Y(y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{2\\sqrt{y}}\\big[f_X(\\sqrt{y})+f_X(-\\sqrt{y})\\big] &amp; \\mbox{si} \\quad y \\ge 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calcul de densités pour \\(h(X)=e^X\\) si \\(y&lt;0\\), \\(F_Y(y) = P(Y\\leq y)=0\\). si \\(y&gt;0\\), \\(F_Y(y) = P(Y\\leq y)=P(e^X \\le y)=P( X \\leq \\ln (y))=F_X(\\ln(y))\\). En dérivant on obtient la densité de \\(Y\\) \\[f_Y(y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{y} f\\big(\\ln (y)\\big) &amp; \\mbox{si} \\quad y \\ge 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Soit la v.a.c \\(X\\) ayant la fonction de densité \\[f_X(x)= \\left\\lbrace \\begin{array}{ll} 2 x &amp; \\mbox{si} \\quad 0 \\le x \\le 1\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Déterminer la densité de: \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\). Espérance et variance de variables aléatoires continues Espérance d’une v.a.c Définition 6.13 Si \\(X\\) est une variable aléatoire absolument continue de densité \\(f\\), on appelle espérance de X, le réel \\(E(X)\\), défini par: \\[E(X)= \\int_{-\\infty}^{+\\infty}x f(x) dx\\] si cette intégrale est convergente. Les propriétés de l’espérance d’une variable aléatoire continue sont les mêmes que pour une variable aléatoire discrète. Propriétés: Soit \\(X\\) une variable aléatoire continue, \\(E(aX+b)=aE(X)+b \\quad \\quad a \\ge 0 \\,\\, \\text{et} \\,\\, b \\in \\mathbb{R}\\). Si \\(X \\ge 0\\) alors \\(E(X) \\ge 0\\). Si \\(X\\) et \\(Y\\) sont deux variables aléatoires définies sur un même univers \\(\\Omega\\) alors \\[E(X+Y)=E(X)+E(Y)\\] Théorème 6.7 (Théorème de transfert) Si \\(X\\) est une variable aléatoire de densité \\(f(x)\\), alors pour toute fonction réelle \\(g\\) on aura \\[E[g(X)] = \\int_{-\\infty}^{+\\infty}g(x) f(x) dx\\] Soit la v.a.c \\(X\\) ayant la fonction de densité \\[f_X(x)= \\left\\lbrace \\begin{array}{ll} 2 x &amp; \\mbox{si} \\quad 0 \\le x \\le 1\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Calculer l’espérance des variables aléatoires \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\). Variance d’une v.a.c La variance d’une variable aléatoire \\(V(X)\\) est l’espérance mathématique du carré de l’écart à l’espérance mathématique. C’est un paramètre de dispersion qui correspond au moment centré d’ordre 2 de la variable aléatoire \\(X\\). Définition 6.14 Si \\(X\\) est une variable aléatoire ayant une espérance \\(E(X)\\), on appelle variance de \\(X\\) le réel \\[V(X)=E\\big([X-E(X)]^2\\big) = E(X^2) - [E(X)]^2\\] Si \\(X\\) est une variable aléatoire continue, on calcule \\(E(X^2)\\) en utilisant le théorème 6.7, \\[E(X^2) = \\int_{-\\infty}^{+\\infty}x^2 f(x)dx\\] Propriétés: Si \\(X\\) est une variable aléatoire admettant une variance alors: \\(V(X) \\ge 0\\), si elle existe. \\(\\forall \\quad a \\in \\mathbb{R}, V(aX) = a^2 V(X)\\) \\(\\forall \\quad (a,b) \\in \\mathbb{R}, V(aX+b) = a^2 V(X)\\) Si \\(X\\) et \\(Y\\) sont deux variables aléatoires indépendantes, \\(V(X+Y)=V(X)+V(Y)\\) Définition 6.15 (Ecart-type) Si \\(X\\) est une variable aléatoire ayant une variance \\(V(X)\\), on appelle écart-type de \\(X\\), le réel: \\[\\sigma_X = \\sqrt{V(X)}\\] Lois usuelles de v.a.c Loi uniforme \\(U(a,b)\\) La loi uniforme est la loi exacte de phénomènes continus uniformément répartis sur un intervalle. Définition 6.16 La variable aléatoire \\(X\\) suit une loi uniforme sur le segment \\([a,b]\\) avec \\(a &lt; b\\) si sa densité de probabilité est donnée par \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{b-a} &amp; \\mbox{si} \\quad x \\in [a,b]\\\\ 0 &amp; \\mbox{si} \\quad x \\notin [a,b] \\end{array} \\right. = \\frac{1}{b-a} {1}_{[a,b]}(x)\\] Figure 6.5: Fonction de densité de \\(U([a,b]\\)) Quelques commentaires: La loi uniforme continue étant une loi de probabilité, l’aire hachurée en bleu sur la Figure 6.5 vaut \\(1\\). La fonction de répartition associée à la loi uniforme continue est \\[F_X(x)= \\left\\lbrace \\begin{array}{ll} 0 &amp; \\mbox{si} \\quad x &lt; a \\\\ \\frac{x-a}{b-a} &amp; \\mbox{si} \\quad a \\le x \\le b \\\\ 1 &amp; \\mbox{si} \\quad x &gt; b \\end{array} \\right.\\] Propriétés: Si \\(X\\) est une v.a.c qui suit la loi uniforme sur \\([a,b]\\): \\(E(X) = \\frac{b+a}{2}\\) \\(V(X) =\\frac{(b-a)^2}{12}\\) Soit \\(X \\thicksim U(0,10)\\). Calculer: \\(P(X &lt;3)\\) \\(P(X\\ge 6)\\) \\(P(3 &lt; X &lt; 8)\\) Loi exponentielle \\(\\mathcal{E}(\\lambda)\\) Définition 6.17 On dit qu’une variable aléatoire \\(X\\) est exponentielle (ou suit la loi exponentielle) de paramètre \\(\\lambda\\) si sa densité est donnée par \\[f(x)= \\left\\lbrace \\begin{array}{ll} \\lambda e^{- \\lambda x} &amp; \\mbox{si} \\quad x \\ge 0\\\\ 0 &amp; \\mbox{si} \\quad x &lt; 0 \\end{array} \\right. = \\lambda e^{- \\lambda x} {1}_{\\mathbb{R}^{+}}(x)\\] On dit \\(X \\thicksim \\mathcal{E}(\\lambda)\\) La fonction de répartition \\(F\\) d’une variable aléatoire exponentielle est donnée par \\[\\mbox{Si}\\,\\, x \\ge 0 \\quad F(x) = P(X \\le x) = \\int_0^x f(t)dt = \\int_0^x \\lambda e^{- \\lambda t} dt = \\big[ -e^{- \\lambda t} \\big]_0^x = 1-e^{- \\lambda x} \\quad\\] Propriétés: Si \\(X \\thicksim \\mathcal{E}(\\lambda)\\) \\(E(X) = \\frac{1}{\\lambda}\\) \\(V(X)= \\frac{1}{\\lambda^2}\\) Cas d’utilisations de la loi exponentielle : Dans la pratique, on rencontre souvent la distribution exponentielle lorsqu’il s’agit de représenter le temps d’attente avant l’arrivée d’un événement spécifié. Une loi exponentielle modélise la durée de vie d’un phénomène sans mémoire, ou sans vieillissement, ou sans usure. En d’autres termes, le fait que le phénomène ait duré pendant un temps \\(t\\) ne change rien à son espérance de vie à partir du temps \\(t\\). On dit qu’une variable aléatoire non négative \\(X\\) est sans mémoire lorsque \\[P(X &gt; t+h | X &gt; t) = P(X &gt; h) \\quad \\quad \\forall \\quad t,h \\ge 0\\] Par exemple, la durée de vie de la radioactivité ou d’un composant électronique, le temps qui nous sépare d’un prochain tremblement de terre ou du prochain appel téléphonique mal aiguillé sont toutes des variables aléatoires dont les distributions tendent en pratique à se rapprocher de distributions exponentielles. Loi Normale ou de Laplace-Gauss \\(\\mathcal{N}(\\mu,\\sigma^2)\\) Définition 6.18 Une variable aléatoire \\(X\\) est dite normale avec paramètres \\(\\mu\\) et \\(\\sigma^2\\) si la densité de \\(X\\) est donnée par \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(x - \\mu)^2/2\\sigma^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R}\\] Avec \\(\\mu \\in \\mathbb{R}\\) et \\(\\sigma \\in \\mathbb{R}^{+}\\). On dit que \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\). Remarque: On admet que \\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\) dans la mesure où l’intégration analytique est impossible. Étude de la densité de la loi Normale La fonction \\(f\\) est paire autour d’un axe de symétrie \\(x = \\mu\\) car \\(f(x + \\mu ) = f(\\mu - x)\\). \\(f&#39;(x)=0\\) pour \\(x=\\mu\\), \\(f&#39;(x) &lt; 0\\) pour \\(x &lt; \\mu\\) et \\(f&#39;(x) &gt; 0\\) pour \\(x &gt; \\mu\\) Figure 6.6: Représentation graphique de la densité d’une loi normale. Remarque: Le paramètre \\(\\mu\\) représente l’axe de symétrie et s le degré d’aplatissement de la courbe de la loi normale dont la forme est celle d’une courbe en cloche Propriétés: Soit \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\), on a: \\(E(X)=\\mu\\) \\(V(X)=\\sigma^2\\) Théorème 6.8 (Stabilité de la loi normale) Soit \\(X_1\\) et \\(X_2\\) deux variables aléatoires normales et indépendantes de paramètres respectifs \\((\\mu_1,\\sigma_1^2)\\) et \\((\\mu_2,\\sigma_2^2)\\), alors leur somme \\(X_1+X_2\\) est une variable aléatoire normale de paramètres \\((\\mu_1 + \\mu_2,\\sigma_1^2+\\sigma_2^2)\\). Loi Normale centrée réduite \\(\\mathcal{N}(0,1)\\) Définition 6.19 Une variable aléatoire continue \\(X\\) suit une loi normale centrée réduite si sa densité de probabilité est donnée par \\[\\begin{equation} f(x) = \\frac{1}{{\\sqrt {2\\pi } }}e^{- \\frac{1}{2} x^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R} \\end{equation}\\] On dit \\(X \\thicksim \\mathcal{N}(0,1)\\). Remarque: \\(E(X)=0\\) et \\(V(X)=1\\). Figure 6.7: (gauche): Densité d’une loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). (droite): Fonction de répartition de \\(\\mathcal{N}(0,1)\\). Relation entre loi normale et loi normale centrée réduite Théorème 6.9 (Relation avec la loi normale) Si \\(X\\) suit une loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\), alors \\(Z= \\frac{X-\\mu}{\\sigma}\\) est une variable centrée réduite qui suit la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\). Calcul des probabilités d’une loi normale La fonction de répartition de la loi normale réduite permet d’obtenir les probabilités associées à toutes variables aléatoires normales \\(\\mathcal{N}(\\mu,\\sigma^2)\\) après transformation en variable centrée réduite. Définition 6.20 On appelle fonction \\(\\Phi\\), la fonction de répartition de la loi normale centrée réduite \\(\\mathcal{N}(0,1)\\), telle que \\[\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) = P(X \\le x) = \\frac{1}{{\\sqrt {2\\pi}}} \\int_{-\\infty}^x f(t)dt\\] Propriétés: Les propriétés associées à la fonction de répartition \\(\\Phi\\) sont: \\(\\Phi\\) est croissante, continue et dérivable sur \\(\\mathbb{R}\\) et vérifie: \\(\\lim\\limits_{x\\to - \\infty} \\Phi(x) = 0\\) et \\(\\lim\\limits_{x\\to\\infty} \\Phi(x) = 1\\) \\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) + \\Phi(-x) = 1\\) \\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) - \\Phi(-x) = 2\\Phi(x) -1\\) Une application directe de la fonction \\(\\Phi\\) est la lecture des probabilités de la loi normale sur la table de la loi normale centrée réduite. Soit \\(X\\) une variable aléatoire normale de paramètres \\(\\mu =3\\) et \\(\\sigma^2=4\\). Calculer: \\(P(X &gt; 0)\\) \\(P(2 &lt; X &lt; 5)\\) \\(P(|X-3| &gt; 4)\\) Approximation normale d’une répartition binomiale Un résultat important de la théorie de probabilité est connu sous le nom de théorème limite de Moivre-Laplace. Il dit que pour \\(n\\) grand, une variable binomiale \\(\\mathcal{B}(n,p)\\) suivra approximativement la même loi qu’une variable aléatoire normale avec même moyenne et même variance. Ce théorème énonce que si “on standardise” une variable aléatoire binomiale \\(\\mathcal{B}(n,p)\\) en soustrayant d’abord sa moyenne \\(np\\) puis en divisant le résultat par son écart-type \\(\\sqrt{np(1-p)}\\), alors la variable aléatoire standardisée (de moyenne 0 et variance 1) suivra approximativement, lorsque \\(n\\) est grand, une distribution normale standard. Ce résultat fut ensuite progressivement généralisé par Laplace, Gauss et d’autres pour devenir le théorème actuellement connu comme théorème centrale limite qui est un des deux résultats les plus importants de la théorie de probabilités. Ce théorème sert de base théorique pour expliquer un fait empirique souvent relevé, à savoir qu’en pratique de très nombreux phénomènes aléatoires suivent approximativement une distribution normale. On remarquera qu’à ce stade deux approximations de la répartition binomiale ont été proposées: l’approximation de Poisson, satisfaisante lorsque \\(n\\) est grand et lorsque \\(np\\) n’est pas extrême; l’approximation normale pour laquelle on peut montrer qu’elle est de bonne qualité lorsque \\(np(1-p)\\) est grand (dès que \\(np(1-p)\\) dépasse 10). Figure 6.8: La loi de probabilité d’une variable aléatoire \\(B( n,p )\\) devient de plus en plus normale à mesure que \\(n\\) augmente. Loi de \\(\\chi^{2}\\) de Pearson Définition 6.21 Soit \\(X_1,X_2,\\ldots,X_n\\), \\(n\\) variables normales centrées réduites, et \\(Y\\) la variable aléatoire définie par \\[Y = X_1^2 + X_2^2 + \\ldots + X_i^2 + \\ldots + X_n^2 = \\sum_{i=1}^n X_i^2\\] On dit que \\(Y\\) suit la loi de \\(\\chi^2\\) (ou loi de Pearson) à \\(n\\) degrés de liberté, \\(Y \\thicksim \\chi^2 (n)\\) La loi de \\(\\chi^2\\) trouve de nombreuses applications dans le cadre de la comparaison de proportions, des tests de conformité d’une distribution observée à une distribution théorique et le test d’indépendance de deux caractères qualitatifs. Ce sont les tests du khi-deux. Remarque: Si \\(n=1\\), la variable du \\(\\chi^2\\) correspond au carré d’une variable normale centrée réduite \\(\\mathcal{N}(0,1)\\). Propriétés: Si \\(Y \\thicksim \\chi^2 (n)\\), alors: \\(E(Y)= n\\) \\(V(Y) = 2n\\) L’augmentation de degré de liberté (\\(n\\)) déplace la distribution vers la droite (le mode devient plus grand) et augmente la dispersion (la variance de la distribution augmente). Loi de Student \\(St(n)\\) Définition 6.22 Soit \\(U\\) une variable aléatoire suivant une loi normale centrée réduite \\(\\mathcal{N}(0,1)\\) et \\(V\\) une variable aléatoire suivant une loi de \\(\\chi^2(n)\\), \\(U\\) et \\(V\\) étant indépendantes, on dit alors que \\(T_n = \\frac{U}{\\sqrt{\\frac{V}{n}}}\\) suit une loi de Student à \\(n\\) degrés de liberté. \\(T_n \\thicksim St(n)\\) La loi de Student est utilisée lors des tests de comparaison de paramètres comme la moyenne et dans l’estimation de paramètres de la population à partir de données sur un échantillon (Test de Student). Le graphique de plusieurs densités de la loi de Student illustre le suivant: à mesure que les degrés de liberté augmentent, la forme de la densité de Student se rapproche de celle d’une courbe de la loi normale centrée réduite. Déjà pour \\(n = 25\\), nous trouvons peu de différence avec la densité normale standard. Si \\(n\\) est petit, nous trouvons que la distribution a des queues plus lourdes qu’une normale, c’est-à-dire qu’elle a la forme d’une cloche plus grosse. Loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\) Définition 6.23 Soit \\(U\\) et \\(V\\) deux variables aléatoires indépendantes suivant une loi de \\(\\chi^2\\) respectivement à \\(n\\) et \\(m\\) degrés de liberté. On dit que \\(F= \\frac{U/n}{V/m}\\) suit une loi de Fisher-Snedecor à \\((n,m)\\) degrés de liberté. \\(F \\thicksim \\mathcal{F}(n,m)\\) La loi de Fisher-Snedecor est utilisée pour comparer deux variances observées et sert surtout dans les très nombreux tests d’analyse de variance et de covariance. Couple de variables aléatoires continues Densité conjointe Définition 6.24 On dit que \\((X,Y)\\) est un couple aléatoire continu s’il existe une fonction \\(f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) telle que pour tout \\(D \\subseteq \\mathbb{R}\\) on a \\[P\\{(X,Y) \\in D\\} = \\iint\\limits_{(x,y) \\in D} f(x,y) dx dy\\] Remarque: On a la condition de normalité \\(\\iint\\limits_{\\mathbb{R}^2} f(x,y)dxdy=1\\) La fonction \\(f\\) s’appelle densité conjointe de \\(X\\) et \\(Y\\). Notons par \\(A\\) et \\(B\\) deux ensembles de nombres réels. En définissant \\(D=\\{(x,y) : x \\in A, y \\in B\\}\\), on obtient \\[P(X\\in A, Y \\in B) = \\int_A \\int_B f(x,y) dxdy\\] La fonction de répartition du \\((X,Y)\\) est définie par \\[F(a,b)=P(X \\le a, Y \\le b) = \\int_{- \\infty}^b \\int_{- \\infty}^a f(x,y) dx dy\\] \\(f\\) est le dérivé de \\(F\\): \\(f(a,b)= \\frac{\\partial^2}{\\partial a \\partial b} F(a,b)\\) Soit \\((X,Y)\\) un couple aléatoire continu de densité \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} a x y^2 &amp; \\mbox{si} \\quad 0 \\le x \\le y \\le 1 \\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Trouver la constante \\(a\\). Soit \\((X,Y)\\) un couple aléatoire continu de densité \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} 2 e^{-x} e^{-2y} &amp; \\mbox{si} \\quad x &gt; 0, \\,\\, y &gt; 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Montrer que: \\(P(X &gt; 1, Y &lt; 1)=e^{-1}(1-e^{-2})\\) \\(P(X &lt; a) = 1-e^{-a}\\) \\(P(X &lt; Y ) = 1/3\\) Densités marginales Si on dispose de la densité du couple, on peut retrouver les densités de \\(X\\) et de \\(Y\\), appelées les densités marginales: Densité marginale de X: \\[f(x,.)=f_X(x)=\\int_{\\mathbb{R}} f(x,y)dy\\] Densité marginale de Y: \\[f(.,y)=f_Y(y)=\\int_{\\mathbb{R}} f(x,y)dx\\] Espérance d’une fonction du couple Si \\((X,Y)\\) est un couple continu de densité \\(f(x,y)\\) et \\(g: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) on a \\[E[ g(X,Y)] = \\iint\\limits_{\\mathbb{R}^2} g(x,y) f(x,y)dxdy\\] Indépendance Les v.a. \\(X\\) et \\(Y\\) sont indépendantes ssi \\(\\forall \\, (x,y) \\in \\mathbb{R}^2\\) on a \\[f(x,y)=f_X(x) f_Y(y)\\] Distribution conditionnelle Si \\((X,Y)\\) est un couple continu de densité \\(f(x,y)\\), on définit densité conditionnelle de \\(X\\), sous la condition \\(Y=y\\) et lorsque \\(f_Y(y) &gt; 0\\) par la relation \\[f_{X|Y} (x|y) = \\frac{f(x,y)}{f_Y(y)}\\] Supposons que \\(X\\) et \\(Y\\) aient pour densité conjointe \\[f(x,y)= \\left\\lbrace \\begin{array}{ll} \\frac{1}{y} e^{- x/y}e^{-y} &amp; \\mbox{si} \\quad x &gt; 0, \\,\\, y &gt; 0\\\\ 0 &amp; \\mbox{sinon} \\end{array} \\right.\\] Déterminer la densité conditionnelle de \\(X\\) lorsque \\(Y=y\\). Calculer \\(P(X&gt;1 | Y = y)\\) "],
["app-introRStudio.html", "A Introduction to RStudio", " A Introduction to RStudio RStudio is the most employed Integrated Development Environment (IDE) for nowadays. When you start RStudio you will see a window similar to Figure A.1. There are a lot of items in the GUI, most of them described in the RStudio IDE Cheat Sheet . The most important things to keep in mind are: The code is written in scripts in the source panel (upper-right panel in Figure A.1); for running a line or code selection from the script in the console (first tab in the lower-right panel in Figure A.1), you do it with the keyboard shortcut 'Ctrl+Enter' (Windows and Linux) or 'Cmd+Enter' (Mac OS X). Figure A.1: Main window of RStudio. The red shows the code panel and the yellow shows the console output. Extracted from here. "],
["tab-normale1.html", "B Table 1 de la loi Normale centrée réduite", " B Table 1 de la loi Normale centrée réduite \\(X\\) étant une variable alétoire de loi \\(\\mathcal{N}(0,1)\\), la table donne la valeur de \\(\\Phi(a)=P(X\\leq a)\\). En , la commande correspondante est pnorm(a). \\(a\\) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 Par exemple, pour \\(x = 1.23\\) (intersection de la ligne 1.2 et de la colonne 0.03), on obtient : \\(\\Phi(1.23) \\approx 0.8907\\). "],
["tab-normale2.html", "C Table 2 de la loi Normale centrée réduite", " C Table 2 de la loi Normale centrée réduite \\(X\\) étant une variable alétoire de loi \\(\\mathcal{N}(0,1)\\) et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(z_{1-\\frac{\\alpha}{2}}=\\Phi^{-1}(1-\\frac{\\alpha}{2})\\) telle que \\(P(|X| &gt; z_{1-\\frac{\\alpha}{2}})=\\alpha\\). En , la commande correspondante est qnorm(1-alpha/2). \\(\\alpha\\) 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 Inf 2.5758 2.3263 2.1701 2.0537 1.9600 1.8808 1.8119 1.7507 1.6954 0.1 1.6449 1.5982 1.5548 1.5141 1.4758 1.4395 1.4051 1.3722 1.3408 1.3106 0.2 1.2816 1.2536 1.2265 1.2004 1.1750 1.1503 1.1264 1.1031 1.0803 1.0581 0.3 1.0364 1.0152 0.9945 0.9741 0.9542 0.9346 0.9154 0.8965 0.8779 0.8596 0.4 0.8416 0.8239 0.8064 0.7892 0.7722 0.7554 0.7388 0.7225 0.7063 0.6903 0.5 0.6745 0.6588 0.6433 0.6280 0.6128 0.5978 0.5828 0.5681 0.5534 0.5388 0.6 0.5244 0.5101 0.4959 0.4817 0.4677 0.4538 0.4399 0.4261 0.4125 0.3989 0.7 0.3853 0.3719 0.3585 0.3451 0.3319 0.3186 0.3055 0.2924 0.2793 0.2663 0.8 0.2533 0.2404 0.2275 0.2147 0.2019 0.1891 0.1764 0.1637 0.1510 0.1383 0.9 0.1257 0.1130 0.1004 0.0878 0.0753 0.0627 0.0502 0.0376 0.0251 0.0125 "],
["tab-student.html", "D Table de la loi de Student", " D Table de la loi de Student Attention pour la description de cette table. Ici on donne directement le quantile \\(t_{n,1-\\frac{\\alpha}{2}}\\). \\(X\\) étant une variable alétoire de loi de Student à \\(n\\) degrés de liberté \\(St(n)\\) et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(t_{n,1-\\frac{\\alpha}{2}} = F^{-1}(1-\\frac{\\alpha}{2})\\) telle que \\(P(|X| &gt; t_{n,1-\\frac{\\alpha}{2}})=\\alpha\\). En , la commande correspondante est qt(1-alpha/2, n). \\(n \\backslash \\alpha\\) 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.05 0.02 0.01 0.001 1 0.158 0.325 0.510 0.727 1.000 1.376 1.96 3.08 6.31 12.71 31.82 63.66 636.62 2 0.142 0.289 0.445 0.617 0.816 1.061 1.39 1.89 2.92 4.30 6.96 9.93 31.60 3 0.137 0.277 0.424 0.584 0.765 0.978 1.25 1.64 2.35 3.18 4.54 5.84 12.92 4 0.134 0.271 0.414 0.569 0.741 0.941 1.19 1.53 2.13 2.78 3.75 4.60 8.61 5 0.132 0.267 0.408 0.559 0.727 0.920 1.16 1.48 2.02 2.57 3.37 4.03 6.87 6 0.131 0.265 0.404 0.553 0.718 0.906 1.13 1.44 1.94 2.45 3.14 3.71 5.96 7 0.130 0.263 0.402 0.549 0.711 0.896 1.12 1.42 1.90 2.37 3.00 3.50 5.41 8 0.130 0.262 0.399 0.546 0.706 0.889 1.11 1.40 1.86 2.31 2.90 3.35 5.04 9 0.129 0.261 0.398 0.543 0.703 0.883 1.10 1.38 1.83 2.26 2.82 3.25 4.78 10 0.129 0.260 0.397 0.542 0.700 0.879 1.09 1.37 1.81 2.23 2.76 3.17 4.59 11 0.129 0.260 0.396 0.540 0.697 0.876 1.09 1.36 1.80 2.20 2.72 3.11 4.44 12 0.128 0.259 0.395 0.539 0.695 0.873 1.08 1.36 1.78 2.18 2.68 3.06 4.32 13 0.128 0.259 0.394 0.538 0.694 0.870 1.08 1.35 1.77 2.16 2.65 3.01 4.22 14 0.128 0.258 0.393 0.537 0.692 0.868 1.08 1.34 1.76 2.14 2.62 2.98 4.14 15 0.128 0.258 0.393 0.536 0.691 0.866 1.07 1.34 1.75 2.13 2.60 2.95 4.07 16 0.128 0.258 0.392 0.535 0.690 0.865 1.07 1.34 1.75 2.12 2.58 2.92 4.01 17 0.128 0.257 0.392 0.534 0.689 0.863 1.07 1.33 1.74 2.11 2.57 2.90 3.96 18 0.127 0.257 0.392 0.534 0.688 0.862 1.07 1.33 1.73 2.10 2.55 2.88 3.92 19 0.127 0.257 0.391 0.533 0.688 0.861 1.07 1.33 1.73 2.09 2.54 2.86 3.88 20 0.127 0.257 0.391 0.533 0.687 0.860 1.06 1.32 1.73 2.09 2.53 2.85 3.85 21 0.127 0.257 0.391 0.532 0.686 0.859 1.06 1.32 1.72 2.08 2.52 2.83 3.82 22 0.127 0.256 0.390 0.532 0.686 0.858 1.06 1.32 1.72 2.07 2.51 2.82 3.79 23 0.127 0.256 0.390 0.532 0.685 0.858 1.06 1.32 1.71 2.07 2.50 2.81 3.77 24 0.127 0.256 0.390 0.531 0.685 0.857 1.06 1.32 1.71 2.06 2.49 2.80 3.75 25 0.127 0.256 0.390 0.531 0.684 0.856 1.06 1.32 1.71 2.06 2.48 2.79 3.73 26 0.127 0.256 0.390 0.531 0.684 0.856 1.06 1.31 1.71 2.06 2.48 2.78 3.71 27 0.127 0.256 0.389 0.531 0.684 0.855 1.06 1.31 1.70 2.05 2.47 2.77 3.69 28 0.127 0.256 0.389 0.530 0.683 0.855 1.06 1.31 1.70 2.05 2.47 2.76 3.67 29 0.127 0.256 0.389 0.530 0.683 0.854 1.05 1.31 1.70 2.04 2.46 2.76 3.66 30 0.127 0.256 0.389 0.530 0.683 0.854 1.05 1.31 1.70 2.04 2.46 2.75 3.65 40 0.126 0.255 0.388 0.529 0.681 0.851 1.05 1.30 1.68 2.02 2.42 2.70 3.55 80 0.126 0.254 0.387 0.526 0.678 0.846 1.04 1.29 1.66 1.99 2.37 2.64 3.42 120 0.126 0.254 0.386 0.526 0.677 0.845 1.04 1.29 1.66 1.98 2.36 2.62 3.37 \\(+\\infty\\) 0.126 0.253 0.385 0.524 0.674 0.842 1.04 1.28 1.64 1.96 2.33 2.58 3.29 "],
["tab-khideux.html", "E Table de la loi de Khi-deux \\(\\chi^2\\)", " E Table de la loi de Khi-deux \\(\\chi^2\\) \\(X\\) étant une variable alétoire de loi de \\(\\chi^2\\) à \\(n\\) degrés de liberté et \\(\\alpha\\) un réel de \\([0,1]\\), la table donne la valeur de \\(z_{n,\\alpha} = F_{\\chi_n^2}^{-1}(1-\\alpha)\\) telle que \\(P(X &gt; z_{n,\\alpha})=\\alpha\\). En , la commande correspondante est qchisq(1-alpha, n). \\(n \\backslash \\alpha\\) 0.995 0.99 0.975 0.95 0.9 0.8 0.7 0.5 0.3 0.2 0.1 0.05 0.025 0.01 0.005 0.001 1 0.000 0.000 0.001 0.004 0.016 0.064 0.148 0.455 1.07 1.64 2.71 3.84 5.02 6.63 7.88 10.8 2 0.010 0.020 0.051 0.103 0.211 0.446 0.713 1.386 2.41 3.22 4.61 5.99 7.38 9.21 10.60 13.8 3 0.072 0.115 0.216 0.352 0.584 1.005 1.424 2.366 3.66 4.64 6.25 7.82 9.35 11.35 12.84 16.3 4 0.207 0.297 0.484 0.711 1.064 1.649 2.195 3.357 4.88 5.99 7.78 9.49 11.14 13.28 14.86 18.5 5 0.412 0.554 0.831 1.145 1.610 2.343 3.000 4.351 6.06 7.29 9.24 11.07 12.83 15.09 16.75 20.5 6 0.676 0.872 1.237 1.635 2.204 3.070 3.828 5.348 7.23 8.56 10.64 12.59 14.45 16.81 18.55 22.5 7 0.989 1.239 1.690 2.167 2.833 3.822 4.671 6.346 8.38 9.80 12.02 14.07 16.01 18.48 20.28 24.3 8 1.344 1.646 2.180 2.733 3.490 4.594 5.527 7.344 9.52 11.03 13.36 15.51 17.54 20.09 21.95 26.1 9 1.735 2.088 2.700 3.325 4.168 5.380 6.393 8.343 10.66 12.24 14.68 16.92 19.02 21.67 23.59 27.9 10 2.156 2.558 3.247 3.940 4.865 6.179 7.267 9.342 11.78 13.44 15.99 18.31 20.48 23.21 25.19 29.6 11 2.603 3.053 3.816 4.575 5.578 6.989 8.148 10.341 12.90 14.63 17.27 19.68 21.92 24.73 26.76 31.3 12 3.074 3.571 4.404 5.226 6.304 7.807 9.034 11.340 14.01 15.81 18.55 21.03 23.34 26.22 28.30 32.9 13 3.565 4.107 5.009 5.892 7.042 8.634 9.926 12.340 15.12 16.98 19.81 22.36 24.74 27.69 29.82 34.5 14 4.075 4.660 5.629 6.571 7.790 9.467 10.821 13.339 16.22 18.15 21.06 23.68 26.12 29.14 31.32 36.1 15 4.601 5.229 6.262 7.261 8.547 10.307 11.721 14.339 17.32 19.31 22.31 25.00 27.49 30.58 32.80 37.7 16 5.142 5.812 6.908 7.962 9.312 11.152 12.624 15.338 18.42 20.46 23.54 26.30 28.84 32.00 34.27 39.3 17 5.697 6.408 7.564 8.672 10.085 12.002 13.531 16.338 19.51 21.61 24.77 27.59 30.19 33.41 35.72 40.8 18 6.265 7.015 8.231 9.390 10.865 12.857 14.440 17.338 20.60 22.76 25.99 28.87 31.53 34.80 37.16 42.3 19 6.844 7.633 8.907 10.117 11.651 13.716 15.352 18.338 21.69 23.90 27.20 30.14 32.85 36.19 38.58 43.8 20 7.434 8.260 9.591 10.851 12.443 14.578 16.266 19.337 22.77 25.04 28.41 31.41 34.17 37.57 40.00 45.3 21 8.034 8.897 10.283 11.591 13.240 15.445 17.182 20.337 23.86 26.17 29.61 32.67 35.48 38.93 41.40 46.8 22 8.643 9.542 10.982 12.338 14.041 16.314 18.101 21.337 24.94 27.30 30.81 33.92 36.78 40.29 42.80 48.3 23 9.260 10.196 11.689 13.091 14.848 17.187 19.021 22.337 26.02 28.43 32.01 35.17 38.08 41.64 44.18 49.7 24 9.886 10.856 12.401 13.848 15.659 18.062 19.943 23.337 27.10 29.55 33.20 36.41 39.36 42.98 45.56 51.2 25 10.520 11.524 13.120 14.611 16.473 18.940 20.867 24.337 28.17 30.68 34.38 37.65 40.65 44.31 46.93 52.6 26 11.160 12.198 13.844 15.379 17.292 19.820 21.792 25.336 29.25 31.80 35.56 38.88 41.92 45.64 48.29 54.1 27 11.808 12.879 14.573 16.151 18.114 20.703 22.719 26.336 30.32 32.91 36.74 40.11 43.20 46.96 49.65 55.5 28 12.461 13.565 15.308 16.928 18.939 21.588 23.647 27.336 31.39 34.03 37.92 41.34 44.46 48.28 50.99 56.9 29 13.121 14.256 16.047 17.708 19.768 22.475 24.577 28.336 32.46 35.14 39.09 42.56 45.72 49.59 52.34 58.3 30 13.787 14.953 16.791 18.493 20.599 23.364 25.508 29.336 33.53 36.25 40.26 43.77 46.98 50.89 53.67 59.7 "],
["tab-fisher.html", "F Table de la loi de Fisher \\(F\\) F.1 Pour \\(\\alpha = 5\\%\\) F.2 Pour \\(\\alpha = 2.5\\%\\) F.3 Pour \\(\\alpha = 1\\%\\) F.4 Pour \\(\\alpha = 0.05\\%\\)", " F Table de la loi de Fisher \\(F\\) \\(X\\) étant une variable aléatoire de loi \\(F(\\nu_1,\\nu_2)\\), les tables donnent les valeurs de \\(f_{1-\\alpha; \\,\\nu_1,\\nu_2}= F_{F(\\nu_1,\\nu_2)}^{-1}(1-\\alpha)\\) telles que \\(P(X&lt; f_{1-\\alpha; \\,\\nu_1,\\nu_2})=1-\\alpha\\) pour \\(\\alpha = 5\\%\\) et \\(\\alpha =1\\%\\). En , la commande correspondante est qf(1-alpha, nu1, nu2). Remarque: \\(f_{\\alpha; \\, \\nu_1,\\nu_2} = \\frac{1}{f_{1-\\alpha; \\, \\nu_2,\\nu_1}}\\) F.1 Pour \\(\\alpha = 5\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88 243.91 246.46 248.01 249.05 251.14 252.20 253.04 254.31 2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.39 19.40 19.41 19.43 19.45 19.45 19.47 19.48 19.49 19.50 3 10.13 9.55 9.28 9.12 9.01 8.94 8.89 8.85 8.81 8.79 8.74 8.69 8.66 8.64 8.59 8.57 8.55 8.53 4 7.71 6.94 6.59 6.39 6.26 6.16 6.09 6.04 6.00 5.96 5.91 5.84 5.80 5.77 5.72 5.69 5.66 5.63 5 6.61 5.79 5.41 5.19 5.05 4.95 4.88 4.82 4.77 4.74 4.68 4.60 4.56 4.53 4.46 4.43 4.41 4.37 6 5.99 5.14 4.76 4.53 4.39 4.28 4.21 4.15 4.10 4.06 4.00 3.92 3.87 3.84 3.77 3.74 3.71 3.67 7 5.59 4.74 4.35 4.12 3.97 3.87 3.79 3.73 3.68 3.64 3.58 3.49 3.44 3.41 3.34 3.30 3.27 3.23 8 5.32 4.46 4.07 3.84 3.69 3.58 3.50 3.44 3.39 3.35 3.28 3.20 3.15 3.12 3.04 3.00 2.98 2.93 9 5.12 4.26 3.86 3.63 3.48 3.37 3.29 3.23 3.18 3.14 3.07 2.99 2.94 2.90 2.83 2.79 2.76 2.71 10 4.96 4.10 3.71 3.48 3.33 3.22 3.13 3.07 3.02 2.98 2.91 2.83 2.77 2.74 2.66 2.62 2.59 2.54 11 4.84 3.98 3.59 3.36 3.20 3.10 3.01 2.95 2.90 2.85 2.79 2.70 2.65 2.61 2.53 2.49 2.46 2.40 12 4.75 3.88 3.49 3.26 3.11 3.00 2.91 2.85 2.80 2.75 2.69 2.60 2.54 2.50 2.43 2.38 2.35 2.30 13 4.67 3.81 3.41 3.18 3.02 2.92 2.83 2.77 2.71 2.67 2.60 2.52 2.46 2.42 2.34 2.30 2.26 2.21 14 4.60 3.74 3.34 3.11 2.96 2.85 2.76 2.70 2.65 2.60 2.53 2.44 2.39 2.35 2.27 2.22 2.19 2.13 15 4.54 3.68 3.29 3.06 2.90 2.79 2.71 2.64 2.59 2.54 2.48 2.38 2.33 2.29 2.20 2.16 2.12 2.07 16 4.49 3.63 3.24 3.01 2.85 2.74 2.66 2.59 2.54 2.49 2.42 2.33 2.28 2.23 2.15 2.11 2.07 2.01 17 4.45 3.59 3.20 2.96 2.81 2.70 2.61 2.55 2.49 2.45 2.38 2.29 2.23 2.19 2.10 2.06 2.02 1.96 18 4.41 3.56 3.16 2.93 2.77 2.66 2.58 2.51 2.46 2.41 2.34 2.25 2.19 2.15 2.06 2.02 1.98 1.92 19 4.38 3.52 3.13 2.90 2.74 2.63 2.54 2.48 2.42 2.38 2.31 2.21 2.15 2.11 2.03 1.98 1.94 1.88 20 4.35 3.49 3.10 2.87 2.71 2.60 2.51 2.45 2.39 2.35 2.28 2.18 2.12 2.08 1.99 1.95 1.91 1.84 21 4.33 3.47 3.07 2.84 2.68 2.57 2.49 2.42 2.37 2.32 2.25 2.16 2.10 2.05 1.97 1.92 1.88 1.81 22 4.30 3.44 3.05 2.82 2.66 2.55 2.46 2.40 2.34 2.30 2.23 2.13 2.07 2.03 1.94 1.89 1.85 1.78 23 4.28 3.42 3.03 2.80 2.64 2.53 2.44 2.38 2.32 2.27 2.20 2.11 2.05 2.00 1.91 1.86 1.82 1.76 24 4.26 3.40 3.01 2.78 2.62 2.51 2.42 2.35 2.30 2.25 2.18 2.09 2.03 1.98 1.89 1.84 1.80 1.73 25 4.24 3.38 2.99 2.76 2.60 2.49 2.40 2.34 2.28 2.24 2.16 2.07 2.01 1.96 1.87 1.82 1.78 1.71 26 4.22 3.37 2.98 2.74 2.59 2.47 2.39 2.32 2.27 2.22 2.15 2.05 1.99 1.95 1.85 1.80 1.76 1.69 27 4.21 3.35 2.96 2.73 2.57 2.46 2.37 2.31 2.25 2.20 2.13 2.04 1.97 1.93 1.84 1.78 1.74 1.67 28 4.20 3.34 2.95 2.71 2.56 2.44 2.36 2.29 2.24 2.19 2.12 2.02 1.96 1.92 1.82 1.77 1.73 1.65 29 4.18 3.33 2.93 2.70 2.54 2.43 2.35 2.28 2.22 2.18 2.10 2.01 1.95 1.90 1.81 1.75 1.71 1.64 30 4.17 3.32 2.92 2.69 2.53 2.42 2.33 2.27 2.21 2.16 2.09 2.00 1.93 1.89 1.79 1.74 1.70 1.62 40 4.08 3.23 2.84 2.61 2.45 2.34 2.25 2.18 2.12 2.08 2.00 1.90 1.84 1.79 1.69 1.64 1.59 1.51 50 4.03 3.18 2.79 2.56 2.40 2.29 2.20 2.13 2.07 2.03 1.95 1.85 1.78 1.74 1.63 1.58 1.52 1.44 60 4.00 3.15 2.76 2.52 2.37 2.25 2.17 2.10 2.04 1.99 1.92 1.81 1.75 1.70 1.59 1.53 1.48 1.39 100 3.94 3.09 2.70 2.46 2.31 2.19 2.10 2.03 1.98 1.93 1.85 1.75 1.68 1.63 1.51 1.45 1.39 1.28 \\(+\\infty\\) 3.84 3.00 2.60 2.37 2.21 2.10 2.01 1.94 1.88 1.83 1.75 1.64 1.57 1.52 1.39 1.32 1.24 1.00 F.2 Pour \\(\\alpha = 2.5\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 647.79 799.50 864.16 899.58 921.85 937.11 948.22 956.66 963.28 968.63 976.71 986.92 993.10 997.25 1005.60 1009.80 1013.17 1018.26 2 38.51 39.00 39.16 39.25 39.30 39.33 39.35 39.37 39.39 39.40 39.41 39.44 39.45 39.46 39.47 39.48 39.49 39.50 3 17.44 16.04 15.44 15.10 14.88 14.73 14.62 14.54 14.47 14.42 14.34 14.23 14.17 14.12 14.04 13.99 13.96 13.90 4 12.22 10.65 9.98 9.61 9.36 9.20 9.07 8.98 8.90 8.84 8.75 8.63 8.56 8.51 8.41 8.36 8.32 8.26 5 10.01 8.43 7.76 7.39 7.15 6.98 6.85 6.76 6.68 6.62 6.53 6.40 6.33 6.28 6.17 6.12 6.08 6.01 6 8.81 7.26 6.60 6.23 5.99 5.82 5.70 5.60 5.52 5.46 5.37 5.24 5.17 5.12 5.01 4.96 4.92 4.85 7 8.07 6.54 5.89 5.52 5.29 5.12 5.00 4.90 4.82 4.76 4.67 4.54 4.47 4.42 4.31 4.25 4.21 4.14 8 7.57 6.06 5.42 5.05 4.82 4.65 4.53 4.43 4.36 4.29 4.20 4.08 4.00 3.95 3.84 3.78 3.74 3.67 9 7.21 5.71 5.08 4.72 4.48 4.32 4.20 4.10 4.03 3.96 3.87 3.74 3.67 3.61 3.50 3.45 3.40 3.33 10 6.94 5.46 4.83 4.47 4.24 4.07 3.95 3.85 3.78 3.72 3.62 3.50 3.42 3.37 3.25 3.20 3.15 3.08 11 6.72 5.26 4.63 4.28 4.04 3.88 3.76 3.66 3.59 3.53 3.43 3.30 3.23 3.17 3.06 3.00 2.96 2.88 12 6.55 5.10 4.47 4.12 3.89 3.73 3.61 3.51 3.44 3.37 3.28 3.15 3.07 3.02 2.91 2.85 2.80 2.73 13 6.41 4.96 4.35 4.00 3.77 3.60 3.48 3.39 3.31 3.25 3.15 3.03 2.95 2.89 2.78 2.72 2.67 2.60 14 6.30 4.86 4.24 3.89 3.66 3.50 3.38 3.29 3.21 3.15 3.05 2.92 2.84 2.79 2.67 2.61 2.56 2.49 15 6.20 4.76 4.15 3.80 3.58 3.42 3.29 3.20 3.12 3.06 2.96 2.84 2.76 2.70 2.58 2.52 2.47 2.40 16 6.12 4.69 4.08 3.73 3.50 3.34 3.22 3.12 3.05 2.99 2.89 2.76 2.68 2.62 2.51 2.45 2.40 2.32 17 6.04 4.62 4.01 3.66 3.44 3.28 3.16 3.06 2.98 2.92 2.83 2.70 2.62 2.56 2.44 2.38 2.33 2.25 18 5.98 4.56 3.95 3.61 3.38 3.22 3.10 3.00 2.93 2.87 2.77 2.64 2.56 2.50 2.38 2.32 2.27 2.19 19 5.92 4.51 3.90 3.56 3.33 3.17 3.05 2.96 2.88 2.82 2.72 2.59 2.51 2.45 2.33 2.27 2.22 2.13 20 5.87 4.46 3.86 3.52 3.29 3.13 3.01 2.91 2.84 2.77 2.68 2.55 2.46 2.41 2.29 2.22 2.17 2.08 21 5.83 4.42 3.82 3.48 3.25 3.09 2.97 2.87 2.80 2.73 2.64 2.51 2.42 2.37 2.25 2.18 2.13 2.04 22 5.79 4.38 3.78 3.44 3.21 3.06 2.93 2.84 2.76 2.70 2.60 2.47 2.39 2.33 2.21 2.14 2.09 2.00 23 5.75 4.35 3.75 3.41 3.18 3.02 2.90 2.81 2.73 2.67 2.57 2.44 2.36 2.30 2.18 2.11 2.06 1.97 24 5.72 4.32 3.72 3.38 3.15 3.00 2.87 2.78 2.70 2.64 2.54 2.41 2.33 2.27 2.15 2.08 2.02 1.94 25 5.69 4.29 3.69 3.35 3.13 2.97 2.85 2.75 2.68 2.61 2.52 2.38 2.30 2.24 2.12 2.05 2.00 1.91 26 5.66 4.26 3.67 3.33 3.10 2.94 2.82 2.73 2.65 2.59 2.49 2.36 2.28 2.22 2.09 2.03 1.97 1.88 27 5.63 4.24 3.65 3.31 3.08 2.92 2.80 2.71 2.63 2.57 2.47 2.34 2.25 2.19 2.07 2.00 1.95 1.85 28 5.61 4.22 3.63 3.29 3.06 2.90 2.78 2.69 2.61 2.55 2.45 2.32 2.23 2.17 2.05 1.98 1.92 1.83 29 5.59 4.20 3.61 3.27 3.04 2.88 2.76 2.67 2.59 2.53 2.43 2.30 2.21 2.15 2.03 1.96 1.90 1.81 30 5.57 4.18 3.59 3.25 3.03 2.87 2.75 2.65 2.58 2.51 2.41 2.28 2.19 2.14 2.01 1.94 1.88 1.79 40 5.42 4.05 3.46 3.13 2.90 2.74 2.62 2.53 2.45 2.39 2.29 2.15 2.07 2.01 1.88 1.80 1.74 1.64 50 5.34 3.98 3.39 3.05 2.83 2.67 2.55 2.46 2.38 2.32 2.22 2.08 1.99 1.93 1.80 1.72 1.66 1.54 60 5.29 3.92 3.34 3.01 2.79 2.63 2.51 2.41 2.33 2.27 2.17 2.03 1.94 1.88 1.74 1.67 1.60 1.48 100 5.18 3.83 3.25 2.92 2.70 2.54 2.42 2.32 2.24 2.18 2.08 1.94 1.85 1.78 1.64 1.56 1.48 1.35 \\(+\\infty\\) 5.02 3.69 3.12 2.79 2.57 2.41 2.29 2.19 2.11 2.05 1.95 1.80 1.71 1.64 1.48 1.39 1.30 1.00 F.3 Pour \\(\\alpha = 1\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 4052.18 4999.50 5403.35 5624.58 5763.65 5858.99 5928.36 5981.07 6022.47 6055.85 6106.32 6170.10 6208.73 6234.63 6286.78 6313.03 6334.11 6365.86 2 98.50 99.00 99.17 99.25 99.30 99.33 99.36 99.37 99.39 99.40 99.42 99.44 99.45 99.46 99.47 99.48 99.49 99.50 3 34.12 30.82 29.46 28.71 28.24 27.91 27.67 27.49 27.34 27.23 27.05 26.83 26.69 26.60 26.41 26.32 26.24 26.12 4 21.20 18.00 16.69 15.98 15.52 15.21 14.98 14.80 14.66 14.55 14.37 14.15 14.02 13.93 13.74 13.65 13.58 13.46 5 16.26 13.27 12.06 11.39 10.97 10.67 10.46 10.29 10.16 10.05 9.89 9.68 9.55 9.47 9.29 9.20 9.13 9.02 6 13.74 10.93 9.78 9.15 8.75 8.47 8.26 8.10 7.98 7.87 7.72 7.52 7.40 7.31 7.14 7.06 6.99 6.88 7 12.25 9.55 8.45 7.85 7.46 7.19 6.99 6.84 6.72 6.62 6.47 6.28 6.16 6.07 5.91 5.82 5.75 5.65 8 11.26 8.65 7.59 7.01 6.63 6.37 6.18 6.03 5.91 5.81 5.67 5.48 5.36 5.28 5.12 5.03 4.96 4.86 9 10.56 8.02 6.99 6.42 6.06 5.80 5.61 5.47 5.35 5.26 5.11 4.92 4.81 4.73 4.57 4.48 4.42 4.31 10 10.04 7.56 6.55 5.99 5.64 5.39 5.20 5.06 4.94 4.85 4.71 4.52 4.41 4.33 4.16 4.08 4.01 3.91 11 9.65 7.21 6.22 5.67 5.32 5.07 4.89 4.74 4.63 4.54 4.40 4.21 4.10 4.02 3.86 3.78 3.71 3.60 12 9.33 6.93 5.95 5.41 5.06 4.82 4.64 4.50 4.39 4.30 4.16 3.97 3.86 3.78 3.62 3.54 3.47 3.36 13 9.07 6.70 5.74 5.21 4.86 4.62 4.44 4.30 4.19 4.10 3.96 3.78 3.66 3.59 3.42 3.34 3.27 3.16 14 8.86 6.51 5.56 5.04 4.70 4.46 4.28 4.14 4.03 3.94 3.80 3.62 3.50 3.43 3.27 3.18 3.11 3.00 15 8.68 6.36 5.42 4.89 4.56 4.32 4.14 4.00 3.90 3.81 3.67 3.48 3.37 3.29 3.13 3.05 2.98 2.87 16 8.53 6.23 5.29 4.77 4.44 4.20 4.03 3.89 3.78 3.69 3.55 3.37 3.26 3.18 3.02 2.93 2.86 2.75 17 8.40 6.11 5.18 4.67 4.34 4.10 3.93 3.79 3.68 3.59 3.46 3.27 3.16 3.08 2.92 2.83 2.76 2.65 18 8.29 6.01 5.09 4.58 4.25 4.01 3.84 3.71 3.60 3.51 3.37 3.19 3.08 3.00 2.83 2.75 2.68 2.57 19 8.19 5.93 5.01 4.50 4.17 3.94 3.77 3.63 3.52 3.43 3.30 3.12 3.00 2.92 2.76 2.67 2.60 2.49 20 8.10 5.85 4.94 4.43 4.10 3.87 3.70 3.56 3.46 3.37 3.23 3.05 2.94 2.86 2.69 2.61 2.54 2.42 21 8.02 5.78 4.87 4.37 4.04 3.81 3.64 3.51 3.40 3.31 3.17 2.99 2.88 2.80 2.64 2.55 2.48 2.36 22 7.95 5.72 4.82 4.31 3.99 3.76 3.59 3.45 3.35 3.26 3.12 2.94 2.83 2.75 2.58 2.50 2.42 2.31 23 7.88 5.66 4.76 4.26 3.94 3.71 3.54 3.41 3.30 3.21 3.07 2.89 2.78 2.70 2.54 2.45 2.37 2.26 24 7.82 5.61 4.72 4.22 3.90 3.67 3.50 3.36 3.26 3.17 3.03 2.85 2.74 2.66 2.49 2.40 2.33 2.21 25 7.77 5.57 4.67 4.18 3.85 3.63 3.46 3.32 3.22 3.13 2.99 2.81 2.70 2.62 2.45 2.36 2.29 2.17 26 7.72 5.53 4.64 4.14 3.82 3.59 3.42 3.29 3.18 3.09 2.96 2.78 2.66 2.58 2.42 2.33 2.25 2.13 27 7.68 5.49 4.60 4.11 3.79 3.56 3.39 3.26 3.15 3.06 2.93 2.75 2.63 2.55 2.38 2.29 2.22 2.10 28 7.64 5.45 4.57 4.07 3.75 3.53 3.36 3.23 3.12 3.03 2.90 2.72 2.60 2.52 2.35 2.26 2.19 2.06 29 7.60 5.42 4.54 4.04 3.73 3.50 3.33 3.20 3.09 3.00 2.87 2.69 2.57 2.50 2.33 2.23 2.16 2.03 30 7.56 5.39 4.51 4.02 3.70 3.47 3.30 3.17 3.07 2.98 2.84 2.66 2.55 2.47 2.30 2.21 2.13 2.01 40 7.31 5.18 4.31 3.83 3.51 3.29 3.12 2.99 2.89 2.80 2.66 2.48 2.37 2.29 2.11 2.02 1.94 1.80 50 7.17 5.06 4.20 3.72 3.41 3.19 3.02 2.89 2.79 2.70 2.56 2.38 2.27 2.18 2.01 1.91 1.82 1.68 60 7.08 4.98 4.13 3.65 3.34 3.12 2.95 2.82 2.72 2.63 2.50 2.31 2.20 2.12 1.94 1.84 1.75 1.60 100 6.89 4.82 3.98 3.51 3.21 2.99 2.82 2.69 2.59 2.50 2.37 2.18 2.07 1.98 1.80 1.69 1.60 1.43 \\(+\\infty\\) 6.63 4.61 3.78 3.32 3.02 2.80 2.64 2.51 2.41 2.32 2.18 2.00 1.88 1.79 1.59 1.47 1.36 1.00 F.4 Pour \\(\\alpha = 0.05\\%\\) \\(\\nu_2 \\backslash \\nu_1\\) 1 2 3 4 5 6 7 8 9 10 12 16 20 24 40 60 100 \\(+\\infty\\) 1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88 243.91 246.46 248.01 249.05 251.14 252.20 253.04 254.31 2 18.51 19.00 19.16 19.25 19.30 19.33 19.35 19.37 19.39 19.40 19.41 19.43 19.45 19.45 19.47 19.48 19.49 19.50 3 10.13 9.55 9.28 9.12 9.01 8.94 8.89 8.85 8.81 8.79 8.74 8.69 8.66 8.64 8.59 8.57 8.55 8.53 4 7.71 6.94 6.59 6.39 6.26 6.16 6.09 6.04 6.00 5.96 5.91 5.84 5.80 5.77 5.72 5.69 5.66 5.63 5 6.61 5.79 5.41 5.19 5.05 4.95 4.88 4.82 4.77 4.74 4.68 4.60 4.56 4.53 4.46 4.43 4.41 4.37 6 5.99 5.14 4.76 4.53 4.39 4.28 4.21 4.15 4.10 4.06 4.00 3.92 3.87 3.84 3.77 3.74 3.71 3.67 7 5.59 4.74 4.35 4.12 3.97 3.87 3.79 3.73 3.68 3.64 3.58 3.49 3.44 3.41 3.34 3.30 3.27 3.23 8 5.32 4.46 4.07 3.84 3.69 3.58 3.50 3.44 3.39 3.35 3.28 3.20 3.15 3.12 3.04 3.00 2.98 2.93 9 5.12 4.26 3.86 3.63 3.48 3.37 3.29 3.23 3.18 3.14 3.07 2.99 2.94 2.90 2.83 2.79 2.76 2.71 10 4.96 4.10 3.71 3.48 3.33 3.22 3.13 3.07 3.02 2.98 2.91 2.83 2.77 2.74 2.66 2.62 2.59 2.54 11 4.84 3.98 3.59 3.36 3.20 3.10 3.01 2.95 2.90 2.85 2.79 2.70 2.65 2.61 2.53 2.49 2.46 2.40 12 4.75 3.88 3.49 3.26 3.11 3.00 2.91 2.85 2.80 2.75 2.69 2.60 2.54 2.50 2.43 2.38 2.35 2.30 13 4.67 3.81 3.41 3.18 3.02 2.92 2.83 2.77 2.71 2.67 2.60 2.52 2.46 2.42 2.34 2.30 2.26 2.21 14 4.60 3.74 3.34 3.11 2.96 2.85 2.76 2.70 2.65 2.60 2.53 2.44 2.39 2.35 2.27 2.22 2.19 2.13 15 4.54 3.68 3.29 3.06 2.90 2.79 2.71 2.64 2.59 2.54 2.48 2.38 2.33 2.29 2.20 2.16 2.12 2.07 16 4.49 3.63 3.24 3.01 2.85 2.74 2.66 2.59 2.54 2.49 2.42 2.33 2.28 2.23 2.15 2.11 2.07 2.01 17 4.45 3.59 3.20 2.96 2.81 2.70 2.61 2.55 2.49 2.45 2.38 2.29 2.23 2.19 2.10 2.06 2.02 1.96 18 4.41 3.56 3.16 2.93 2.77 2.66 2.58 2.51 2.46 2.41 2.34 2.25 2.19 2.15 2.06 2.02 1.98 1.92 19 4.38 3.52 3.13 2.90 2.74 2.63 2.54 2.48 2.42 2.38 2.31 2.21 2.15 2.11 2.03 1.98 1.94 1.88 20 4.35 3.49 3.10 2.87 2.71 2.60 2.51 2.45 2.39 2.35 2.28 2.18 2.12 2.08 1.99 1.95 1.91 1.84 21 4.33 3.47 3.07 2.84 2.68 2.57 2.49 2.42 2.37 2.32 2.25 2.16 2.10 2.05 1.97 1.92 1.88 1.81 22 4.30 3.44 3.05 2.82 2.66 2.55 2.46 2.40 2.34 2.30 2.23 2.13 2.07 2.03 1.94 1.89 1.85 1.78 23 4.28 3.42 3.03 2.80 2.64 2.53 2.44 2.38 2.32 2.27 2.20 2.11 2.05 2.00 1.91 1.86 1.82 1.76 24 4.26 3.40 3.01 2.78 2.62 2.51 2.42 2.35 2.30 2.25 2.18 2.09 2.03 1.98 1.89 1.84 1.80 1.73 25 4.24 3.38 2.99 2.76 2.60 2.49 2.40 2.34 2.28 2.24 2.16 2.07 2.01 1.96 1.87 1.82 1.78 1.71 26 4.22 3.37 2.98 2.74 2.59 2.47 2.39 2.32 2.27 2.22 2.15 2.05 1.99 1.95 1.85 1.80 1.76 1.69 27 4.21 3.35 2.96 2.73 2.57 2.46 2.37 2.31 2.25 2.20 2.13 2.04 1.97 1.93 1.84 1.78 1.74 1.67 28 4.20 3.34 2.95 2.71 2.56 2.44 2.36 2.29 2.24 2.19 2.12 2.02 1.96 1.92 1.82 1.77 1.73 1.65 29 4.18 3.33 2.93 2.70 2.54 2.43 2.35 2.28 2.22 2.18 2.10 2.01 1.95 1.90 1.81 1.75 1.71 1.64 30 4.17 3.32 2.92 2.69 2.53 2.42 2.33 2.27 2.21 2.16 2.09 2.00 1.93 1.89 1.79 1.74 1.70 1.62 40 4.08 3.23 2.84 2.61 2.45 2.34 2.25 2.18 2.12 2.08 2.00 1.90 1.84 1.79 1.69 1.64 1.59 1.51 50 4.03 3.18 2.79 2.56 2.40 2.29 2.20 2.13 2.07 2.03 1.95 1.85 1.78 1.74 1.63 1.58 1.52 1.44 60 4.00 3.15 2.76 2.52 2.37 2.25 2.17 2.10 2.04 1.99 1.92 1.81 1.75 1.70 1.59 1.53 1.48 1.39 100 3.94 3.09 2.70 2.46 2.31 2.19 2.10 2.03 1.98 1.93 1.85 1.75 1.68 1.63 1.51 1.45 1.39 1.28 \\(+\\infty\\) 3.84 3.00 2.60 2.37 2.21 2.10 2.01 1.94 1.88 1.83 1.75 1.64 1.57 1.52 1.39 1.32 1.24 1.00 "]
]
