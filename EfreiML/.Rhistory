X_train.ndim
exit
getwd()
rmarkdown::render("TP2.Rmd", output_file = "TP2_corrections.html")
reticulate::repl_python()
# Scientific and vector computation for python
import numpy as np
# Plotting library
import matplotlib.pyplot as plt
# Optimization module in scipy
from scipy import optimize
# do not forget to tell matplotlib to embed plots within the notebook
%matplotlib inline
data = np.loadtxt("tp2data1.txt", delimiter=',')
X = data[:, 0:2]
y = data[:, 2]
data = np.loadtxt("tp2data1.txt", delimiter=',')
X = data[:, 0:2]
y = data[:, 2]
# Chunk 1
#sidebar h2 {
background-color: #077BBF;
}
#postamble {
color: #fcfcfc;
background: #077BBF;
border-top: solid 10px #077BBF;
}
#sidebar {
background: #000;
}
h1, h2, h3, h4, h5, h6, legend {
color: #077BBF;
}
# Chunk 2: setup
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
# Chunk 4
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from scipy import optimize
# Chunk 5
data = np.loadtxt("tp2data1.txt", delimiter=',')
X = data[:, 0:2]
y = data[:, 2]
# Chunk 6
data = np.loadtxt("tp2data1.txt", delimiter=',')
X = data[:, 0:2]
y = data[:, 2]
# Chunk 7
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Admitted")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Not admitted")
plt.xlabel("Exam 1 score")
plt.ylabel("Exam 2 score")
plt.legend()
plt.show()
# Chunk 8
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Admitted")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Not admitted")
plt.xlabel("Exam 1 score")
plt.ylabel("Exam 2 score")
plt.legend()
plt.show()
# Chunk 9
def sigmoid(z):
return 1 / (1 + np.exp(-z))
plt.figure()
X_plot = np.linspace(-10, 10, 100)
plt.plot(X_plot, sigmoid(X_plot))
plt.show()
# Chunk 10
def sigmoid(z):
return 1 / (1 + np.exp(-z))
# Chunk 11
plt.figure()
X_plot = np.linspace(-10, 10, 100)
plt.plot(X_plot, sigmoid(X_plot))
plt.show()
# Chunk 12
# Setup the data matrix appropriately, and add ones for the intercept term
m, n = X.shape
# Add intercept term to X
X = np.concatenate([np.ones((m, 1)), X], axis=1)
# Or you can use hpstack
X = np.hstack((np.ones((X.shape[0], 1)), X))
# Chunk 13
def costFunction(theta, X, y):
h = sigmoid(np.dot(X, theta))
J = -(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
grad = np.dot(X.T, h - y) / X.shape[0]
return J, grad
# Chunk 14
X_train = np.hstack((np.ones((X.shape[0], 1)), X))
theta = np.array([0, 0, 0])
J, grad = costFunction(theta, X_train, y)
print(J)  # 0.693
print(grad)  # -0.1000 -12.0092 -11.2628
# Chunk 15
X_train = np.hstack((np.ones((X.shape[0], 1)), X))
theta = np.array([-24, 0.2, 0.2])
J, grad = costFunction(theta, X_train, y)
print(J)  # 0.218
print(grad)  # 0.043 2.566 2.647
# Chunk 17
def cost(theta, X, y):
h = sigmoid(np.dot(X, theta))
J = - (np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
return J
def gradient(theta, X, y):
h = sigmoid(np.dot(X, theta))
grad = np.dot(X.T, h - y) / X.shape[0]
return grad
X_train = np.hstack((np.ones((X.shape[0], 1)), X))
theta = np.array([0, 0, 0])
res = optimize.minimize(fun=cost, x0=theta, args=(X_train, y),
method='Newton-CG', jac=gradient)
print(res.fun)  # 0.203
print(res.x)  # -25.161 0.206 0.201
# Chunk 20
def sigmoid(z):
return 1 / (1 + np.exp(-z))
def cost(theta, X, y):
h = sigmoid(np.dot(X, theta))
J = - (np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
return J
def gradient(theta, X, y):
h = sigmoid(np.dot(X, theta))
grad = np.dot(X.T, h - y) / X.shape[0]
return grad
X_train = np.hstack((np.ones((X.shape[0], 1)), X))
theta = np.array([0, 0, 0])
res = optimize.minimize(fun=cost, x0=theta, args=(X_train, y),
method='Newton-CG', jac=gradient)
# print(res.fun)  # 0.203
# print(res.x)  # -25.161 0.206 0.201
def plotDecisionBoundary(theta, X, y):
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
np.arange(y_min, y_max, 0.1))
X_plot = np.c_[xx.ravel(), yy.ravel()]
X_plot = np.hstack((np.ones((X_plot.shape[0], 1)), X_plot))
y_plot = np.dot(X_plot, theta).reshape(xx.shape)
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="Admitted")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="Not admitted")
plt.contour(xx, yy, y_plot, levels=[0])
plt.xlabel("Exam 1 score")
plt.ylabel("Exam 2 score")
plt.legend()
plt.show()
plotDecisionBoundary(res.x, X, y)
# Chunk 21
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
# Chunk 22
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="y = 1")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="y = 0")
plt.xlabel("Microchip Test 1")
plt.ylabel("Microchip Test 2")
plt.legend(['y = 1', 'y = 0'], loc='upper right')
plt.show()
# Chunk 23
def mapFeature(X1, X2, degree=6):
"""
Maps the two input features to quadratic features used in the regularization exercise.
Returns a new feature array with more features, comprising of
X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
Parameters
----------
X1 : array_like
A vector of shape (m, 1), containing one feature for all examples.
X2 : array_like
A vector of shape (m, 1), containing a second feature for all examples.
Inputs X1, X2 must be the same size.
degree: int, optional
The polynomial degree.
Returns
-------
: array_like
A matrix of of m rows, and columns depend on the degree of polynomial.
"""
if X1.ndim > 0:
out = [np.ones(X1.shape[0])]
else:
out = [np.ones(1)]
for i in range(1, degree + 1):
for j in range(i + 1):
out.append((X1 ** (i - j)) * (X2 ** j))
if X1.ndim > 0:
return np.stack(out, axis=1)
else:
return np.array(out)
# Chunk 25
def costFunctionReg(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return J, grad
# Initialize fitting parameters
initial_theta = np.zeros(X.shape[1])
# Set regularization parameter lambda to 1
# DO NOT use `lambda` as a variable name in python
# because it is a python keyword
lambda_ = 1
# Compute and display initial cost and gradient for regularized logistic
# regression
cost, grad = costFunctionReg(initial_theta, X, y, lambda_)
print('Cost at initial theta (zeros): {:.3f}'.format(cost))
print('Expected cost (approx)       : 0.693\n')
print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n')
# Compute and display cost and gradient
# with all-ones theta and lambda = 10
test_theta = np.ones(X.shape[1])
cost, grad = costFunctionReg(test_theta, X, y, 10)
print('------------------------------\n')
print('Cost at test theta    : {:.2f}'.format(cost))
print('Expected cost (approx): 3.16\n')
print('Gradient at test theta - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')
def cost(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
return J
def gradient(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return grad
lam = 1
theta = np.zeros(X_train.shape[1])
res = optimize.minimize(fun=cost, x0=theta, args=(X_train, y, lam),
method='Newton-CG', jac=gradient)
print(res.fun)
print(res.x[:5])
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="y = 1")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="y = 0")
plt.xlabel("Microchip Test 1")
plt.ylabel("Microchip Test 2")
plt.legend(['y = 1', 'y = 0'], loc='upper right')
plt.show()
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="y = 1")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="y = 0")
plt.xlabel("Microchip Test 1")
plt.ylabel("Microchip Test 2")
plt.legend(['y = 1', 'y = 0'], loc='upper right')
plt.show()
data = np.loadtxt('tp2data2.txt', delimiter=',')
X = data[:, :2]
y = data[:, 2]
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="y = 1")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="y = 0")
plt.xlabel("Microchip Test 1")
plt.ylabel("Microchip Test 2")
plt.legend(['y = 1', 'y = 0'], loc='upper right')
plt.show()
def mapFeature(X1, X2, degree=6):
"""
Maps the two input features to quadratic features used in the regularization exercise.
Returns a new feature array with more features, comprising of
X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..
Parameters
----------
X1 : array_like
A vector of shape (m, 1), containing one feature for all examples.
X2 : array_like
A vector of shape (m, 1), containing a second feature for all examples.
Inputs X1, X2 must be the same size.
degree: int, optional
The polynomial degree.
Returns
-------
: array_like
A matrix of of m rows, and columns depend on the degree of polynomial.
"""
if X1.ndim > 0:
out = [np.ones(X1.shape[0])]
else:
out = [np.ones(1)]
for i in range(1, degree + 1):
for j in range(i + 1):
out.append((X1 ** (i - j)) * (X2 ** j))
if X1.ndim > 0:
return np.stack(out, axis=1)
else:
return np.array(out)
X1
X2
X1 = X[:, 0]
X2 = X[:, 1]
X1
X2
degree = 6
out = np.ones((X1.shape[0], 1))
for i in range(1, degree + 1):
for j in range(i + 1):
out = np.hstack((out, (X1 ** (i - j) * X2 ** j)[:, np.newaxis]))
out
out.shape
if X1.ndim > 0:
out = [np.ones(X1.shape[0])]
else:
out = [np.ones(1)]
out = [np.ones(X1.shape[0])]
for i in range(1, degree + 1):
for j in range(i + 1):
out.append((X1 ** (i - j)) * (X2 ** j))
for i in range(1, degree + 1):
for j in range(i + 1):
out.append((X1 ** (i - j)) * (X2 ** j))
np.stack(out, axis=1)
np.stack(out, axis=1).shape
def costFunctionReg(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return J, grad
lambda_ = 1
# Compute and display initial cost and gradient for regularized logistic
# regression
cost, grad = costFunctionReg(initial_theta, X, y, lambda_)
print('Cost at initial theta (zeros): {:.3f}'.format(cost))
print('Expected cost (approx)       : 0.693\n')
print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
grad
X
y
# X_tain has 28 columns
initial_theta = np.zeros(X_train.shape[1])
initial_theta
X_tain.shape
X_train.shape
X_train = mapFeature(X[:, 0], X[:, 1])
X_train.shape
def costFunctionReg(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return J, grad
initial_theta = np.zeros(X_train.shape[1])
# Set regularization parameter lambda to 1
# DO NOT use `lambda` as a variable name in python
# because it is a python keyword
lambda_ = 1
# Compute and display initial cost and gradient for regularized logistic
# regression
cost, grad = costFunctionReg(initial_theta, X_train, y, lambda_)
print('Cost at initial theta (zeros): {:.3f}'.format(cost))
print('Expected cost (approx)       : 0.693\n')
print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n')
# Compute and display cost and gradient
# with all-ones theta and lambda = 10
test_theta = np.ones(X.shape[1])
cost, grad = costFunctionReg(test_theta, X, y, 10)
print('------------------------------\n')
print('Cost at test theta    : {:.2f}'.format(cost))
print('Expected cost (approx): 3.16\n')
print('Gradient at test theta - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')
X = mapFeature(X[:, 0], X[:, 1])
def costFunctionReg(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return J, grad
# Initialize fitting parameters
# X here has 28 columns
initial_theta = np.zeros(X.shape[1])
# Set regularization parameter lambda to 1
# DO NOT use `lambda` as a variable name in python
# because it is a python keyword
lambda_ = 1
# Compute and display initial cost and gradient for regularized logistic
# regression
cost, grad = costFunctionReg(initial_theta, X, y, lambda_)
print('Cost at initial theta (zeros): {:.3f}'.format(cost))
print('Expected cost (approx)       : 0.693\n')
print('Gradient at initial theta (zeros) - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\n')
# Compute and display cost and gradient
# with all-ones theta and lambda = 10
test_theta = np.ones(X.shape[1])
cost, grad = costFunctionReg(test_theta, X, y, 10)
print('------------------------------\n')
print('Cost at test theta    : {:.2f}'.format(cost))
print('Expected cost (approx): 3.16\n')
print('Gradient at test theta - first five values only:')
print('\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))
print('Expected gradients (approx) - first five values only:')
print('\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')
lam = 1
theta = np.zeros(X_train.shape[1])
res = optimize.minimize(fun=cost, x0=theta, args=(X_train, y, lam),
method='Newton-CG', jac=gradient)
y
lambda_ = 1
cost, grad = costFunctionReg(initial_theta, X, y, lambda_)
theta = np.zeros(X.shape[1])
res = optimize.minimize(fun=cost, x0=theta, args=(X, y, lambda_),method='Newton-CG',jac=gradient)
cost
J
res = optimize.minimize(fun=costFunctionReg, x0=theta, args=(X, y, lambda_),method='Newton-CG',jac=gradient)
X
X.sha^pe
X.shape
def cost(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
return J
def gradient(theta, X, y, lam):
def cost(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
return J
def cost(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
J = (-(np.dot(y, np.log(h)) + np.dot(1 - y, np.log(1 - h))) / X.shape[0]
+ lam * np.dot(t, t) / (2 * X.shape[0]))
return J
def gradient(theta, X, y, lam):
h = sigmoid(np.dot(X, theta))
t = np.zeros(len(theta))
t[1:] = theta[1:]
grad = np.dot(X.T, h - y) / X.shape[0] + lam * t / X.shape[0]
return grad
lam = 1
theta = np.zeros(X.shape[1])
res = optimize.minimize(fun=cost, x0=theta, args=(X, y, lambda_),method='Newton-CG',jac=gradient)
print(res.fun)
print(res.x[:5])
def plotDecisionBoundary(theta, X, y):
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),
np.arange(y_min, y_max, 0.05))
X_plot = np.c_[xx.ravel(), yy.ravel()]
X_plot = mapFeature(X_plot[:, 0], X_plot[:, 1])
y_plot = np.dot(X_plot, theta).reshape(xx.shape)
plt.figure()
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label="y = 1")
plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label="y = 0")
plt.contour(xx, yy, y_plot, levels=[0])
plt.xlabel("Microchip Test 1")
plt.ylabel("Microchip Test 2")
plt.xlim(-1.5, 1.5)
plt.ylim(-1.5, 1.5)
plt.legend()
plt.show()
plotDecisionBoundary(res.x, X, y)
def predict(theta, X):
X_train = mapFeature(X[:, 0], X[:, 1])
prob = sigmoid(np.dot(X_train, theta))
return (prob >= 0.5).astype(int)
plotDecisionBoundary(res.x, X, y)
exit
rmarkdown::render("TP2.Rmd", output_file = "TP2_corrections.html")
