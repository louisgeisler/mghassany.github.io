---
title: "TP3: Decision Trees & Random Forests"
author: "Mohamad Ghassany"
date: "sept 2021"
output: rmdformats::readthedown
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
#sidebar h2 {
    background-color: #077BBF;
}

#postamble {
    color: #fcfcfc;
    background: #077BBF;
    border-top: solid 10px #077BBF;
}

#sidebar {
    background: #077bbfcc;
}

#sidebar a {
    color: #fff;
}

h1, h2, h3, h4, h5, h6, legend {
    color: #077BBF;
}


```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE, fig.align = "center", warning = F, message = F)
```



:::{.alert .alert-block .alert-warning}
- **The estimated work time on this lab is 2 hours**.
- You are free to apply this lab in `r fontawesome::fa("r-project", fill ="steelblue")` or Python. The codes given in the lab and the main instructions are `r fontawesome::fa("r-project", fill ="steelblue")` codes. It is up to you to adapt them if you use Python. 
- If you use Python, verify that `scikit-learn` is installed and verify its version (it should at least 0.21).
:::

In this practical work, we will build some decision trees for both *regression* and *classification* problems. 

# In R

Note that there are many packages to do this in `r fontawesome::fa("r-project", fill ="steelblue")` . The `tree` package is the basic package to do so, while the `rpart`^[[An Introduction to Recursive Partitioning Using the `rpart` Routines](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) - Details of the `rpart` package.] package seems more widely suggested and provides better plotting features. So we will use the `rpart` package.

:::{.alert .alert-block .alert-warning}
It is recommended for correct and better using of `r fontawesome::fa("r-project", fill ="steelblue")` functions that you consult their documentations. Every `r fontawesome::fa("r-project", fill ="steelblue")` function is well documented indeed. You can do so by writing `?function_name` or `help(function_name)`in the console.

Especially for functions with multiple use, for example, `glm()` is a function that fits generalizes linear models, one of them is logistic regression when `type = "binomial"`.

Another example, is the function `predict()`, that is a generic function for predictions from the results of various model fitting functions. Its first argument is a `r fontawesome::fa("r-project", fill ="steelblue")` object, a model, and the rest of the arguments depends on the nature of the object. If you want to consult the documentation about using `predict()` for a tree built with `rpart()`, do `?predict.rpart` or `help(predict.rpart)`
:::

:::{.alert .alert-block .alert-warning}
If you want to run a function from a certain package without loading the package, you can write `package::function()`. For example `MASS::lda()` or `rpart::rpart()`. It is also helpful to remember the name of the package in which the function is defined.
:::


# In Python

In scikit-learn, `DecisionTreeClassifier` is a class capable of performing multi-class classification on a dataset. `plot_tree()` is a function to plot a decision tree. You can also visualize Decision Tree with `graphviz` package or also `dtreeviz` package. Some sources: [1](https://scikit-learn.org/stable/modules/tree.html){target="_blank"}, [2](http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Scikit_Learn_Decision_Tree.pdf){target="_blank"}, [3](https://mljar.com/blog/visualize-decision-tree/){target="_blank"}.

# Regression Trees 

For building trees for regression we are going to use the `Housing` dataset, (know also as `Boston` dataset). In the Housing dataset, there is 13 features and one target variable, described as follows: 

- **crim**: per capita crime rate by town.
- **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.
- **indus**: proportion of non-retail business acres per town.
- **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- **nox**: nitrogen oxides concentration (parts per 10 million).
- **rm**: average number of rooms per dwelling.
- **age**: proportion of owner-occupied units built prior to 1940.
- **dis**: weighted mean of distances to five Boston employment centres.
- **rad**: index of accessibility to radial highways.
- **tax**: full-value property-tax rate per \$10,000.
- **ptratio**: pupil-teacher ratio by town.
- **black**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
- **lstat**: lower status of the population (percent).

And the **target** variable:

- **medv**: median value of owner-occupied homes in \$1000s.

## Single tree

To demonstrate regression trees, we will use the `Boston` dataset that we used during the first two practical works, from the `MASS` package. Recall that `medv` is the response. 

**1**. Load the dataset and split it randomly in half.

In `r fontawesome::fa("r-project", fill ="steelblue")`, load it from `MASS` package. 

```{r, echo=TRUE, eval=TRUE}
library(MASS)
library(caTools)
set.seed(18)
Boston_idx = sample(1:nrow(Boston), nrow(Boston) / 2) 
# You don't know what we just did?
# open the documentation of the function sample by 
# writing ?sample in the R console.
# Note that this is one of the ways to split it randomly and it is not necessary the best.
Boston_train = Boston[Boston_idx,]
Boston_test  = Boston[-Boston_idx,]
```

In Python, load it from sklearn's datasets.

```{python, echo=TRUE, eval=FALSE}
from sklearn import datasets

# Loading the Housing dataset
boston= datasets.load_boston()

# Create feature matrix
X = boston.data
print(X.shape)

# Create target vector
y=boston.target
print(y.shape)

# Then use train_test_split()
```

**2**. Fit a regression tree to the training data using the `rpart()` function from the `rpart` package. Name the tree `Boston_tree`.

```{r, echo=F, eval=T}
library(rpart)
Boston_tree = rpart(medv ~ ., data = Boston_train)
```

**3**. Plot the obtained tree using the following code.

```{r, fig.height=8, fig.width=12}
plot(Boston_tree)
text(Boston_tree, pretty = 0)
title(main = "Regression Tree")
```

**4**. A better plot can be obtained using the `rpart.plot`^[[`rpart.plot` Package](http://www.milbo.org/doc/prp.pdf) - Detailed manual on plotting with `rpart` using the `rpart.plot` package.] package. Re-plot the tree using it. You can use the `rpart.plot()` function which by default, when the output is continuous, each node shows: the predicted value, and the percentage of observations in the node. You can also use the `prp()` function.

```{r, echo=T, eval=T, fig.show="hold", out.width="50%"}
library(rpart.plot)
rpart.plot(Boston_tree)
prp(Boston_tree)
```

**5**. Print the obtained tree and print its summary. Between the things that you can see in the summary, the CP (complexity parameter) table and the importance of each variable in the model. Print the CP table using the `printcp()` function to see the cross validation results. Plot a comparison figure using the `plotcp()` function.

You will notice the obtained tree is **pruned**. This is because `rpart` prunes the tree by default by performing 10-fold cross-validation. 

:::{.alert .alert-block .alert-warning}
`rpart` keeps track of something called the complexity of a tree. The complexity measure is a combination of the size of a tree and the ability of the tree to separate the classes of the target variable. If the next best split in growing a tree does not reduce the tree's overall complexity by a certain amount, `rpart` will terminate the growing process. This amount is specified by the complexity parameter, `cp`, in the call to `rpart()`. Setting `cp` to a negative amount (like -1) ensures that the tree will be fully grown. You can try it and then plot the tree.

Notice that the default `cp` value may over prune the tree. As a rule of thumb, it's best to prune a decision tree using the `cp` of smallest tree that is within one standard deviation of the tree with the smallest `xerror`. In the example above, it's maybe best to prune the tree with a `cp` slightly greater than `0.03`.
:::

Next we will compare this regression tree to a linear model and will use RMSE as our metric. RMSE is the **R**oot **M**ean **S**quare **E**rror, which is the square root of the MSE.

**5**. Write a `r fontawesome::fa("r-project", fill ="steelblue")` function that returns the RMSE of two vectors. 

```{r, echo=F, eval=T}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

**6**. Use the function `predict()` to predict the response on the test set. Then calculate the RMSE obtained with tree model.

```{r, echo=F, eval=T}
Boston_tree_pred = predict(Boston_tree, newdata = Boston_test)
```

```{r, echo=F, eval=F}
rmse(Boston_tree_pred, Boston_test$medv)
```

**7**. Fit a linear regression model on the training set. Then predict the response on the test set using the linear model. Calculate the RMSE and compare the performance of the tree and the linear regression model.

```{r, echo=F, eval=T}
Boston_lm = lm(medv ~ ., data = Boston_train)
Boston_lm_pred = predict(Boston_lm, newdata = Boston_test)
```

```{r, echo=F, eval=F}
rmse(Boston_lm_pred, Boston_test$medv)
```


:::{.alert .alert-block .alert-warning}
Here the most obvious linear regression beats the tree! We'll improve on this tree by considering ensembles of trees.
:::

You can visually compare the performance of both models by plotting the Actual (reality) response values against the predicted values. The model with closer points are to the diagonal (`y=x`) line is the better one. You can try to reproduce the figure below.

```{r, echo=F, eval=T, fig.show="hold", out.width="50%"}

plot(Boston_tree_pred, Boston_test$medv, 
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Single Tree, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)

plot(Boston_lm_pred, Boston_test$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Linear Model, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)
```

By aggregating many decision trees, using methods like *bagging*,
*random forests*, and *boosting*, the predictive performance of trees can be
substantially improved. We will now use these concepts, called *ensemble methods*.

## Bagging

Bagging, or *Bootstrap aggregation*, is a general-purpose procedure for reducing the variance of a statistical learning method, it is particularly useful and frequently used in the context of decision trees. The idea is to  take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Generally we do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set.

To apply bagging to regression
trees, we simply construct $B$ regression trees using B bootstrapped training
sets, and average the resulting predictions. These trees are grown deep,
and are not pruned. Hence each individual tree has high variance, but
low bias. Averaging these $B$ trees reduces the variance.

**8**. Fit a bagged model, using the `randomForest()` function from the `randomForest` package.

```{r, echo=F, eval=T, results=T}
library(randomForest)
Boston_bagging = randomForest(medv ~ ., data = Boston_train, mtry = 13,
                              importance = TRUE, ntrees = 500)
# Boston_bagging
```


:::{.alert .alert-block .alert-warning}
Bagging is actually a special case of a random forest where `mtry` is equal to $p$, the number of predictors.
:::

**9**. Predict the response on the test set using the bagging model. Calculate the RMSE. Is the performance of the model better than linear regression or a simple tree?

```{r, echo=F, eval=T}
Boston_bagging_pred = predict(Boston_bagging, newdata = Boston_test)
```

```{r, echo=F, eval=F}
rmse(Boston_bagging_pred, Boston_test$medv)
```


Note that the “Mean of squared residuals” which is output by `randomForest()` is the **Out of Bag**^[Read about OOB error [here](https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710){target="_blank"}] estimate of the error. Here is its plot:

```{r, echo=F, eval=T}
plot(Boston_bagging, col = "#cd0050", lwd = 2, main = "Bagged Trees: Error vs Number of Trees")
grid()
```

## Random Forests

Now try a random forest. For regression, on suggestion is to use `mtry` equal to $p/3$.^[For classification a suggestion is `mtry` = $\sqrt{p}$.]

**10**. Fit a random forest on the training set and compare its performance with the previous models by calculating the predictions and the RMSE.

```{r, echo=F, eval=T, results=T}
Boston_forest = randomForest(medv ~ ., data = Boston_train, mtry = 4,
                             importance = TRUE, ntrees = 500)
# Boston_forest
```

```{r, echo=F, eval=T}
Boston_forest_pred = predict(Boston_forest, newdata = Boston_test)
```

```{r, echo=F, eval=F}
rmse(Boston_forest_pred, Boston_test$medv)
```


**11**. Use the function `importance()` from the `randomForest` package to see the most important predictors in the obtained random forest model. What are the three most important predictors? Did you find the same results when you selected the best predictors for the linear regression model during session 2?

```{r, echo=F, eval=F}
importance(Boston_forest, type = 1)
```


**12**. Plot the importance of the predictors to the model using the `varImpPlot()` function.

```{r, echo=F, eval=T}
varImpPlot(Boston_forest, type = 1)
```


## Boosting

Last and not least, let us try a *boosted* model, which by default will produce a nice **variable importance** plot as well as plots of the marginal effects of the predictors. To do so, we will use the `gbm` package^[[**g**eneralized **b**oosted **m**odels](https://www.rdocumentation.org/packages/gbm/versions/2.1.8) `r fontawesome::fa("r-project", fill ="steelblue")` package].

**10**. Using the `gbm()` function like following, fit a boosted model on the training set. Then compare its performance with the previous models by calculating the predictions and the RMSE.

```{r, echo=T, eval=T, results=T}
library(gbm)
Boston_boost = gbm(medv ~ ., data = Boston_train, distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
```

```{r, echo=T, eval=T}
Boston_boost_pred = predict(Boston_boost, newdata = Boston_test)
```

```{r, echo=T, eval=T}
rmse(Boston_boost_pred, Boston_test$medv)
```

**11**. Show the summary of the boosted model. A figure of the variable importance will be shown.

```{r, echo=T, eval=T}
summary(Boston_boost)
```


## Comparison

**12**. Reproduce the following comparison: A table in which we show the obtained RMSE with each tested model, you can create a $5 \times 2$ data.frame in which you put the names of the models and the corresponding RMSE. To visualize the data frame in the compiled html report you can use the `kable()` function from the `knitr` package. Or, compare the models by plotting the Actual (reality) response values against the predicted values.

```{r, echo=T, eval=T, out.width = "90%", fig.asp=0.8}
par(mfrow=c(2,2))
plot(Boston_tree_pred, Boston_test$medv, 
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Single Tree, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)

plot(Boston_bagging_pred, Boston_test$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Bagging, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)

plot(Boston_forest_pred, Boston_test$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Random Forest, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)

plot(Boston_boost_pred, Boston_test$medv,
     xlab = "Predicted", ylab = "Actual",
     main = "Boosting, Test Data",
     col = "#cd0050", pch = 20)
grid()
abline(0, 1, col = "dodgerblue", lwd = 2)
```



# Classification Trees

A classification tree is very similar to a regression tree, except that the classification tree is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the
same terminal node. In contrast, for a classification tree, we predict that
each observation belongs to the ***most commonly occurring class*** of training
observations in the region to which it belongs.

To construct classification trees, we will use the **spam**^[[Source](https://archive.ics.uci.edu/ml/datasets/Spambase){target="_blank"}] dataset, available [here `r fontawesome::fa("table",     , fill = "#cd0050")`](spam.csv){target="_blank"}. A description of the dataset is given below.

:::rmdexercise
For the rest of this PW, you must:

- Import the `spam` dataset and explore it. Be aware that it is preferable that the response column is of type factor.
- Split the dataset into training and test sets (choose your own seed when using `set.seed()`).
- Fit (using `rpart` and `gbm` packages):
  + A **logistic** regression model. 
  + A simple classification tree.
  + Bagging, Random Forests^[For classification, the suggested `mtry` for a random forest is $\sqrt{p}$.], and Boosting models.
- For each model, predict the response on the test set and evaluate the performance of the model, using the *prediction accuracy* (create a function that returns the accuracy for two binary vectors).
:::

```{r, echo=F, eval=F}
# version minim

calc_acc = function(predicted, actual) {
  mean(predicted == actual)
}

dataset = read.csv("spam.csv")

dataset$spam = as.factor(dataset$spam)

library(caTools)
set.seed(123)
split = sample.split(dataset$spam, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# log reg

classifier.logreg <- glm(spam ~ ., family = "binomial", data=training_set)
classifier.logreg
summary(classifier.logreg)

pred.glm = predict(classifier.logreg, newdata = test_set, type="response")

pred.glm_T_F = ifelse(pred.glm >= 0.5, TRUE,FALSE)

acc_logreg = calc_acc(pred.glm_T_F, test_set$spam)

# tree

spam_tree = rpart(spam ~ ., data = training_set)
rpart.plot(spam_tree)

spam_tree_pred = predict(spam_tree, test_set, type="class") # spam must be of type vector if we want to predict the class
# otherwise, by default the prob will be returned
acc_tree = calc_acc(spam_tree_pred, test_set$spam)

# bagging

spam_bag = randomForest(spam ~ ., data = training_set, mtry = 57,
                        importance = TRUE, ntrees = 500)
spam_bag

spam_bag_pred = predict(spam_bag, test_set, type = "class")
acc_bag = calc_acc(spam_bag_pred, test_set$spam)

# RF

spam_rf = randomForest(spam ~ ., data = training_set, mtry = 8,
                        importance = TRUE, ntrees = 500)
spam_rf

spam_rf_pred = predict(spam_rf, test_set, type = "class")
acc_rf = calc_acc(spam_rf_pred, test_set$spam)

# Boosting

# gbm requires that the response is 0 or 1
training_set$spam01 = ifelse(training_set$spam == FALSE, 0, 1)
test_set$spam01 = ifelse(test_set$spam == FALSE, 0, 1)
spam_boost = gbm(spam01 ~ . - spam, data = training_set, distribution = "bernoulli", 
                 n.trees = 5000, interaction.depth = 4, shrinkage = 0.01) # attention distribution bernoulli car response 0 ou 1
spam_boost

spam_boost_pred = predict(spam_boost, test_set, type = "response") # pour savoir les arguments taper ?predict.gbm
spam_boost_pred01 = ifelse(spam_boost_pred >= 0.5, 1,0)
acc_boost = calc_acc(spam_boost_pred01, test_set$spam01)


# comparaison
spam_acc = data.frame(
  Model = c("Single Tree", "Logistic Regression", "Bagging",  "Random Forest",  "Boosting"),
  TestAccuracy = c(acc_logreg, acc_tree, acc_bag, acc_rf, acc_boost)
  )

spam_acc

# try knitr::kable(spam_acc) to a beautiful table in html
```


## The Spam dataset

This dataset consists of information from 4601 email messages, in a study to try to predict whether the email was junk email, or
"spam". For all 4601 email messages, the true outcome, spam or not, is available,
along with 57 predictors as described below:

- 48 quantitative predictors: the *percentage* of words in the email that match a given word. Examples include *business*, *address*, *internet*; etc. 
-  6 quantitative predictors: the percentage of characters in the email that match a given character. The characters are `;` , `(` , `[` , `!` , `$` and `#`.
- The average length of uninterrupted sequences of capital letters: `crl.ave`.
- The length of the longest uninterrupted sequence of capital letters: `crl.long`.
- The sum of the length of uninterrupted sequences of capital letters: `crl.tot`.

:::{.alert .alert-block .alert-warning}
Note that the spam dataset given here is already treated and ready to be explored. To achieve this stage, some steps are required to treat the raw data, like **Tokenization**, **Stemming**, and **Lemmatization**. In this dataset the most important words are already selected and other variables are added. 
Curious students can read more about these steps. Two famous `r fontawesome::fa("r-project", fill ="steelblue")` packages for text mining are [tm](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf){target="_blank"} and [tidytext](https://www.tidytextmining.com/index.html){target="_blank"}.
:::

## Extra: Tuning {-}

So far in this PW, we fit bagging, boosting and random forest models, but did not **tune** any of them, we simply used certain, somewhat arbitrary, parameters. Actually, to make these models better the parameters should be tuned. The parameters include: 

- Bagging: Actually just a subset of Random Forest with `mtry` = $p$.
- Random Forest: `mtry`
- Boosting: `n.trees`, `interaction.depth`, `shrinkage`, `n.minobsinnode`

The `caret` package in R provides excellent functions to accomplish this. Note that with these tree-based ensemble methods there are two resampling solutions for tuning the model:

- Out of Bag 
- Cross-Validation

Using Out of Bag samples is advantageous with these methods as compared to Cross-Validation since it removes the need to refit the model and is thus much more computationally efficient. Unfortunately OOB methods cannot be used with `gbm` models. See the `caret` documentation: [Short intro](https://cran.r-project.org/web/packages/caret/vignettes/caret.html){target="_blank"}, [Long intro](https://topepo.github.io/caret/index.html){target="_blank"} for details.


```{r, echo=F, eval=F}
#randoomForest : mtry

# see  https://rpubs.com/phamdinhkhanh/389752

library(randomForest)
library(mlbench)
library(caret)
library(e1071)

# Load Dataset

# dataset <- training_set



#################################
#Create control function for training with 10 folds and keep 3 folds for training. search method is grid.
control <- trainControl(method='oob', 
                        number=4, 
                        search='grid')



#create tunegrid with 15 values from 1:15 for mtry to tunning model. Our train function will change number of entry variable at each split according to tunegrid. 
tunegrid <- expand.grid(.mtry = (1:5)) 

rf_gridsearch <- train(spam ~ ., 
                       data = training_set,
                       method = 'rf',
                       metric = 'Accuracy',
                       tuneGrid = tunegrid)
print(rf_gridsearch)
plot(rf_gridsearch)



# Trees do not make any assumptions on the distribution. In gradient boosting 
# you need to specify the loss function to be optimized, e.g. logloss or mse.
# Modern implementations like lightgbm or XGBoost just ask for these. The gbm package 
# uses another implementation strategy: it asks for the distribution and uses the loss 
# function derived from the corresponding likelihood (plus some special ones). 
# Logloss resp. Cross-entropy is chosen for Bernoulli, mse for normal etc. 


######################################

control <- trainControl(method='cv', 
                        number=10)
#Metric compare model is Accuracy
metric <- "Accuracy"
tunegrid <- expand.grid(.shrinkage =c(0.001,0.01,0.05),.n.trees = c(2,5,8,11,20),.interaction.depth = c(2,3,4,5),.n.minobsinnode =c( 4,1,2) )


gbm_cv <- train(spam~., 
                    data=training_set, 
                    method='gbm', 
                    metric='Accuracy', 
                    tuneGrid=tunegrid, 
                    trControl=control, verbose=FALSE)
print(gbm_cv)
gbm_cv$bestTune
plot(gbm_cv)
```


<p class="text-right">&#9724;</p>
