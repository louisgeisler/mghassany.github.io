<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Variables Aléatoires Discrètes | Statistique Inférentielle</title>
  <meta name="description" content="Cours de Statistique Inférentielle A3 ESILV" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Variables Aléatoires Discrètes | Statistique Inférentielle" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Cours de Statistique Inférentielle A3 ESILV" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Variables Aléatoires Discrètes | Statistique Inférentielle" />
  
  <meta name="twitter:description" content="Cours de Statistique Inférentielle A3 ESILV" />
  

<meta name="author" content="Mohamad Ghassany" />


<meta name="date" content="2019-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="img/favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html">
<link rel="next" href="variables-aleatoires-continues.html">
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<link href="book_assets/font-awesome-5.3.1/css/fontawesome-all.min.css" rel="stylesheet" />
<script src="book_assets/kePrint-0.0.1/kePrint.js"></script>
<link href="book_assets/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='beforeimg'>            
   <a href="https://www.esilv.fr/">
       <img src="img/Logo_ESILV_mycolor.png" style="width:75%; padding:0px 0; display:block; margin: 0 auto;" alt="ESILV logo">
    </a>
</li>
<li class='before'><a href="./">Statistique Inférentielle</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="part"><span><b>Complément de formation Statistique</b></span></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html"><i class="fa fa-check"></i>Variables Aléatoires Discrètes</a><ul>
<li class="chapter" data-level="0.1" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#notion-de-variable-aleatoire-reelle-v.a.r."><i class="fa fa-check"></i><b>0.1</b> Notion de variable aléatoire réelle (v.a.r.)</a></li>
<li class="chapter" data-level="0.2" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#variables-aleatoires-discretes-1"><i class="fa fa-check"></i><b>0.2</b> Variables aléatoires discrètes</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#definition-loi-de-probabilite"><i class="fa fa-check"></i>Définition, loi de probabilité</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#fonction-de-repartition-dune-variable-aleatoire-discrete"><i class="fa fa-check"></i>Fonction de répartition d’une variable aléatoire discrète</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#moments-dune-variable-aleatoire-discrete"><i class="fa fa-check"></i><b>0.3</b> Moments d’une variable aléatoire discrète</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#esperance-mathematique"><i class="fa fa-check"></i>Espérance mathématique</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#esperance-dune-fonction-dune-variable-aleatoire"><i class="fa fa-check"></i>Espérance d’une fonction d’une variable aléatoire</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#variance"><i class="fa fa-check"></i>Variance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#ecart-type"><i class="fa fa-check"></i>Ecart-type</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#moments-non-centres-et-centres"><i class="fa fa-check"></i>Moments non centrés et centrés</a></li>
</ul></li>
<li class="chapter" data-level="0.4" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#couple-de-variables-aleatoires-discretes"><i class="fa fa-check"></i><b>0.4</b> Couple de variables aléatoires discrètes</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#table-de-probabilite-conjointe"><i class="fa fa-check"></i>Table de probabilité conjointe</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-marginales"><i class="fa fa-check"></i>Lois marginales</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-conditionnelles"><i class="fa fa-check"></i>Lois conditionnelles</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#independance-de-variables-aleatoires"><i class="fa fa-check"></i>Indépendance de variables aléatoires</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#covariance"><i class="fa fa-check"></i>Covariance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#coefficient-de-correlation-lineaire"><i class="fa fa-check"></i>Coefficient de corrélation linéaire</a></li>
</ul></li>
<li class="chapter" data-level="0.5" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#lois-usuelles-discretes"><i class="fa fa-check"></i><b>0.5</b> Lois usuelles discrètes</a><ul>
<li class="chapter" data-level="0.5.1" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#loi-uniforme-discrete-mathcalun"><i class="fa fa-check"></i><b>0.5.1</b> Loi uniforme discrète <span class="math inline">\(\mathcal{U}(n)\)</span></a></li>
<li class="chapter" data-level="0.5.2" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#loi-de-bernoulli-mathcalbp"><i class="fa fa-check"></i><b>0.5.2</b> Loi de Bernoulli <span class="math inline">\(\mathcal{B}(p)\)</span></a></li>
<li class="chapter" data-level="0.5.3" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#loi-binomiale-mathcalbnp"><i class="fa fa-check"></i><b>0.5.3</b> Loi Binomiale <span class="math inline">\(\mathcal{B}(n,p)\)</span></a></li>
<li><a href="variables-aleatoires-discretes.html#loi-de-poisson-mathcalplambda">Loi de Poisson <span class="math inline">\(\mathcal{P}(\lambda)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-discretes.html"><a href="variables-aleatoires-discretes.html#approximation-dune-loi-binomiale"><i class="fa fa-check"></i>Approximation d’une loi binomiale</a></li>
<li><a href="variables-aleatoires-discretes.html#loi-geometrique-ou-de-pascal-mathcalgp">Loi Géométrique ou de Pascal <span class="math inline">\(\mathcal{G}(p)\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html"><i class="fa fa-check"></i>Variables Aléatoires Continues</a><ul>
<li class="chapter" data-level="0.6" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densite-dune-variable-aleatoire-continue"><i class="fa fa-check"></i><b>0.6</b> Densité d’une variable aléatoire continue</a></li>
<li class="chapter" data-level="0.7" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#fonction-de-repartition-dune-v.a.c"><i class="fa fa-check"></i><b>0.7</b> Fonction de répartition d’une v.a.c</a></li>
<li class="chapter" data-level="0.8" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#fonction-dune-variable-aleatoire-continue"><i class="fa fa-check"></i><b>0.8</b> Fonction d’une variable aléatoire continue</a><ul>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxaxb">Calcul de densités pour <span class="math inline">\(h(X)=aX+b\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxx2">Calcul de densités pour <span class="math inline">\(h(X)=X^2\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#calcul-de-densites-pour-hxex">Calcul de densités pour <span class="math inline">\(h(X)=e^X\)</span></a></li>
</ul></li>
<li class="chapter" data-level="0.9" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-et-variance-de-variables-aleatoires-continues"><i class="fa fa-check"></i><b>0.9</b> Espérance et variance de variables aléatoires continues</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-dune-v.a.c"><i class="fa fa-check"></i>Espérance d’une v.a.c</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#variance-dune-v.a.c"><i class="fa fa-check"></i>Variance d’une v.a.c</a></li>
</ul></li>
<li class="chapter" data-level="0.10" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#lois-usuelles-de-v.a.c"><i class="fa fa-check"></i><b>0.10</b> Lois usuelles de v.a.c</a><ul>
<li><a href="variables-aleatoires-continues.html#loi-uniforme-uab">Loi uniforme <span class="math inline">\(U(a,b)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-exponentielle-mathcalelambda">Loi exponentielle <span class="math inline">\(\mathcal{E}(\lambda)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-normale-ou-de-laplace-gauss-mathcalnmusigma2">Loi Normale ou de Laplace-Gauss <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#etude-de-la-densite-de-la-loi-normale"><i class="fa fa-check"></i>Étude de la densité de la loi Normale</a></li>
<li><a href="variables-aleatoires-continues.html#loi-normale-centree-reduite-mathcaln01">Loi Normale centrée réduite <span class="math inline">\(\mathcal{N}(0,1)\)</span></a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#relation-entre-loi-normale-et-loi-normale-centree-reduite"><i class="fa fa-check"></i>Relation entre loi normale et loi normale centrée réduite</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#calcul-des-probabilites-dune-loi-normale"><i class="fa fa-check"></i>Calcul des probabilités d’une loi normale</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#approximation-normale-dune-repartition-binomiale"><i class="fa fa-check"></i>Approximation normale d’une répartition binomiale</a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-chi2-de-pearson">Loi de <span class="math inline">\(\chi^{2}\)</span> de Pearson</a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-student-stn">Loi de Student <span class="math inline">\(St(n)\)</span></a></li>
<li><a href="variables-aleatoires-continues.html#loi-de-fisher-snedecor-mathcalfnm">Loi de Fisher-Snedecor <span class="math inline">\(\mathcal{F}(n,m)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="0.11" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#couple-de-variables-aleatoires-continues"><i class="fa fa-check"></i><b>0.11</b> Couple de variables aléatoires continues</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densite-conjointe"><i class="fa fa-check"></i>Densité conjointe</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#densites-marginales"><i class="fa fa-check"></i>Densités marginales</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#esperance-dune-fonction-du-couple"><i class="fa fa-check"></i>Espérance d’une fonction du couple</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#independance"><i class="fa fa-check"></i>Indépendance</a></li>
<li class="chapter" data-level="" data-path="variables-aleatoires-continues.html"><a href="variables-aleatoires-continues.html#distribution-conditionnelle"><i class="fa fa-check"></i>Distribution conditionnelle</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html"><i class="fa fa-check"></i>Exercices</a><ul>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#combinatoire"><i class="fa fa-check"></i>Combinatoire</a></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#evenements"><i class="fa fa-check"></i>Événements</a></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#probabilite"><i class="fa fa-check"></i>Probabilité</a></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#variables-aleatoires-discretes-2"><i class="fa fa-check"></i>Variables aléatoires discrètes</a></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#variables-aletoires-continues"><i class="fa fa-check"></i>Variables alétoires continues</a></li>
<li class="chapter" data-level="" data-path="exercices.html"><a href="exercices.html#couple-de-variables-aleatoires-continues-1"><i class="fa fa-check"></i>Couple de variables aléatoires continues</a></li>
</ul></li>
<li class="part"><span><b>Statistique Inférentielle</b></span></li>
<li class="chapter" data-level="1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction-1.html"><a href="introduction-1.html#la-demarche-statistique"><i class="fa fa-check"></i>La démarche statistique</a></li>
<li class="chapter" data-level="" data-path="introduction-1.html"><a href="introduction-1.html#objectifs-et-plan-du-cours"><i class="fa fa-check"></i>Objectifs et plan du cours</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html"><i class="fa fa-check"></i><b>2</b> Statistique descriptive</a><ul>
<li class="chapter" data-level="2.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#introduction-2"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#echantillonnage-statistique"><i class="fa fa-check"></i><b>2.2</b> Echantillonnage statistique</a><ul>
<li class="chapter" data-level="" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#echantillonnage-aleatoire-simple"><i class="fa fa-check"></i>Echantillonnage aléatoire simple</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#statistique-et-probabilites"><i class="fa fa-check"></i><b>2.3</b> Statistique et Probabilités</a></li>
<li class="chapter" data-level="2.4" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#description-dune-serie-de-valeurs"><i class="fa fa-check"></i><b>2.4</b> Description d’une série de valeurs</a></li>
<li class="chapter" data-level="2.5" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#graphiques"><i class="fa fa-check"></i><b>2.5</b> Graphiques</a><ul>
<li class="chapter" data-level="2.5.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#variable-qualitative-nominale"><i class="fa fa-check"></i><b>2.5.1</b> Variable qualitative nominale</a></li>
<li class="chapter" data-level="2.5.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#variable-qualitative-ordinale"><i class="fa fa-check"></i><b>2.5.2</b> Variable qualitative ordinale</a></li>
<li class="chapter" data-level="2.5.3" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#variable-quantitative-continue"><i class="fa fa-check"></i><b>2.5.3</b> Variable quantitative (continue)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#indicateurs"><i class="fa fa-check"></i><b>2.6</b> Indicateurs</a><ul>
<li class="chapter" data-level="2.6.1" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#tendance-centrale"><i class="fa fa-check"></i><b>2.6.1</b> Tendance centrale</a></li>
<li class="chapter" data-level="2.6.2" data-path="statistique-descriptive.html"><a href="statistique-descriptive.html#dispersion"><i class="fa fa-check"></i><b>2.6.2</b> Dispersion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html"><i class="fa fa-check"></i><b>3</b> Échantillonnage et Théorèmes limites</a><ul>
<li class="chapter" data-level="3.1" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#echantillonnage"><i class="fa fa-check"></i><b>3.1</b> Échantillonnage</a></li>
<li class="chapter" data-level="3.2" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#theoremes-limites"><i class="fa fa-check"></i><b>3.2</b> Théorèmes limites</a><ul>
<li class="chapter" data-level="3.2.1" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#introduction-3"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#loi-faible-des-grands-nombres"><i class="fa fa-check"></i><b>3.2.2</b> Loi faible des grands nombres</a></li>
<li class="chapter" data-level="3.2.3" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#loi-forte-des-grands-nombres"><i class="fa fa-check"></i><b>3.2.3</b> Loi forte des grands nombres</a></li>
<li class="chapter" data-level="3.2.4" data-path="echantillonnage-et-theoremes-limites.html"><a href="echantillonnage-et-theoremes-limites.html#theoreme-central-limite"><i class="fa fa-check"></i><b>3.2.4</b> Théorème central limite</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html"><i class="fa fa-check"></i><b>4</b> Estimation ponctuelle</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#introduction-4"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#methodes-destimation"><i class="fa fa-check"></i><b>4.2</b> Méthodes d’estimation</a></li>
<li class="chapter" data-level="4.3" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-methode-des-moments"><i class="fa fa-check"></i><b>4.3</b> La méthode des moments</a><ul>
<li class="chapter" data-level="4.3.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#lestimateur-des-moments-emm"><i class="fa fa-check"></i><b>4.3.1</b> L’estimateur des moments (EMM)</a></li>
<li class="chapter" data-level="4.3.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#exemples"><i class="fa fa-check"></i><b>4.3.2</b> Exemples</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-methode-du-maximum-de-vraisemblance"><i class="fa fa-check"></i><b>4.4</b> La méthode du maximum de vraisemblance</a><ul>
<li class="chapter" data-level="4.4.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#la-fonction-de-vraisemblance"><i class="fa fa-check"></i><b>4.4.1</b> La fonction de vraisemblance</a></li>
<li class="chapter" data-level="4.4.2" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#lestimateur-de-maximum-de-vraisemblance-emv"><i class="fa fa-check"></i><b>4.4.2</b> L’estimateur de maximum de vraisemblance (EMV)</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#qualite-dun-estimateur"><i class="fa fa-check"></i><b>4.5</b> Qualité d’un estimateur</a><ul>
<li class="chapter" data-level="4.5.1" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#estimateur-sans-biais-et-de-variance-minimale-esbvm"><i class="fa fa-check"></i><b>4.5.1</b> Estimateur sans biais et de variance minimale (ESBVM)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="estimation-ponctuelle.html"><a href="estimation-ponctuelle.html#proprietes-des-emm-et-des-emv"><i class="fa fa-check"></i><b>4.6</b> Propriétés des EMM et des EMV</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="intervalle-de-confiance.html"><a href="intervalle-de-confiance.html"><i class="fa fa-check"></i><b>5</b> Intervalle de confiance</a></li>
<li class="chapter" data-level="6" data-path="tests-dhpotheses.html"><a href="tests-dhpotheses.html"><i class="fa fa-check"></i><b>6</b> Tests d’hpothèses</a></li>
<li class="appendix"><span><b>Annexe</b></span></li>
<li class="chapter" data-level="A" data-path="tab-normale.html"><a href="tab-normale.html"><i class="fa fa-check"></i><b>A</b> Table de la loi Normale centrée réduite</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistique Inférentielle</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variables-aleatoires-discretes" class="section level1 unnumbered">
<h1>Variables Aléatoires Discrètes</h1>
<div id="notion-de-variable-aleatoire-reelle-v.a.r." class="section level2">
<h2><span class="header-section-number">0.1</span> Notion de variable aléatoire réelle (v.a.r.)</h2>
<p>Après avoir réalisé une expérience aléatoire, il arrive bien souvent
qu’on s’intéresse plus à une fonction du résultat qu’au résultat
lui-même. Expliquons ceci au moyen des exemples suivants: lorsqu’on joue
au dés, certains jeux accordent de l’importance à la somme obtenue sur
deux dés, 7 par exemple, plutôt qu’à la question de savoir si c’est la
paire (1,6) qui est apparue, ou (2,5), (3,4), (4,3), (5,2) ou plutôt
(6,1). Dans le cas du jet d’une pièce, il peut être plus intéressant de
connaître le nombre de fois où le côté pile est apparue plutôt que la
séquence détaillée des jets pile et face. Ces grandeurs auxquelles on
s’intéresse sont en fait des fonctions réelles définies sur l’ensemble
fondamental et sont appelées <strong><em>variables aléatoires</em></strong>.</p>
<p>Du fait que la valeur d’une variable aléatoire est déterminée par le
résultat de l’expérience, il est possible d’attribuer une probabilité
aux différentes valeurs que la variable aléatoire peut prendre.</p>
<p>Soient <span class="math inline">\(\varepsilon\)</span> une expérience aléatoire et
<span class="math inline">\((\Omega,\mathcal{A},P)\)</span> un espace probabilisé lié à cette expérience.
Dans de nombreuses situations, on associe à chaque résultat
<span class="math inline">\(\omega \in \Omega\)</span> un nombre réel noté <span class="math inline">\(X(\omega)\)</span>; on construit ainsi
une application <span class="math inline">\(X : \Omega \rightarrow \mathbb{R}\)</span>. Historiquement,
<span class="math inline">\(\varepsilon\)</span> était un jeu et <span class="math inline">\(X\)</span> représentait le gain du joueur.</p>
<!-- ```{r} -->
<!-- # https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf -->
<!-- library(kableExtra) -->
<!-- dt <- mtcars[1:5, 1:6] -->
<!-- kable(dt, "latex", booktabs = T) %>% -->
<!--   column_spec(1, bold = T) %>% -->
<!--   kable_as_image() -->
<!-- ``` -->
<p><strong>Exemple:</strong> Un joueur lance un dé équilibré à 6 faces numérotées de 1 à 6, et on
observe le numéro obtenu.</p>
<ul>
<li><p>Si le joueur obtient 1, 3 ou 5, il gagne 1 euro.</p></li>
<li><p>S’il obtient 2 ou 4, il gagne 5 euros.</p></li>
<li><p>S’il obtient 6, il perd 10 euros.</p></li>
</ul>
<p>Selon l’expérience aléatoire (lancer d’un dé équilibré) l’ensemble
fondamental est <span class="math inline">\(\Omega = \{1,2,3,4,5,6\}\)</span>,
<span class="math inline">\(\mathcal{A} = \mathcal{P}(\Omega)\)</span> et <span class="math inline">\(P\)</span> l’équiprobabilité sur
<span class="math inline">\((\Omega,\mathcal{A})\)</span>. Soit <span class="math inline">\(X\)</span> l’application de <span class="math inline">\(\Omega\)</span> dans
<span class="math inline">\(\mathbb{R}\)</span> qui à tout <span class="math inline">\(\omega \in \Omega\)</span> associe le gain
correspondant. On a donc</p>
<ul>
<li><p><span class="math inline">\(X(1) = X(3) = X(5) = 1\)</span></p></li>
<li><p><span class="math inline">\(X(2) = X(4) = 5\)</span></p></li>
<li><p><span class="math inline">\(X(6) = -10\)</span></p></li>
</ul>
<p>On dit que <span class="math inline">\(X\)</span> est une <span style="color: blue"><strong>variable aléatoire</strong></span> sur
<span class="math inline">\(\Omega\)</span>.</p>
<p>On peut s’intéresser à la probabilité de gagner 1 euro, c’est-à-dire
d’avoir <span class="math inline">\(X(\omega) = 1\)</span>, ce qui se réalise si et seulement si
<span class="math inline">\(\omega \in \{1,3,5\}\)</span>. La probabilité cherchée est donc égale à
<span class="math inline">\(P(\{1,3,5\}) = 1/2\)</span>. On écrira aussi <span class="math inline">\(P(X=1) = 1/2\)</span>.</p>
<p>On pourra donc considérer l’événement :
<span class="math display">\[\{X=1\} = \{\omega \in \Omega / X(\omega) = 1\} = \{\omega \in \Omega / X(\omega) \in \{1\}\}  = X^{-1} (\{1\}) = \{1,3,5\}.\]</span></p>
<p>On aura du même <span class="math inline">\(P(X=5) = 1/3\)</span> et <span class="math inline">\(P(X=-10) = 1/6\)</span>. Ce que l’on peut
présenter dans un tableau</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center">-10</th>
<th align="center">1</th>
<th align="center">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_i=P(X = x_i)\)</span></td>
<td align="center"><span class="math inline">\(1/6\)</span></td>
<td align="center"><span class="math inline">\(1/2\)</span></td>
<td align="center"><span class="math inline">\(1/3\)</span></td>
</tr>
</tbody>
</table>
<p>Cela revient à considérer un nouvel ensemble d’événements élémentaires:
<span class="math display">\[\Omega_X = X(\Omega)= \{-10,1,5\}\]</span> et à munir cet ensemble de la
probabilité <span class="math inline">\(P_X\)</span> définie par le tableau des <span class="math inline">\(P(X=x_i)\)</span> ci dessus. Cette
nouvelle probabilité s’appelle <span style="color: blue"><strong>loi de la variable
aléatoire</strong></span> X.</p>
<p>Remarquer que
<span class="math display">\[P(\bigcup_{x_i \in \Omega_X} \{X=x_i\}) = \sum_{x_i \in \Omega_X} P(X=x_i) = 1\]</span></p>
<p>Dans ce chapitre, nous traitons le cas où <span class="math inline">\(X(\Omega)\)</span> est dénombrable.
La variable aléatoire est alors dite <strong><em>discrète</em></strong>. Sa loi de
probabilité, qui peut être toujours définie par sa fonction de
répartition, le sera plutôt par les probabilités individuelles. Nous
définirons les deux caractéristiques numériques principales d’une
variable aléatoire discrète, l’espérance caractéristique de valeur
centrale, et la variance, caractéristique de dispersion. Nous définirons
aussi les couples de variables aléatoires.</p>
</div>
<div id="variables-aleatoires-discretes-1" class="section level2">
<h2><span class="header-section-number">0.2</span> Variables aléatoires discrètes</h2>
<div id="definition-loi-de-probabilite" class="section level3 unnumbered">
<h3>Définition, loi de probabilité</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-6" class="definition"><strong>Définition 0.1  </strong></span>On dit qu’une variable aléatoire réelle (v.a.r.) <span class="math inline">\(X\)</span> est <strong><em>discrète</em></strong>
(v.a.r.d.) si l’ensemble des valeurs que prend <span class="math inline">\(X\)</span> est fini ou infini
dénombrable.</p>
<p>Si on suppose <span class="math inline">\(X(\Omega)\)</span> l’ensemble des valeurs de <span class="math inline">\(X\)</span> qui admet un
plus petit élément <span class="math inline">\(x_1\)</span>. Alors la v.a.r.d. <span class="math inline">\(X\)</span> est entièrement définie
par:</p>
<ul>
<li><p>L’ensemble <span class="math inline">\(X(\Omega)\)</span> des valeurs prises par <span class="math inline">\(X\)</span>, rangées par ordre
croissant: <span class="math inline">\(X(\Omega) = \{x_1, x_2,\ldots,x_i,\ldots\}\)</span> avec
<span class="math inline">\(x_1 \leq x_2 \leq \ldots \leq x_i \leq \ldots\)</span>.</p></li>
<li><p>La <strong><em>loi de probabilité</em></strong> définie sur <span class="math inline">\(X(\Omega)\)</span> par
<span class="math display">\[p_i = P(X=x_i) \,\,\,\,\, \forall \,\, i=1,2,\ldots\]</span></p></li>
</ul>
</div>

<p><strong>Remarques</strong>:</p>
<ul>
<li><p>Soit <span class="math inline">\(B\)</span> un ensemble de <span class="math inline">\(\mathbb{R}\)</span>,
<span class="math display">\[P(X \in B) = \sum_{i / x_i \in B} p(x_i)\]</span></p></li>
<li><p>En particulier
<span class="math display">\[P( a &lt; X \leq b) =  \sum_{i / a &lt; x_i \leq b} p(x_i)\]</span></p></li>
<li><p>Bien sûr tous les <span class="math inline">\(p(x_i)\)</span> sont positives et
<span class="math inline">\(\sum_{i=1}^{\infty} p(x_i) =1\)</span>.</p></li>
<li><p>Si <span class="math inline">\(X\)</span> ne prend qu’un petit nombre de valeurs, cette loi est
généralement présentée dans un tableau.</p></li>
</ul>
</div>
<div id="fonction-de-repartition-dune-variable-aleatoire-discrete" class="section level3 unnumbered">
<h3>Fonction de répartition d’une variable aléatoire discrète</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-7" class="definition"><strong>Définition 0.2  </strong></span>On appelle <em>fonction de répartition</em> de la v.a. <span class="math inline">\(X\)</span>, qu’on note <span class="math inline">\(F(a)\)</span>
de la v.a.r.d. <span class="math inline">\(X\)</span>, ou <span class="math inline">\(F_X(a)\)</span>, la fonction définie pour tout réel <span class="math inline">\(a\)</span>,
<span class="math inline">\(-\infty &lt; a &lt; \infty\)</span>, par</p>
<span class="math display">\[F(a)=P(X \leq a)=\sum_{i / x_{i}\leq a} P(X=x_{i})\]</span>
</div>

<p>Cette valeur représente la probabilité de toutes les réalisations
inférieures ou égales au réel <span class="math inline">\(a\)</span>.</p>
<p><strong>Propriétés</strong>: Voici quelques propriétés de cette fonction:</p>
<ol style="list-style-type: decimal">
<li><p>C’est une fonction en escalier (constante par morceaux).</p></li>
<li><p><span class="math inline">\(F(a) \leq 1\)</span> car c’est une probabilité.</p></li>
<li><p><span class="math inline">\(F(a)\)</span> est continue à droite.</p></li>
<li><p><span class="math inline">\(\lim\limits_{a\to - \infty} F(a) = 0\)</span> et
<span class="math inline">\(\lim\limits_{a\to\infty} F(a) = 1\)</span></p></li>
</ol>
<p>La fonction de répartition caractérise la loi de <span class="math inline">\(X\)</span>, autrement dit:
<span class="math inline">\(F_{X} = F_{Y}\)</span> si et seulement si les variables aléatoires <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>
ont la même loi de probabilité.</p>
<div id="fonction-de-repartition-et-probabilites-sur-x" class="section level4 unnumbered">
<h4>Fonction de répartition et probabilités sur <span class="math inline">\(X\)</span></h4>
<p>Tous les calculs de probabilité concernant <span class="math inline">\(X\)</span> peuvent être traités en
termes de fonction de répartition. Par exemple,</p>
<p><span class="math display">\[P(a &lt; X \leq b) = F(b) - F(a) \quad \quad \text{pour tout } a &lt; b\]</span></p>
<p>On peut mieux s’en rendre compte en écrivant <span class="math inline">\(\{X \leq b\}\)</span> comme union
des deux événements incompatibles <span class="math inline">\(\{X \leq a\}\)</span> et <span class="math inline">\(\{ a &lt; X \leq b\}\)</span>,
soit</p>
<p><span class="math display">\[\{X \leq b\} = \{X \leq a\} \cup   \{ a &lt; X \leq b\}\]</span></p>
<p>et ainsi</p>
<p><span class="math display">\[P(X \leq b) = P(X \leq a) + P(a &lt; X \leq b)\]</span> ce qui établit l’égalité
ci dessus.</p>
<div class="rmdinsight">
<p>
On peut déduire de <span class="math inline"><span class="math inline">\(F\)</span></span> les probabilités individuelles par:
</p>
<p>
<span class="math display"><span class="math display">\[p_{i}=F(x_{i})-F(x_{i-1})\quad \quad \text{pour  } 1 \leq i \leq n\]</span></span>
</p>
</div>
<p><strong>Exemple:</strong> On joue trois fois à pile ou face. Soit <span class="math inline">\(X\)</span> la variable aléatoire
“nombre de pile obtenus”. Ici <span class="math inline">\(\Omega=\{P, F\}^3\)</span>, et donc
<span class="math display">\[X(\Omega)=\{0, 1, 2, 3\}.\]</span></p>
<p>On a <span class="math inline">\(card(\Omega)=2^3=8\)</span>. Calculons par exemple <span class="math inline">\(P(X=1)\)</span>, c’est à dire
la probabilité d’avoir exactement une pile.
<span class="math display">\[X^{-1}(1)=\{(P, F, F), (F, P, F), (F, F, P) \}\]</span> D’où
<span class="math inline">\(P(X=1)=\frac{3}{8}\)</span>.</p>
<p>En procédant de la même façon, on obtient la loi de probabilité de <span class="math inline">\(X\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(k\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = k)\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
</tr>
</tbody>
</table>
<p>La fonction de répartition de <span class="math inline">\(X\)</span> est donc donnée par:</p>
<p><span class="math display">\[F(x) = \left\{ 
\begin{array}{l l}
 0 &amp; \quad \text{si $x&lt;0$}\\
  1/8 &amp; \quad \text{si $0 \leq x &lt; 1$}\\ 
   1/2 &amp; \quad \text{si $1 \leq x &lt; 2$}\\
    7/8 &amp; \quad \text{si $2 \leq x &lt; 3$}\\
     1 &amp; \quad \text{si $x \geq 3$}\\
\end{array} \right.\]</span></p>
<p>Le graphe de cette dernière est représentée dans la figure suivante:</p>
<p><img src="Statistique_infentielle_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><strong>Exemple:</strong> Soit <span class="math inline">\(A\)</span> un événement quelconque. On appelle variable aléatoire <em>indicatrice</em> de cet événement <span class="math inline">\(A\)</span>, la variable aléatoire définie par: <span class="math display">\[X(\omega) = \left\{ 
\begin{array}{l l}
 1 &amp; \quad \text{si $\omega \in A$}\\
 0 &amp; \quad \text{si $\omega \in \bar{A}$}\\   
  \end{array} \right.\]</span></p>
<p>et notée <span class="math inline">\(X=1_A\)</span>. Ainsi: <span class="math display">\[P(X=1)=P(A)=p\]</span>
<span class="math display">\[P(X=0)=P(\bar{A})=1-p\]</span></p>
<p>La fonction de répartition de <span class="math inline">\(X\)</span> est donc donnée par:</p>
<p><span class="math display">\[F(x) = \left\{ 
\begin{array}{l l}
 0 &amp; \quad \text{si $x&lt;0$}\\
  1-p &amp; \quad \text{si $0 \leq x &lt; 1$}\\ 
   1 &amp; \quad \text{si $x \geq 1$}\\
\end{array} \right.\]</span></p>
<p>On peut prendre par exemple le cas d’un tirage d’une boule dans une urne
contenant 2 boules blanches et 3 boules noires. Soit
<span class="math inline">\(A:\text{&quot;obtenir une boule blanche&quot;}\)</span> et <span class="math inline">\(X\)</span> la variable indicatrice
de <span class="math inline">\(A\)</span>. La loi de probabilité de <span class="math inline">\(X\)</span> est alors</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(k\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = k)\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{5}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{5}\)</span></td>
</tr>
</tbody>
</table>
<p>et sa fonction de répartition est:</p>
<p><span class="math display">\[F(x) = \left\{ 
\begin{array}{l l}
 0 &amp; \quad \text{si $x&lt;0$}\\
  3/5 &amp; \quad \text{si $0 \leq x &lt; 1$}\\ 
   1 &amp; \quad \text{si $x \geq 1$}\\
\end{array} \right.\]</span></p>
</div>
</div>
</div>
<div id="moments-dune-variable-aleatoire-discrete" class="section level2">
<h2><span class="header-section-number">0.3</span> Moments d’une variable aléatoire discrète</h2>
<div id="esperance-mathematique" class="section level3 unnumbered">
<h3>Espérance mathématique</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-10" class="definition"><strong>Définition 0.3  </strong></span>Pour une variable aléatoire discrète <span class="math inline">\(X\)</span> de loi de probabilité <span class="math inline">\(p(.)\)</span>,
on définit l’<em>espérance</em> de <span class="math inline">\(X\)</span>, notée <span class="math inline">\(E(X)\)</span>, par l’expression</p>
<p><span class="math display">\[E(X)=\sum_{i \in \mathbb{N}} x_{i} p(x_i)\]</span></p>
En termes concrets, l’espérance de <span class="math inline">\(X\)</span> est la moyenne pondérée des
valeurs que <span class="math inline">\(X\)</span> peut prendre, les poids étant les probabilités que ces
valeurs soient prises.
</div>

<p>Reprenons l’exemple où on joue 3 fois à pile ou face. L’espérance
de <span class="math inline">\(X=\)</span>“nombre de pile obtenus” est égal à:
<span class="math display">\[E(X)=0 \times \frac{1}{8}+1 \times \frac{3}{8}+2 \times \frac{3}{8}+3 \times \frac{1}{8}=1.5\]</span></p>
<p>Dans le cas de la loi uniforme sur <span class="math inline">\(X(\Omega)=\{x_{1},\ldots, x_{k}\}\)</span>,
c’est à dire avec équiprobabilité de toutes les valeurs <span class="math inline">\(p_{i}=1/k\)</span>, on
obtient: <span class="math display">\[E(X)=\frac{1}{k} \sum_{i=1}^k x_{i}\]</span> et dans ce cas <span class="math inline">\(E(X)\)</span>
se confond avec la moyenne arithmétique simple <span class="math inline">\(\bar{x}\)</span> des valeurs
possibles de <span class="math inline">\(X\)</span>.</p>
<p>Pour le jet d’un dé équilibré par exemple:
<span class="math display">\[E(X)=\frac{1}{6} \sum_{i=1}^6 i=\frac{7}{2}=3.5\]</span></p>
</div>
<div id="esperance-dune-fonction-dune-variable-aleatoire" class="section level3 unnumbered">
<h3>Espérance d’une fonction d’une variable aléatoire</h3>

<div class="theorem">
<p><span id="thm:unnamed-chunk-11" class="theorem"><strong>Théorème 0.1  (Théorème du transfert)  </strong></span>Si X est une variable aléatoire discrète pouvant prendre ses valeurs
parmi les valeurs <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i \geq 1\)</span>, avec des probabilités respectives
<span class="math inline">\(p(x_i)\)</span>, alors pour toute fonction réelle <span class="math inline">\(g\)</span> on a</p>
<span class="math display">\[E(g(X)) = \sum_i g(x_i)p(x_i)\]</span>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-12" class="example"><strong>Exemple0.1  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire qui prend une des trois valeurs
<span class="math inline">\(\{-1,0,1\}\)</span> avec les probabilités respectives</p>
<p><span class="math display">\[P(X=-1) = 0.2 \quad \quad P(X=0)=0.5 \quad \quad P(X=1) = 0.3\]</span></p>
Calculer <span class="math inline">\(E(X^2)\)</span>.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution: </em></span> <em>Première approche</em>: Soit <span class="math inline">\(Y=X^2\)</span>. La distribution de <span class="math inline">\(Y\)</span> est donnée par
<span class="math display">\[\begin{aligned}
    P(Y=1) &amp;= P(X=-1) + P(X=1) = 0.5 \\
    P(Y=0) &amp;= P(X=0) = 0.5
  \end{aligned}\]</span> Donc <span class="math display">\[E(X^2)=E(Y) = 1(0.5) + 0(0.5) = 0.5\]</span></p>
<p><em>Deuxième approche</em>: En utilisant le théorème</p>
<p><span class="math display">\[\begin{aligned}
  E(X^2) &amp;= (-1)^2(0.2) + 0^2(0.5) + 1^2 (0.3) \\
         &amp;= 1(0.2+0.3)+0(0.5)=0.5
  \end{aligned}\]</span></p>
Remarquer que <span class="math display">\[0.5=E(X^2) \neq (E(X))^2 = 0.01\]</span>
</div>

<div id="linearite-de-lesperance-proprietes-de-lesperance" class="section level4 unnumbered">
<h4>Linéarité de l’espérance Propriétés de l’espérance</h4>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E(X+a)=E(X)+a, \quad a \in \mathbb{R}\)</span><br />
résultat qui se déduit de:
<span class="math display">\[\sum_{i}p_{i}(x_{i}+a)= \sum_{i}p_{i}x_{i}+\sum_{i}ap_{i}=\sum_{i}p_{i}x_{i}+a \sum_{i}p_{i}=\sum_{i}p_{i}x_{i}+a\]</span></p></li>
<li><p><span class="math inline">\(E(aX)=aE(X), \quad a\in \mathbb{R}\)</span><br />
il suffit d’écrire: <span class="math display">\[\sum_{i}p_{i}a x_{i}=a\sum_{i}p_{i}x_{i}\]</span></p></li>
<li><p><span class="math inline">\(E(X+Y)=E(X)+E(Y)\)</span>, <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> étant deux variables aléatoire.</p></li>
</ol>
<p>On peut résumer ces trois propriétés en disant que l’espérance
mathématique est linéaire:
<span class="math display">\[E(\lambda X + \mu Y)= \lambda E(X)+\mu E(Y), \quad \forall \lambda \in \mathbb{R}, \, \forall \mu \in \mathbb{R}.\]</span></p>
</div>
</div>
<div id="variance" class="section level3 unnumbered">
<h3>Variance</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-14" class="definition"><strong>Définition 0.4  </strong></span>La variance est un indicateur mesurant la dispersion des valeurs <span class="math inline">\(x_{i}\)</span>
que peut prendre la v.a. <span class="math inline">\(X\)</span> et son espérance <span class="math inline">\(E(X)\)</span>. On appelle
<strong>variance</strong> de X, que l’on note <span class="math inline">\(V(X)\)</span>, la quantité</p>
<span class="math display">\[V(X)=E\big[ (X-E(X))^2 \big]\]</span> lorsque cette quantité existe.<br />
C’est l’espérance mathématique du carré de la v.a. centrée <span class="math inline">\(X-E(X)\)</span>.
</div>

<p>On peut établir une autre formule pour le calcul de <span class="math inline">\(V(X)\)</span>:</p>
<p><span class="math display">\[V(X)=E(X^2)-E^2(X)\]</span></p>
<p>Or: <span class="math display">\[\begin{aligned}
      V(X)&amp;= E\left[X^2-2XE(X)+E^2(X)\right] \\
           &amp;=E(X^2)-E[2XE(X)]+ E[E^2(X)]\\
           &amp;=E(X^2)-2E^2(X)+E^2(X) \\ 
           &amp;=E(X^2)-E^2(X)
    \end{aligned}\]</span></p>
<p>On cherche <span class="math inline">\(V(X)\)</span> où <span class="math inline">\(X\)</span> est le nombre obtenu lors du jet d’un dé
équilibré. On a vu dans l’exemple
que <span class="math inline">\(E(X) = \frac{7}{2}\)</span>. De plus,</p>
<p><span class="math display">\[\begin{aligned}
  E(X^2) &amp;= 1^2 \bigg(\frac{1}{6}\bigg) + 2^2 \bigg(\frac{1}{6}\bigg) + 3^2 \bigg(\frac{1}{6}\bigg) + 4^2 \bigg(\frac{1}{6}\bigg) + 5^2 \bigg(\frac{1}{6}\bigg) + 6^2 \bigg(\frac{1}{6}\bigg) \\
        &amp;=\bigg(\frac{1}{6}\bigg) (91) = \frac{91}{6}.\end{aligned}\]</span> Et
donc</p>
<p><span class="math display">\[V(X) = \frac{91}{6} - \bigg(\frac{7}{2}\bigg)^2 = \frac{35}{12}\]</span></p>
<p><strong>Propriétés de la variance</strong></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(V(X) \geq 0\)</span></p></li>
<li><p><span class="math inline">\(V(X+a)=V(X)\)</span><br />
en effet: <span class="math display">\[\begin{aligned}
   V(X+a)   &amp;= E\big[\left[X+a-E(X+a)\right]^2\big] \\ 
        &amp;=E\big[\left[X+a-E(X)-a\right]^2\big] \\ 
        &amp;=E\big[\left[X-E(X)\right]^2\big] \\
        &amp;=V(X). 
   \end{aligned}\]</span></p></li>
<li><p><span class="math inline">\(V(aX)=a^2V(X)\)</span><br />
en effet: <span class="math display">\[\begin{aligned}
   V(aX)  &amp;= E\big[\left[aX-E(aX)\right]^2\big] \\
      &amp;=E\big[\left[aX-aE(X)\right]^2\big] \\ 
      &amp;=E\big[a^2\left[X-E(X)\right]^2\big] \\
      &amp;=a^2\big[E\left[X-E(X)\right]^2\big] \\
      &amp;= a^2V(X). 
   \end{aligned}\]</span></p></li>
</ol>
</div>
<div id="ecart-type" class="section level3 unnumbered">
<h3>Ecart-type</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-15" class="definition"><strong>Définition 0.5  </strong></span>La racine carrée de <span class="math inline">\(V(X)\)</span> est appelée l’<strong><em>écart-type</em></strong> de <span class="math inline">\(X\)</span>, qui se
note <span class="math inline">\(\sigma_{X}\)</span>. On a</p>
<p><span class="math display">\[\sigma_{X} = \sqrt{V(X)}\]</span></p>
<span class="math inline">\(\sigma_{X}\)</span> s’exprime dans les mêmes unités de mesure que la variable
aléatoire <span class="math inline">\(X\)</span>.
</div>

<p>A noter:</p>
<ul>
<li><p>L’écart type sert à mesurer la dispersion d’un ensemble de données.</p></li>
<li><p>Plus il est faible, plus les valeurs sont regroupées autour de la
moyenne.</p></li>
<li><p>Exemple: La répartition des notes d’une classe. Plus l’écart type
est faible, plus la classe est homogène.</p></li>
<li><p>L’espérance et l’écart-type sont reliés par l’<em>inégalité de
Bienaymé-Tchebychev</em>.</p></li>
</ul>
<div id="inegalite-de-bienayme-tchebychev" class="section level4 unnumbered">
<h4>Inégalité de Bienaymé-Tchebychev</h4>

<div class="theorem">
<p><span id="thm:unnamed-chunk-16" class="theorem"><strong>Théorème 0.2  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire d’espérance <span class="math inline">\(\mu\)</span> et de variance
<span class="math inline">\(\sigma^2\)</span>. Pour tout <span class="math inline">\(\varepsilon &gt; 0\)</span>, on a l’inégalité suivante:
<span class="math display">\[P\left(|X-E(X)| \geq \varepsilon \right) \leq \frac{\sigma^2}{\varepsilon^2}\]</span></p>
On peut l’écrire autrement. Soit <span class="math inline">\(k=\varepsilon/\sigma\)</span>.
<span class="math display">\[P\left(|X-E(X)| \geq k\sigma \right) \leq \frac{1}{k^2}\]</span>
</div>

<div class="rmdinsight">
<p>
<span style="color: blue">Importance</span>: Cette inégalité relie la probabilité pour <span class="math inline"><span class="math inline">\(X\)</span></span> de s’écarter de sa moyenne <span class="math inline"><span class="math inline">\(E(X)\)</span></span>, à sa variance qui est justement un indicateur de dispersion autour de la moyenne de la loi. Elle montre quantitativement que “plus l’écart type est faible, plus la probabilité de s’écarter de la moyenne est faible”.
</p>
</div>

<div class="theorem">
<span id="thm:unnamed-chunk-18" class="theorem"><strong>Théorème 0.3  (Inégalité de Markov)  </strong></span>Soit <span class="math inline">\(X\)</span> une variable aléatoire à valeur non
négatives. Pour tout réel <span class="math inline">\(a &gt; 0\)</span> <span class="math display">\[P(X&gt;a) \leq \frac{E(X)}{a}\]</span>
</div>

</div>
</div>
<div id="moments-non-centres-et-centres" class="section level3 unnumbered">
<h3>Moments non centrés et centrés</h3>
<p>On appelle moment non centré d’ordre <span class="math inline">\(r \in \mathbb{N^*}\)</span> de <span class="math inline">\(X\)</span> la
quantité, lorsqu’elle existe:
<span class="math display">\[m_{r}(X)=\sum_{i \in \mathbb{N} } x_{i}^r p(x_{i})=E(X^r).\]</span> Le moment
centré d’ordre <span class="math inline">\(r \in \mathbb{N^*}\)</span> est la quantité, lorsqu’elle existe:
<span class="math display">\[\mu_{r}(X)=\sum_{i \in \mathbb{N} } p_{i}\left[x_{i}-E(X)\right]^r=E\left[X-E(X)\right]^r.\]</span></p>
<p>Les premiers moments sont: <span class="math display">\[m_{1}(X)=E(X), \quad \mu_{1}(X)=0\]</span>
<span class="math display">\[\mu_{2}(X)=V(X)=m_{2}(X)-m_{1}^2(X)\]</span></p>
</div>
</div>
<div id="couple-de-variables-aleatoires-discretes" class="section level2">
<h2><span class="header-section-number">0.4</span> Couple de variables aléatoires discrètes</h2>
<p>Considérons deux variables aléatoires discrètes <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>. Il nous faut
pour modéliser le problème une fonction qui nous donne la probabilité
que <span class="math inline">\((X = x_i )\)</span> en même temps que <span class="math inline">\((Y = y_j )\)</span>. C’est la loi de
probabilité conjointe.</p>
<p>Soit <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> deux variables aléatoires réelles discrètes, définies
sur un espace probabilisé <span class="math inline">\((\Omega,\mathcal{A},P)\)</span> et que</p>
<p><span class="math display">\[\begin{aligned}
  X(\Omega) &amp;= \{x_1,x_2,\ldots,x_l\} \\
  Y(\Omega) &amp;= \{y_1,y_2,\ldots,y_k\} \\
            &amp; \quad (l \text{ et } k \in \mathbb{N})\end{aligned}\]</span></p>
<p>La <strong><em>loi du couple <span class="math inline">\((X,Y)\)</span></em></strong>, dite <strong>loi de probabilité conjointe ou
simultanée</strong>, est entièrement définie par les probabilités:</p>
<p><span class="math display">\[p_{ij} = P(X=x_i;Y=y_j) = P(\{X=x_i\}\cap\{Y=y_j\})\]</span></p>
<p>On a</p>
<p><span class="math display">\[p_{ij} \geq 0 \quad \text{et} \quad \sum_{i=1}^{l} \sum_{j=1}^{k} p_{ij} = 1\]</span></p>
<p>Le couple <span class="math inline">\((X,Y)\)</span> s’appelle variable aléatoire à deux dimensions et peut
prendre <span class="math inline">\(l\times k\)</span> valeurs.</p>
<div id="table-de-probabilite-conjointe" class="section level3 unnumbered">
<h3>Table de probabilité conjointe</h3>
<p>Les probabilités <span class="math inline">\(p_{ij}\)</span> peuvent être présentées dans un tableau à deux
dimensions qu’on appelle table de probabilité conjointe:</p>
<table>
<caption>Table de probabilité conjointe</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(X\)</span>\<span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(y_1\)</span></th>
<th align="center"><span class="math inline">\(y_2\)</span></th>
<th align="center"><span class="math inline">\(\ldots\)</span></th>
<th align="center"><span class="math inline">\(y_j\)</span></th>
<th align="center"><span class="math inline">\(\ldots\)</span></th>
<th align="center"><span class="math inline">\(y_k\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1\)</span></td>
<td align="center"><span class="math inline">\(p_{11}\)</span></td>
<td align="center"><span class="math inline">\(p_{12}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{1j}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{1k}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_2\)</span></td>
<td align="center"><span class="math inline">\(p_{21}\)</span></td>
<td align="center"><span class="math inline">\(p_{22}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{2j}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{2k}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_i\)</span></td>
<td align="center"><span class="math inline">\(p_{i1}\)</span></td>
<td align="center"><span class="math inline">\(p_{i2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{ij}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{ik}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_l\)</span></td>
<td align="center"><span class="math inline">\(p_{l1}\)</span></td>
<td align="center"><span class="math inline">\(p_{l2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{lj}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{lk}\)</span></td>
</tr>
</tbody>
</table>
<p>A la première ligne figure l’ensemble des valeurs de <span class="math inline">\(Y\)</span> et à la
première colonne figure l’ensemble des valeurs de <span class="math inline">\(X\)</span>. La probabilité
<span class="math inline">\(p_{ij} = P(X=x_i;Y=y_j)\)</span> est à l’intersection de la <span class="math inline">\(i^{e}\)</span> et de la
<span class="math inline">\(j^{e}\)</span> colonne.</p>
</div>
<div id="lois-marginales" class="section level3 unnumbered">
<h3>Lois marginales</h3>
<p>Lorsqu’on connaît la loi conjointe des variables aléatoires <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>,
on peut aussi s’intéresser à la loi de probabilité de <span class="math inline">\(X\)</span> seule et de
<span class="math inline">\(Y\)</span> seule. Ce sont les lois de probabilité marginales.</p>
<ul>
<li><p>Loi marginale de <span class="math inline">\(X\)</span>:
<span class="math display">\[p_{i.} = P(X=x_i) = P[\{X=x_i\}\cap \Omega] = \sum_{j=1}^k p_{ij} \quad \quad \forall \, i=1,2,\ldots,l\]</span></p></li>
<li><p>Loi marginale de <span class="math inline">\(Y\)</span>:
<span class="math display">\[p_{.j} = P(Y=y_j) = P[ \Omega \cap \{Y=y_j\}] = \sum_{i=1}^l p_{ij} \quad \quad \forall \, j=1,2,\ldots,k\]</span></p></li>
</ul>
<p>On peut calculer les lois marginales directement depuis la table de la
loi conjointe. La loi marginale de <span class="math inline">\(X\)</span> est
calculée en faisant les totaux par ligne, tandis que celle de <span class="math inline">\(Y\)</span> l’est
en faisant les totaux par colonne.</p>
<p>C’est le fait que les lois de <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> individuellement puissent être
lues dans les marges du tableau qui leur vaut leur nom de lois
marginales.</p>
<table>
<caption>Table de probabilité conjointe avec les lois marginales</caption>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(X\)</span>\<span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(y_1\)</span></th>
<th align="center"><span class="math inline">\(y_2\)</span></th>
<th align="center"><span class="math inline">\(\ldots\)</span></th>
<th align="center"><span class="math inline">\(y_j\)</span></th>
<th align="center"><span class="math inline">\(\ldots\)</span></th>
<th align="center"><span class="math inline">\(y_k\)</span></th>
<th align="center">Marginale de <span class="math inline">\(X\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1\)</span></td>
<td align="center"><span class="math inline">\(p_{11}\)</span></td>
<td align="center"><span class="math inline">\(p_{12}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{1j}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{1k}\)</span></td>
<td align="center"><span class="math inline">\(p_{1.}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_2\)</span></td>
<td align="center"><span class="math inline">\(p_{21}\)</span></td>
<td align="center"><span class="math inline">\(p_{22}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{2j}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{2k}\)</span></td>
<td align="center"><span class="math inline">\(p_{2.}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_i\)</span></td>
<td align="center"><span class="math inline">\(p_{i1}\)</span></td>
<td align="center"><span class="math inline">\(p_{i2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{ij}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{ik}\)</span></td>
<td align="center"><span class="math inline">\(p_{i.}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_l\)</span></td>
<td align="center"><span class="math inline">\(p_{l1}\)</span></td>
<td align="center"><span class="math inline">\(p_{l2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{lj}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{lk}\)</span></td>
<td align="center"><span class="math inline">\(p_{l.}\)</span></td>
</tr>
<tr class="odd">
<td align="center">Marginale de <span class="math inline">\(Y\)</span></td>
<td align="center"><span class="math inline">\(p_{.1}\)</span></td>
<td align="center"><span class="math inline">\(p_{.2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{.l}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(p_{.k}\)</span></td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<div class="rmdexercise">
<p>
On tire au hasard 3 boules d’une urne contenant 3 boules rouges, 4 blanches et 5 noires. <span class="math inline"><span class="math inline">\(X\)</span></span> et <span class="math inline"><span class="math inline">\(Y\)</span></span> désignent respectivement le nombre de boules rouges et celui de boules blanches tirées. Déterminer la loi de probabilité conjointe du couple <span class="math inline"><span class="math inline">\((X,Y)\)</span></span> ainsi que les lois marginales de <span class="math inline"><span class="math inline">\(X\)</span></span> et de <span class="math inline"><span class="math inline">\(Y\)</span></span>.
</p>
</div>
</div>
<div id="lois-conditionnelles" class="section level3 unnumbered">
<h3>Lois conditionnelles</h3>
<p>Pour chaque valeur <span class="math inline">\(y_j\)</span> de <span class="math inline">\(Y\)</span> telle que <span class="math inline">\(p_{.j} = P(Y=y_j) \neq 0\)</span> on
peut définir la loi conditionnelle de <span class="math inline">\(X\)</span> sachant <span class="math inline">\(Y=y_j\)</span> par</p>
<p><span class="math display">\[p_{i/j} = P(X=x_i / Y=y_j) = \frac{P(X=x_i;Y=y_j)}{P(Y=y_j)} = \frac{p_{ij}}{p_{.j}} \quad \quad \forall i = 1,2,\ldots,l\]</span></p>
<p>De même on définit la loi de <span class="math inline">\(Y\)</span> sachant <span class="math inline">\(X=x_i\)</span> par</p>
<p><span class="math display">\[p_{j/i} = P(Y=y_j / X=x_i) = \frac{P(X=x_i;Y=y_j)}{P(X=x_i)} = \frac{p_{ij}}{p_{i.}} \quad \quad \forall j = 1,2,\ldots,k\]</span></p>
</div>
<div id="independance-de-variables-aleatoires" class="section level3 unnumbered">
<h3>Indépendance de variables aléatoires</h3>

<div class="theorem">
<p><span id="thm:unnamed-chunk-20" class="theorem"><strong>Théorème 0.4  </strong></span>On dit que deux v.a.r.d sont indépendantes si et seulement si</p>
<span class="math display">\[P(X=x_i;Y=y_j) = P(X=x_i) P(Y=y_j) \quad \quad \forall \, i = 1,2,\ldots,l \text{ et }  j = 1,2,\ldots,k\]</span>
</div>

<p>On montre que</p>
<p><span class="math display">\[P(\{X\in A\} \cap \{Y \in B\}) = P(\{X\in A\}) P(\{Y \in B\}) \quad \quad \forall \,\, A \text{ et } B \in \mathcal{A}\]</span></p>
<p><strong>Propriétés</strong></p>
<p>Soit deux v.a.r.d. <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>,</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E(X+Y)=E(X)+E(Y)\)</span></p></li>
<li><p>Si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes alors <span class="math inline">\(E(XY)=E(X)E(Y)\)</span>. Mais la
réciproque n’est pas toujours vraie.</p></li>
</ol>
</div>
<div id="covariance" class="section level3 unnumbered">
<h3>Covariance</h3>
<p>Soit <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> deux v.a.r.d. On appelle <strong><em>covariance</em></strong> de <span class="math inline">\(X\)</span> et de
<span class="math inline">\(Y\)</span> la valeur si elle existe de</p>
<p><span class="math display">\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))] = \sum_i \sum_j (x_i-E(X))(y_j-E(Y)) p_{ij}\]</span></p>
<p>qu’on peut calculer en utilisant la formule suivante</p>
<p><span class="math display">\[Cov(X,Y) = E(XY) - E(X)E(Y)\]</span></p>
<p><strong>Propriétés</strong></p>
<ul>
<li><p><span class="math inline">\(Cov(X,Y)=Cov(Y,X)\)</span></p></li>
<li><p><span class="math inline">\(Cov(aX_1+bX_2,Y) = a Cov(X_1,Y) + b Cov(X_2,Y)\)</span></p></li>
<li><p><span class="math inline">\(V(X+Y)= V(X) + V(Y) + 2 Cov(X,Y)\)</span></p></li>
<li><p>Si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont indépendantes alors</p>
<ul>
<li><p><span class="math inline">\(Cov(X,Y) = 0\)</span> (la réciproque n’est pas vraie)</p></li>
<li><p><span class="math inline">\(V(X+Y) = V(X) + V(Y)\)</span> (la réciproque n’est pas vraie)</p></li>
</ul></li>
</ul>
</div>
<div id="coefficient-de-correlation-lineaire" class="section level3 unnumbered">
<h3>Coefficient de corrélation linéaire</h3>
<p>On appelle coefficient de corrélation linéaire de <span class="math inline">\(X\)</span> et de <span class="math inline">\(Y\)</span> la
valeur définie par</p>
<p><span class="math display">\[\rho = \rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}\]</span></p>
<p>On peut montrer que <span class="math display">\[-1 \leq \rho(X,Y) \leq 1\]</span></p>
<p>Pour le montrer on peut partir du fait que la variance est toujours
positive ou nulle. Donc
<span class="math inline">\(V(\frac{X}{\sigma_X} + \frac{Y}{\sigma_Y}) \geq 0\)</span> et
<span class="math inline">\(V(\frac{X}{\sigma_X} - \frac{Y}{\sigma_Y}) \geq 0\)</span>.</p>
<p><strong>Interprétation de <span class="math inline">\(\rho\)</span></strong></p>
<ul>
<li><p>Le coefficient de corrélation est une mesure du degré de linéarité
entre <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Les valeurs de <span class="math inline">\(\rho\)</span> proches de <span class="math inline">\(1\)</span> ou <span class="math inline">\(-1\)</span> indiquent une linéarité
quasiment rigoureuse entre <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Les valeurs de <span class="math inline">\(\rho\)</span> proche de 0 indiquent une absence de toute
relation linéaire.</p></li>
<li><p>Lorsque <span class="math inline">\(\rho(X,Y)\)</span> est positif, <span class="math inline">\(Y\)</span> a tendance à augmenter si <span class="math inline">\(X\)</span>
en fait autant.</p></li>
<li><p>Lorsque <span class="math inline">\(\rho(X,Y) &lt; 0\)</span>, <span class="math inline">\(Y\)</span> a tendance à diminuer si <span class="math inline">\(X\)</span> augmente.</p></li>
<li><p>Si <span class="math inline">\(\rho(X,Y) =0\)</span>, on dit que ces deux statistiques sont non
corrélées.</p></li>
</ul>
</div>
</div>
<div id="lois-usuelles-discretes" class="section level2">
<h2><span class="header-section-number">0.5</span> Lois usuelles discrètes</h2>
<div id="loi-uniforme-discrete-mathcalun" class="section level3">
<h3><span class="header-section-number">0.5.1</span> Loi uniforme discrète <span class="math inline">\(\mathcal{U}(n)\)</span></h3>

<div class="definition">
<p><span id="def:unnamed-chunk-21" class="definition"><strong>Définition 0.6  </strong></span>Une distribution de probabilité suit une loi uniforme lorsque toutes les
valeurs prises par la variable aléatoire sont équiprobables. Si <span class="math inline">\(n\)</span> est
le nombre de valeurs différentes prises par la variable aléatoire alors
on a:</p>
<span class="math display">\[\label{eq:unif}
    P(X=x_i)=\frac{1}{n} \qquad \forall \, i \in \{1,\ldots, n\}\]</span>
</div>

<p><strong>Exemple:</strong> La distribution des chiffres obtenus au lancer de dé (si ce dernier est
non pipé) suit une loi uniforme dont la loi de probabilité est la
suivante :</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = x_i)\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{6}\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Moments de loi uniforme discrète</strong></p>
<p>Dans le <strong>cas particulier</strong> d’une loi uniforme discrète où chaque valeur
de la variable aléatoire <span class="math inline">\(X\)</span> correspond à son rang, i.e.
<span class="math inline">\(x_i=i \, \, \forall i \in \{1,\ldots, n\}\)</span>, on a:
<span class="math display">\[E(X)=\frac{n+1}{2} \quad \text{et} \quad V(X)=\frac{n^2-1}{12}\]</span> La
démonstration de ces résultats est établie en utilisant les égalités
(cf. Annexe)
<span class="math display">\[\sum_{i=1}^n i=\frac{n(n+1)}{2} \quad \text{et} \quad \sum_{i=1}^n i^2=\frac{n(n+1)(2n+1)}{6}.\]</span></p>
<p>En revenant à l’exemple du lancer du dé de cette section, on peut
calculer directement les moments de <span class="math inline">\(X\)</span>: <span class="math display">\[E(X)=\frac{6+1}{2}=3.5\]</span> et
<span class="math display">\[V(X)=\frac{6^2-1}{12}=\frac{35}{12}\simeq 2.92.\]</span></p>
</div>
<div id="loi-de-bernoulli-mathcalbp" class="section level3">
<h3><span class="header-section-number">0.5.2</span> Loi de Bernoulli <span class="math inline">\(\mathcal{B}(p)\)</span></h3>

<div class="definition">
<p><span id="def:unnamed-chunk-22" class="definition"><strong>Définition 0.7  </strong></span>On réalise une expérience dont le résultat sera interprété soit comme un
succès soit comme un échec. On définit alors la variable aléatoire <span class="math inline">\(X\)</span>
en lui donnant la valeur 1 lors d’un succès et 0 lors d’un échec
(variable indicatrice). La loi de probabilité de <span class="math inline">\(X\)</span> est alors</p>
<p><span class="math display" id="eq:bern">\[\begin{align}
    &amp;p(1)=P(X=1)=p \tag{0.1} \\ 
    &amp;p(0)=P(X=0)= 1-p=q \notag
\end{align}\]</span></p>
<p>où <span class="math inline">\(p\)</span> est la probabilité d’un succès, <span class="math inline">\(0 \leq p \leq 1\)</span>.</p>
Une variable aléatoire <span class="math inline">\(X\)</span> est dite de <strong>Bernoulli</strong>
<span class="math inline">\(X \sim \mathcal{B} \left({p}\right)\)</span> s’il existe un nombre
<span class="math inline">\(p \, \in \, ]0,1[\)</span> tel que la loi de probabilité de <span class="math inline">\(X\)</span> soit donnée par <a href="variables-aleatoires-discretes.html#eq:bern">(0.1)</a>.
</div>

<p>La fonction de répartition est définie par: <span class="math display">\[F(x) = 
       \left\{
       \begin{array}{ll}
         0 &amp; \quad \text{si $x &lt; 0$} \\
         1 - p &amp; \quad \text{si $0 \leq x &lt; 1$} \\
         1 &amp; \quad \text{si $x \geq 1$}.
       \end{array}
       \right.\]</span></p>
<p>L’espérance la loi de Bernoulli est <span class="math inline">\(p\)</span>, en effet</p>
<p><span class="math display">\[E(X) =1 \times P(X=1)+0 \times P(X=0)=P(X=1)=p\]</span></p>
<p>La variance la loi de Bernoulli est <span class="math inline">\(np\)</span>, en effet</p>
<p><span class="math display">\[V(X) =E(X^2)-E^2(X)=p-p^2=p(1-p)=pq\]</span> car
<span class="math display">\[E(X^2) =1^2\times P(X=1)+0^2 \times P(X=0)=P(X=1)=p\]</span></p>
</div>
<div id="loi-binomiale-mathcalbnp" class="section level3">
<h3><span class="header-section-number">0.5.3</span> Loi Binomiale <span class="math inline">\(\mathcal{B}(n,p)\)</span></h3>

<div class="rmdtip">
Décrite pour la première fois par <em>Isaac Newton</em> en 1676 et démontrée pour la première fois par le mathématicien suisse <em>Jacob Bernoulli</em> en 1713, la loi binomiale est l’une des distributions de probabilité les plus fréquemment rencontrées en statistique appliquée.
</div>

<p>Supposons qu’on exécute maintenant <span class="math inline">\(n\)</span> épreuves <strong>indépendantes</strong>,
chacune ayant <span class="math inline">\(p\)</span> pour probabilité de succès et <span class="math inline">\(1-p\)</span> pour probabilité
d’échec. La variable aléatoire <span class="math inline">\(X\)</span> qui compte <strong>le nombre de succès</strong>
sur l’ensemble des <span class="math inline">\(n\)</span> épreuves est dite variable aléatoire
<strong>binomiale</strong> de paramètres <span class="math inline">\(n\)</span> et <span class="math inline">\(p\)</span>.</p>

<div class="rmdinsight">
Une variable de Bernoulli n’est donc qu’une variable binomiale de paramètres <span class="math inline">\((1,p)\)</span>.
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-25" class="definition"><strong>Définition 0.8  </strong></span>Si on effectue <span class="math inline">\(n\)</span> épreuves successives indépendantes où on note à
chaque fois la réalisation ou non d’un certain événement <span class="math inline">\(A\)</span>, on obtient une suite de la forme <span class="math inline">\(AA\bar{A}A\bar{A}\ldots \bar{A}AA\)</span>. Soit <span class="math inline">\(X\)</span> le nombre de réalisations de <span class="math inline">\(A\)</span>. On définit ainsi une v.a. <span class="math inline">\(X\)</span> qui suit une loi binomiale de paramètres <span class="math inline">\(n\)</span> et <span class="math inline">\(p=P(A)\)</span>, caractérisée par
<span class="math inline">\(X(\Omega)=\{0, 1,\ldots, n\}\)</span> :</p>
<p><span class="math display" id="eq:binom">\[\begin{equation}
    P(X=k)=\binom{n}{k}p^k (1-p)^{n-k} \qquad 0\leq k \leq n
    \tag{0.2}
\end{equation}\]</span></p>
On écrit <span class="math inline">\(X \sim \mathcal{B} \left({n, p}\right)\)</span>. Donc la loi binomiale modélise le nombre de réalisations de <span class="math inline">\(A\)</span> (succès) obtenues lors de la répétition indépendante et identique de <span class="math inline">\(n\)</span> épreuves de Bernoulli.
</div>


<div class="rmdinsight">
Pour établir <a href="variables-aleatoires-discretes.html#eq:binom">(0.2)</a> il faut remarquer que <span class="math inline">\(\binom{n}{k}\)</span> est le nombre d’échantillons de taille <span class="math inline">\(n\)</span> comportant exactement <span class="math inline">\(k\)</span> événements <span class="math inline">\(A\)</span>, de probabilité <span class="math inline">\(p^k\)</span>, indépendamment de l’ordre, et donc <span class="math inline">\(n-k\)</span> événements <span class="math inline">\(\bar{A}\)</span>, de probabilité <span class="math inline">\((1-p)^{n-k}\)</span>.
</div>

<p><strong>Remarque:</strong> Il est possible d’obtenir aisément les valeurs des combinaisons de la loi binomiale en utilisant le triangle de Pascal.</p>
<p>En utilisant la formule du binôme de Newton, on vérifie
bien que c’est une loi de probabilité:</p>
<p><span class="math display">\[{\sum_{k=0}^nP(X=k)=\sum_{k=0}^n\binom{n}{k} p^{k}(1-p)^{n-k}=[p+(1-p)]^n=1}\]</span></p>
<p><strong>Exemple:</strong> On jette cinq pièces équilibrées. Les résultats sont supposés
indépendants. Donner la loi de probabilité de la variable <span class="math inline">\(X\)</span> qui compte
le nombre de piles obtenus.</p>
<p><strong>Moments de la loi Binomiale</strong></p>
<p>Pour calculer facilement les moments de cette loi, nous allons associer
à chaque épreuve <span class="math inline">\(i\)</span>, <span class="math inline">\(1\leq i \leq n\)</span>, une v.a. de Bernoulli (variable
indicatrice sur <span class="math inline">\(A\)</span>): <span class="math display">\[{1}_A=X_i = \left\{ 
\begin{array}{l l}
 1 &amp; \quad \text{si $A$ est réalisé}\\
 0 &amp; \quad \text{si $\bar{A}$ est réalisé}\\ 
  \end{array} \right.\]</span> On peut écrire alors:
<span class="math inline">\(X=\sum_{i=1}^nX_i=X_1+X_2+\ldots+X_n\)</span>, ce qui nous permet de déduire
aisément: <span class="math display">\[\begin{aligned}
    E(X)&amp;=E\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^nE(X_i)=np \\
    \text{et} \nonumber \\
    V(X)&amp;=V\left(\sum_{i=1}^nX_i\right)=\sum_{i=1}^nV(X_i)=np(1-p) \quad \text{car les v.a. $X_i$ sont indépendantes.}
  \end{aligned}\]</span></p>
<p>Le calcul direct des moments de <span class="math inline">\(X\)</span> peut s’effectuer à partir de la
définition générale, mais de façon beaucoup plus laborieuse:
<span class="math display">\[\begin{aligned}
 E(X)&amp;= \sum_{k=0}^nk \binom{n}{k} p^{k}(1-p)^{n-k}=\sum_{k=1}^nk \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\
 &amp;= \sum_{k=1}^n\frac{n!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k}= np \sum_{k=1}^n\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1}(1-p)^{n-k} \\ 
 &amp;= np \sum_{j=0}^{n-1}\frac{(n-1)!}{j!(n-1-j)!}p^j (1-p)^{n-1-j} =np \sum_{j=0}^{n-1}\binom{n-1}{j} p^{j}(1-p)^{n-1-j} \\
 &amp;= np [p+(1-p)]^{n-1}=np
 \end{aligned}\]</span></p>
<p>Pour obtenir <span class="math inline">\(E(X^2)\)</span> par un procédé de calcul identique, on passe par
l’intermédiaire du moment factoriel <span class="math inline">\(E[X(X-1)]=E(X^2)-E(X)\)</span>:
<span class="math display">\[\begin{aligned}
  E[X(X-1)]&amp;= \sum_{k=0}^nk(k-1) \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\ 
  &amp;= n(n-1)p^2 \sum_{k=2}^{n}\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2}(1-p)^{n-k} \\ &amp;= n(n-1)p^2 \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j}(1-p)^{n-2-j} \\
  &amp;= n(n-1)p^2[p+(1-p)]^{n-2}= n(n-1)p^2
   \end{aligned}\]</span> On en déduit alors:
<span class="math display">\[E(X^2)=E[X(X-1)]+E(X)= n(n-1)p^2+np,\]</span> puis: <span class="math display">\[\begin{aligned}
   V(X)&amp;=n(n-1)p^2+np-(np)^2 \\ &amp;=n^2p^2+np(1-p)-n^2p^2 \\ &amp;=np(1-p).
  \end{aligned}\]</span></p>
<p>Le nombre de résultats pile apparus au cours de <span class="math inline">\(n\)</span> jets d’une pièce de
monnaie suit une loi binomiale <span class="math inline">\(\mathcal{B} \left({n, 1/2}\right)\)</span>:
<span class="math display">\[P(X=k)=\binom{n}{k}\left(\frac{1}{2}\right)^k \left(\frac{1}{2}\right)^{n-k}=\frac{\binom{n}{k}}{2^n}, \quad 0\leq k \leq n\]</span>
avec <span class="math inline">\(E(X)=n/2\)</span> et <span class="math inline">\(V(X)=n/4\)</span>.</p>
<p>Le nombre <span class="math inline">\(N\)</span> de boules rouges apparues au cours de <span class="math inline">\(n\)</span> tirages avec
remise dans une urne contenant deux rouges, trois vertes et une noire
suit une loi binomiale <span class="math inline">\(\mathcal{B} \left({n, 1/3}\right)\)</span>:
<span class="math display">\[P(N=k)=\binom{n}{k}\left(\frac{1}{3}\right)^k \left(\frac{2}{3}\right)^{n-k}=\binom{n}{k} \frac{2^{n-k}}{3^n}, \quad 0\leq k \leq n\]</span>
avec <span class="math inline">\(E(X)=n/3\)</span> et <span class="math inline">\(V(X)=2n/9\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-27" class="theorem"><strong>Théorème 0.5  </strong></span>Si <span class="math inline">\(X_1 \sim \mathcal{B} \left({n_1, p}\right)\)</span> et
<span class="math inline">\(X_2 \sim \mathcal{B} \left({n_2, p}\right)\)</span>, les v.a. <span class="math inline">\(X_1\)</span> et <span class="math inline">\(X_2\)</span>
étant indépendantes, alors
<span class="math inline">\(X_1+X_2 \sim \mathcal{B} \left({n_1+n_2, p}\right)\)</span>. Ceci résulte de la
définition d’une loi binomiale puisqu’on totalise ici le résultat de
<span class="math inline">\(n_1+n_2\)</span> épreuves indépendantes.
</div>

</div>
<div id="loi-de-poisson-mathcalplambda" class="section level3 unnumbered">
<h3>Loi de Poisson <span class="math inline">\(\mathcal{P}(\lambda)\)</span></h3>

<div class="rmdtip">
La loi de Poisson est découverte au début du XIX<span class="math inline">\(^e\)</span> siècle par le
magistrat français <em>Siméon-Denis Poisson</em>. Les variables aléatoires
de Poisson ont un champ d’application fort vaste, en particulier du
fait qu’on peut les utiliser pour approximer des variables
aléatoires binomiales de paramètres <span class="math inline">\((n,p)\)</span> pour autant que <span class="math inline">\(n\)</span> soit
grand et <span class="math inline">\(p\)</span> assez petit pour que <span class="math inline">\(np\)</span> soit d’ordre de grandeur
moyen.
</div>


<div class="definition">
<span id="def:unnamed-chunk-29" class="definition"><strong>Définition 0.9  </strong></span>Une v.a. <span class="math inline">\(X\)</span> suit une loi de Poisson de paramètre <span class="math inline">\(\lambda&gt;0\)</span> si c’est
une variable à valeurs entières, <span class="math inline">\(X(\Omega)=\mathbb{N}\)</span>, donc avec une
infinité de valeurs possibles, de probabilité: <span class="math display">\[\label{eq:poisson}
    P(X=k)=e^{-\lambda} \frac{\lambda^k}{k!}, \quad k \in \mathbb{N}\]</span>
Cette loi ne dépend qu’un seul paramètre réel positif <span class="math inline">\(\lambda\)</span>, avec
l’écriture symbolique <span class="math inline">\(X \sim \mathcal{P}(\lambda)\)</span>.
</div>

<p>Le développement en série entière de l’exponentielle
<span class="math inline">\(e^\lambda=\sum_{k=0}^{+\infty} \frac{\lambda^k}{k!}\)</span> permet de
vérifier qu’il s’agit bien d’une loi de probabilité:
<span class="math display">\[\sum_{k=0}^{\infty} P(X=k)=\sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^k}{k!}=e^{-\lambda}\sum_{k=0}^{\infty} \frac{\lambda^k}{k!}=e^{-\lambda}e^{\lambda}=1\]</span></p>
<p><strong>Moments de loi de Poisson</strong></p>
<p>Le calcul de l’espérance mathématique se déduit du développement en
série entière de l’exponentielle: <span class="math display">\[\begin{aligned}
    E(X)&amp;=\sum_{k=0}^{\infty} k P(X=k)=\sum_{k=1}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} \\
        &amp;=e^{-\lambda} \sum_{k=1}^{\infty}  \frac{\lambda^k}{(k-1)!}=\lambda e^{-\lambda} \sum_{k=1}^{\infty}  \frac{\lambda^{k-1}}{(k-1)!} \\
        &amp;= \lambda e^{-\lambda} \sum_{j=0}^{\infty}  \frac{\lambda^{j}}{j!}= \lambda e^{-\lambda}  e^{\lambda} \\
        &amp;= \lambda.\end{aligned}\]</span> Pour calculer la variance nous
n’allons pas calculer <span class="math inline">\(E(X^2)\)</span> mais le moment factoriel <span class="math inline">\(E[X(X-1)]\)</span> qui
s’obtient plus facilement, selon la méthode précédente:
<span class="math display">\[\begin{aligned}
    E[X(X-1)] &amp;=\sum_{k=0}^{\infty} k(k-1)P(X=k)=\sum_{k=2}^{\infty} k(k-1)  \,e^{-\lambda} \frac{\lambda^k}{k!} \\
        &amp;=e^{-\lambda} \sum_{k=2}^{\infty}  \frac{\lambda^k}{(k-2)!}=\lambda^2 e^{-\lambda} \sum_{k=2}^{\infty}  \frac{\lambda^{k-2}}{(k-2)!} \\ 
        &amp;= \lambda^2 e^{-\lambda} \sum_{j=0}^{\infty}  \frac{\lambda^{j}}{j!}= \lambda^2 e^{-\lambda}  e^{\lambda} = \lambda^2.\end{aligned}\]</span>
On en déduit: <span class="math display">\[\begin{aligned}
    V(X)&amp;=E(X^2)-E^2(X)=E[X(X-1)]+E(X)-E^2(X) \\
        &amp;=\lambda^2+\lambda-\lambda^2=\lambda.\end{aligned}\]</span></p>

<div class="theorem">
<span id="thm:unnamed-chunk-30" class="theorem"><strong>Théorème 0.6  </strong></span>Si <span class="math inline">\(X\)</span> et <span class="math inline">\(Y\)</span> sont deux variables <strong>indépendantes</strong> suivant des lois de
Poisson
<span class="math display">\[X \sim \mathcal{P}(\lambda) \quad \text{et} \quad Y \sim \mathcal{P}(\mu)\]</span>
alors leur somme suit aussi une loi de Poisson:
<span class="math display">\[X+Y \sim \mathcal{P}(\lambda+\mu).\]</span>
</div>

<p><strong>Exemple:</strong> Soit <span class="math inline">\(X\)</span> la variable aléatoire associée au nombre de micro-ordinateurs
vendus chaque jour dans le magasin. On suppose que <span class="math inline">\(X\)</span> suit une loi de
Poisson de paramètre <span class="math inline">\(\lambda=5\)</span>. On écrit alors
<span class="math inline">\(X \sim \mathcal{P}(5).\)</span><br />
La probabilité associée à la vente de 5 micro-ordinateurs se détermine
par : <span class="math display">\[P(X=5)=e^{-5} \frac{5^5}{5!}=e^{-5}\simeq 0.1755\]</span> La
probabilité de vendre au moins 2 micro-ordinateurs est égal à:
<span class="math display">\[\begin{aligned}
P(X \geq 2)&amp;=1-\left(e^{-5} \frac{5^0}{0!}+e^{-5} \frac{5^1}{1!}\right)\simeq 0.9596\end{aligned}\]</span>
Le nombre moyen de micro-ordinateurs vendus chaque jour dans le magasin
est égal à 5 puisque <span class="math inline">\(E(X)=\lambda=5\)</span>.</p>
</div>
<div id="approximation-dune-loi-binomiale" class="section level3 unnumbered">
<h3>Approximation d’une loi binomiale</h3>
<p>Le théorème de Poisson nous montre que si <span class="math inline">\(n\)</span> est suffisamment grand et
<span class="math inline">\(p\)</span> assez petit, alors on peut approcher la distribution d’une loi
binomiale de paramètres <span class="math inline">\(n\)</span> et <span class="math inline">\(p\)</span> par celle d’une loi de Poisson de
paramètre <span class="math inline">\(\lambda=np\)</span>, en effet
<span class="math display">\[\text{si} \; n \rightarrow \infty \; \text{et}\; p \rightarrow 0 \; \text{alors} \; X: \mathcal{B}(n, p) \rightarrow \mathcal{P}(\lambda).\]</span></p>
<p>Une bonne approximation est obtenue si <span class="math inline">\(n \geq 50\)</span> et <span class="math inline">\(np \leq 5\)</span>.</p>
<p>Dans ce contexte, la loi de Poisson est souvent utilisée pour modéliser
le nombre de succès lorsqu’on répète un très grand nombre de fois une
expérience ayant une chance très faible de réussir par une loi de
Poisson (nombre de personnes dans la population française atteints d’une
maladie rare, par exemple).</p>
<p>On cherche la probabilité de trouver au moins un centenaire parmi 200
personnes dans une population où une personne sur cent est un
centenaire.</p>
<p>La probabilité <span class="math inline">\(p=1/100=0.01\)</span> étant faible et <span class="math inline">\(n=200\)</span> étant suffisamment
grand, on peut modéliser le nombre <span class="math inline">\(X\)</span> de centenaires pris parmi 200
personnes par la loi de Poisson de paramètre
<span class="math inline">\(\lambda=200 \times 0.01=2\)</span>. Donc on a:
<span class="math display">\[P(X\geq 1)=1-P(X=0)=1-e^{-2}\simeq 0.86\]</span></p>
<p>Soit une v.a. <span class="math inline">\(X\)</span> telle que <span class="math inline">\(X \sim \mathcal{B}(100, 0.01)\)</span>, les valeurs
des probabilités pour <span class="math inline">\(k\)</span> de 0 à 5 ainsi que leur approximation à
<span class="math inline">\(10^{-3}\)</span> avec une loi de Poisson de paramètre <span class="math inline">\(\lambda= np =1\)</span> sont
données dans le tableau ci-dessous :</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(k\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(P(X = k)\)</span></td>
<td align="center">0.366</td>
<td align="center">0.370</td>
<td align="center">0.185</td>
<td align="center">0.061</td>
<td align="center">0.015</td>
<td align="center">0.000</td>
</tr>
<tr class="even">
<td align="center">Approximation</td>
<td align="center">0.368</td>
<td align="center">0.368</td>
<td align="center">0.184</td>
<td align="center">0.061</td>
<td align="center">0.015</td>
<td align="center">0.003</td>
</tr>
</tbody>
</table>
<p>Dans le cas de cet exemple où <span class="math inline">\(n =100\)</span> et <span class="math inline">\(np =1\)</span>, l’approximation de la
loi binomiale par une loi de poisson donne des valeurs de probabilités
identiques à <span class="math inline">\(10^{-3}\)</span> près.</p>
</div>
<div id="loi-geometrique-ou-de-pascal-mathcalgp" class="section level3 unnumbered">
<h3>Loi Géométrique ou de Pascal <span class="math inline">\(\mathcal{G}(p)\)</span></h3>
<p>On effectue des épreuves successives indépendantes jusqu’à la
réalisation d’un événement particulier <span class="math inline">\(A\)</span> de probabilité <span class="math inline">\(p=P(A)\)</span> et on
note <span class="math inline">\(X\)</span> le nombre aléatoire d’épreuves effectuées. On définit ainsi une
v.a. à valeurs entières de loi géométrique, ou de Pascal. A chaque
épreuve est associé l’ensemble fondamental <span class="math inline">\(\Omega=\{A, \bar{A}\}\)</span> et
l’événement <span class="math inline">\(\{X=k\}\)</span> pour <span class="math inline">\(k\in \mathbb{N^*}\)</span> est représenté par une
suite de <span class="math inline">\(k-1\)</span> événements <span class="math inline">\(\bar{A}\)</span>, terminée par l’événement <span class="math inline">\(A\)</span>:
<span class="math display">\[\underbrace{\bar{A}\bar{A}\ldots \bar{A}}_{k-1}A\]</span> D’où:</p>
<p><span class="math display" id="eq:geom">\[\begin{equation}
    P(X=k)=(1-p)^{k-1}p \quad \forall \, k \in \mathbb{N^*}
    \tag{0.3}
\end{equation}\]</span>
Cette loi peut servir à modéliser des temps de vie, ou des temps
d’attente, lorsque le temps est mesuré de manière discrète (nombre de
jours par exemple).</p>
<p>En utilisant la série entière <span class="math display">\[\label{eq:serie_entiere}
        \sum_{k=0}^\infty x^k = 1/(1-x) \quad \text{pour} \quad |x|&lt;1\]</span>
on vérifie bien que c’est une loi de probabilité:</p>
<p><span class="math display">\[\begin{aligned}
\sum_{k=1}^\infty P(X=k)&amp;= \sum_{k=1}^\infty (1-p)^{k-1}p = p \sum_{j=0}^\infty (1-p)^{j} \\
&amp;= p \frac{1}{1-(1-p)}=1\end{aligned}\]</span></p>
<p><strong>Moments de loi Géométrique</strong></p>
<p>En dérivant la série entière
<a href="variables-aleatoires-discretes.html#eq:geom">(0.3)</a> ci-dessus, on obtient
<span class="math inline">\(\sum_{k=1}^\infty k x^{k-1}=1/(1-x)^2\)</span>. Ceci permet d’obtenir
l’espérance:
<span class="math display">\[E(X)=\sum_{k=1}^\infty kp(1-p)^{k-1}=\frac{p}{[1-(1-p)]^2}=\frac{1}{p}\]</span></p>
<ul>
<li>En d’autres termes, si des épreuves indépendantes ayant une
probabilité <span class="math inline">\(p\)</span> d’obtenir un succès sont réalisés jusqu’à ce que le
premier succès se produise, le nombre espéré d’essais nécessaires
est égal à <span class="math inline">\(1/p\)</span>. Par exemple, le nombre espéré de jets d’un dé
équilibré qu’il faut pour obtenir la valeur 1 est 6.</li>
</ul>
<p>Le calcul de la variance se fait à partir du moment factoriel et en
utilisant la dérivée seconde de la série entière
<a href="variables-aleatoires-discretes.html#eq:geom">(0.3)</a>:
<span class="math inline">\(\sum_{k=2}^\infty k(k-1) x^{k-2} = 2/(1-x)^3\)</span>, Donc</p>
<p><span class="math display">\[\begin{aligned}
E[X(X-1)]&amp;=\sum_{k=2}^\infty k(k-1)p(1-p)^{k-1} \\ &amp;= p(1-p)\sum_{k=2}^\infty k(k-1)(1-p)^{k-2} \\
&amp;=  \frac{2p(1-p)}{[1-(1-p)]^3}=\frac{2(1-p)}{p^2}\end{aligned}\]</span> d’où
on déduit: <span class="math display">\[V(X)=E[X(X-1)]+E(X)-E^2(X)=\frac{1-p}{p^2}.\]</span></p>
<p>Si l’on considère la variable aléatoire <span class="math inline">\(X\)</span> “nombre de naissances
observées jusqu’à l’obtention d’une fille” avec p = 1/2 (même
probabilité de naissance d’une fille ou d’un garçon), alors X suit une
loi géométrique et on a pour tout <span class="math inline">\(k\in \mathbb{N^*}\)</span>:
<span class="math display">\[P(X=k)=(1-1/2)^{k-1}(1/2)=1/2^k\]</span> avec <span class="math inline">\(E(X)=2\)</span> et <span class="math inline">\(V(X)=2.\)</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variables-aleatoires-continues.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Statistique_infentielle.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
